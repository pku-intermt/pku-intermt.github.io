<!DOCTYPE html>
<html class="fontawesome-i2svg-active fontawesome-i2svg-complete">
<head>
	<link rel="preconnect" href="https://fonts.googleapis.com">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300..800;1,300..800&display=swap" rel="stylesheet">
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<style data-merge-styles="true"></style>
	<style data-merge-styles="true"></style>
	<style data-merge-styles="true"></style>
	<meta name="description" content="Multi-Turn Interleaved Preference Alignment with Human Feedback">
	<meta name="keywords" content="Multi-Turn, Multi-Modality">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Multi-Turn Interleaved Preference Alignment with Human Feedback</title>
	<link rel="icon" href="./assets/project_logo.png" type="image/png">
	<!-- ÂºïÂÖ•cssÊñá‰ª∂ -->
	<link href="./assets/css" rel="stylesheet">
	<link rel="stylesheet" href="./assets/bulma.min.css">
	<link rel="stylesheet" href="./assets/bulma-carousel.min.css">
	<link rel="stylesheet" href="./assets/font-face.css">
	<link rel="stylesheet" href="./assets/bulma-slider.min.css">
	<link rel="stylesheet" href="./assets/fontawesome.all.min.css">
	<link rel="stylesheet" href="./assets/academicons.min.css">
	<link rel="stylesheet" href="./assets/index.css">
	<link rel="stylesheet" href="./assets/leaderboard.css">
	<style>
		p, ul {
		    text-align: justify;
		    margin-left: auto;
		    margin-right: auto;
		    width: 75%;
		}
		li {
		    margin-bottom: 10px;
		}
	</style>
	<!-- ÂºïÂÖ•jsÊñá‰ª∂ -->
	<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
	<script type="text/javascript" src="./assets/sort-table.js" defer=""></script>
	<script src="./assets/jquery.min.js"></script>
	<script defer="" src="./assets/fontawesome.all.min.js"></script>
	<script src="./assets/bulma-carousel.min.js"></script>
	<script src="./assets/bulma-slider.min.js"></script>
	<script src="./assets/explorer-index.js"></script>
	<script src="./assets/question_card.js"></script>
	<script src="./assets/leaderboard_testmini.js"></script>
	<script src="./assets/output_folders.js" defer=""></script>
	<script src="./assets/model_scores.js" defer=""></script>
	<script src="./assets/data_public.js" defer=""></script>
</head>
<nav2 class="navbar" role="navigation" aria-label="main navigation">
	<div class="navbar-brand">
		<a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
			<span aria-hidden="true"></span>
			<span aria-hidden="true"></span>
			<span aria-hidden="true"></span>
		</a>
	</div>
	<div class="navbar-menu">
		<div class="navbar-start" style="flex-grow: 1; justify-content: center;">
			<!-- @PAN TODO: consider adding links? -->
			<div class="navbar-item has-dropdown is-hoverable">
				<a class="navbar-link"> More Research </a>
				<div class="navbar-dropdown">
					<a class="navbar-item" href="https://github.com/PKU-Alignment/safe-rlhf">
						<b>PKU-SafeRLHF (ACL2025 Main)</b>
					</a>
					<a class="navbar-item" href="https://github.com/PKU-Alignment/aligner">
						<b>Aligner (NeurIPS 2024 Oral)</b>
					</a>
					<a class="navbar-item" href="https://github.com/PKU-Alignment/safe-sora">
						<b>SafeSora (NeurIPS 2024 DB Track)</b>
					</a>
					<a class="navbar-item" href="https://github.com/PKU-Alignment/beavertails">
						<b>BeaverTails (NeurIPS 2024 DB Track)</b>
					</a>
					<a class="navbar-item" href="https://align-anything.readthedocs.io/en/latest/index.html">
						<b>Align-Anythingüî•üî•üî•</b>
					</a>
				</div>
			</div>
		</div>
	</div>
</nav2>
<section class="hero">
	<div class="hero-body">
		<div class="container is-max-desktop">
			<div class="columns is-centered">
				<div class="column has-text-centered">
					<h1 class="title is-1 publication-title">Multi-Turn Interleaved Preference Alignment with Human Feedback</h1>
					<div class="is-size-5 publication-authors">
						<div class="is-size-5 publication-authors">
							<span class="author-block"> Boyuan Chen<sup>1*</sup>Ôºå </span>
							<span class="author-block"> Donghai Hong<sup>1*</sup>Ôºå </span>
							<span class="author-block"> Jiaming Ji<sup>1*</sup>Ôºå </span>
							<span class="author-block"> Jiacheng Zheng<sup>2</sup>Ôºå </span>
							<span class="author-block"> Bowen Dong<sup>1</sup>Ôºå </span>
							<span class="author-block"> Jiayi Zhou<sup>1</sup>Ôºå </span>
							<span class="author-block"> Kaile Wang<sup>1</sup>Ôºå </span>
							<span class="author-block"> Josef Dai<sup>1</sup>Ôºå </span>
							<span class="author-block"> Xuyao Wang<sup>1</sup>Ôºå </span>
							<span class="author-block"> Wenqi Chen<sup>1</sup>Ôºå </span>
							<span class="author-block"> Qirui Zheng<sup>1</sup>Ôºå </span>
							<span class="author-block"> Wenxin Li<sup>1</sup>Ôºå </span>
							<span class="author-block"> Sirui Han<sup>2</sup>Ôºå </span>
							<span class="author-block"> Yike Guo<sup>2</sup>Ôºå </span>
							<span class="author-block"> Yaodong Yang<sup>1‚Ä†</sup>Ôºå </span>
						</div>
						<div class="is-size-5 publication-authors">
							<span class="author-block">
								<sup>1</sup>
								<em>Peking University</em>
							</span>
							<span class="author-block">
								<sup>2</sup>
								<em>The Hong Kong University of Science and Technology</em>
							</span>
						</div>
						<p>
							<small> * Equal contribution, <sup>‚Ä†</sup> corresponding author. </small>
						</p>
						<br>
					</div>
					<div class="column has-text-centered">
						<div class="publication-links">
							<!-- PDF Link. -->
							<span class="link-block">
								<a href="#" class="external-link button is-normal is-rounded is-dark">
									<span class="icon">
										<svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg="">
											<path fill="currentColor"
												d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z">
											</path>
										</svg><!-- <i class="fas fa-file-pdf"></i> Font Awesome fontawesome.com -->
									</span>
									<span>Paper</span>
								</a>
							</span>
							<span class="link-block">
								<a href="https://huggingface.co/datasets/PKU-Alignment/INTERMT" class="external-link button is-normal is-rounded is-dark">
									<span class="icon">
										<img src="./assets/huggingface-color.svg" alt="HuggingFace" style="width: 1em; height: 1em;" />
									</span>
									<span>HuggingFace</span>
								</a>
							</span>
							<!-- Video Link. -->
							<span class="link-block">
								<a href="https://github.com/cby-pku/INTERMT" class="external-link button is-normal is-rounded is-dark">
									<span class="icon">
										<svg class="svg-inline--fa fa-github fa-w-16" aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" data-fa-i2svg="">
											<path fill="currentColor"
												d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z">
											</path>
										</svg><!-- <i class="fab fa-github"></i> Font Awesome fontawesome.com -->
									</span>
									<span>GitHub</span>
								</a>
							</span>
							<!-- Code Link. -->
							<span class="link-block">
								<a href="https://huggingface.co/PKU-Alignment/INTERMT-Judge" class="external-link button is-normal is-rounded is-dark">
									<span class="icon">
										<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512">
											<!--!Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2025 Fonticons, Inc.-->
											<path fill="#ffffff"
												d="M0 96C0 60.7 28.7 32 64 32l320 0c35.3 0 64 28.7 64 64l0 320c0 35.3-28.7 64-64 64L64 480c-35.3 0-64-28.7-64-64L0 96zm144 4c-24.3 0-44 19.7-44 44l0 48c0 24.3 19.7 44 44 44l32 0c24.3 0 44-19.7 44-44l0-48c0-24.3-19.7-44-44-44l-32 0zm-4 44c0-2.2 1.8-4 4-4l32 0c2.2 0 4 1.8 4 4l0 48c0 2.2-1.8 4-4 4l-32 0c-2.2 0-4-1.8-4-4l0-48zm140-44c-11 0-20 9-20 20c0 9.7 6.9 17.7 16 19.6l0 76.4c0 11 9 20 20 20s20-9 20-20l0-96c0-11-9-20-20-20l-16 0zM132 296c0 9.7 6.9 17.7 16 19.6l0 76.4c0 11 9 20 20 20s20-9 20-20l0-96c0-11-9-20-20-20l-16 0c-11 0-20 9-20 20zm96 24l0 48c0 24.3 19.7 44 44 44l32 0c24.3 0 44-19.7 44-44l0-48c0-24.3-19.7-44-44-44l-32 0c-24.3 0-44 19.7-44 44zm44-4l32 0c2.2 0 4 1.8 4 4l0 48c0 2.2-1.8 4-4 4l-32 0c-2.2 0-4-1.8-4-4l0-48c0-2.2 1.8-4 4-4z" />
										</svg>
									</span>
									<span>Model</span>
								</a>
							</span>
						</div>
					</div>
				</div>
			</div>
		</div>
	</div>
</section>
<section class="hero teaser">
	<div class="container is-max-desktop">
		<div class="content has-text-centered">
			<img src="./assets/figure1_0515.png" alt="geometric reasoning" width="100%" style="box-shadow: 0 4px 10px rgba(0, 0, 0, 0.3); border-radius: 8px;">
			<h2 style="text-align: center;">Abstract</h2>
			<p style="text-align: justify;"> As multimodal large models (MLLMs) continue to advance across challenging tasks, a key question emerges: <strong>
					<em>What essential capabilities are still missing?</em>
				</strong> A critical aspect of human learning is continuous interaction with the environment ‚Äì not limited to language, but also involving multimodal understanding and generation. To move closer to human-level intelligence, models must similarly support <strong>multi-turn</strong>,
				<strong>multimodal interaction</strong>. In particular, they should comprehend interleaved multimodal contexts and respond coherently in ongoing exchanges. In this work, we present <strong>an initial exploration</strong> through the <strong>InterMT</strong> ‚Äì <strong>the first
					preference dataset for <em>multi-turn</em> multimodal interaction</strong>, grounded in real human feedback. In this exploration, we particularly emphasize the importance of human oversight, introducing expert annotations to guide the process, motivated by the fact that current
				MLLMs lack such complex interactive capabilities. <strong>InterMT</strong> captures human preferences at both global and local levels into nine sub-dimensions, consists of 5,437 prompts, 2.6k multi-turn dialogue instances, and 2.1k human-labeled preference pairs. To compensate for
				the lack of capability for multi-modal understanding and generation, we introduce an agentic workflow that leverages tool-augmented MLLMs to construct multi-turn QA instances. We further this goal by introducing <strong>InterMT-Bench</strong> to assess the ability of MLLMs in
				assisting judges with multi-turn, multimodal tasks. We demonstrate the utility of <strong>InterMT</strong> through applications such as judge moderation and further reveal the <em>multi-turn scaling law</em> of judge model. We hope the open-source of our data can help facilitate
				further research on aligning current MLLMs to the next step. </p>
		</div>
	</div>
</section>
<br>
<br>
<section class="hero is-light is-small">
	<div class="hero-body has-text-centered">
		<!-- <h1 class="title is-1 mathvista"><img src="static/images/mathvista.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>MathVista Dataset</h1> -->
		<h1 class="title is-1 mathvista">
			<!--<img src="static/images/mathvista.png" style="width:1em;vertical-align: middle" alt="Logo"/>-->
			<span class="mathvista" style="vertical-align: middle" id="introduction">Introduction</span>
		</h1>
	</div>
</section>
<section class="section">
	<div class="container" style="margin-bottom: 2vh;">
		<div class="columns is-centered has-text-centered">
			<div class="column is-four-fifths">
				<p>Humans perceive the world through dynamic, multimodal interactions involving text, images, audio, video, and more. Building on the success of multimodal large language models (MLLMs), recent efforts aim to develop general-purpose AI assistants that handle multiple mixed
					modalities. A key feature of such assistants is to engage in natural multi-turn conversations, perceive, and generate any modality, enabling smoother interaction and grounded understanding.</p>
				<p>Recent years have seen community efforts in transplanting alignment techniques such as Reinforcement Learning from Human Feedback (RLHF) from the text modality to multiple modality settings. Within this line of research, most studies focus exclusively on either understanding or
					generation. The lack of alignment considerations for multimodal mixed input-output settings exacerbates the imbalance across modalities, a phenomenon referred to as modality disequilibrium. Furthermore, existing methods primarily focus on single-turn interactions, where an AI
					generates a response based on a prompt and receives immediate alignment feedback.</p>
				<p>However, real-world interactions typically occur in long-horizon conversations involving interleaved multimodal inputs and outputs. Addressing this gap, we propose a novel dataset that focuses on capturing human preferences in multi-turn, multimodal interaction settings.</p>
				<blockquote style="border:1px solid #ccc; padding:10px; background:#f9f9f9; margin:20px 0; text-align:center; font-style:italic; font-size:regular;"> How to improve <strong style="color:#8B0000;">multi-turn</strong> interleaved <strong
						style="color:#00008B;">understanding</strong>-<strong style="color:#006400;">generation</strong> alignment via human feedback? </blockquote>
				<h3 style="text-decoration: underline;"><strong>Key Issues in MLLM Alignment:</strong></h3>
				<ul>
					<li><strong>Modality Fusion via Harmonizing Understanding and Generation:</strong> Effective AI systems should not only perceive and understand multimodal content but also generate multimodal outputs to communicate naturally and contextually.</li>
					<li><strong>Modeling Long-Horizon, Interleaved Multimodal Interactions:</strong> Real-world exchanges span multiple turns and interleave various modalities, demanding sustained attention and reasoning over evolving contexts.</li>
					<li><strong>Dynamic Human-in-the-Loop Alignment:</strong> In multi-turn interactions, user preferences evolve dynamically. Capturing and aligning with these emergent preferences requires continuous, iterative human feedback.</li>
				</ul>
				<h3 style="text-decoration: underline;"><strong>Contributions:</strong></h3>
				<ul>
					<li><strong>First Multi-turn Interleaved Preference Dataset:</strong> We introduce a dataset that captures real human preferences for tasks involving multi-turn and interleaved multimodal understanding and generation.</li>
					<li><strong>Agent-based Construction Workflow:</strong> The dataset is constructed using a multi-turn QA workflow that leverages strong MLLMs augmented with external tools to simulate realistic interactions.</li>
					<li><strong>Decoupled Helpfulness in Multi-turn Scenarios:</strong> Helpfulness is assessed at both local (turn-level) and global (conversation-level) scales, with 9 specific dimensions of evaluation.</li>
					<li><strong>Multi-turn Alignment Insights:</strong> We investigate how multi-turn interactions facilitate preference modeling, revealing phenomena such as preference transfer and multi-turn scaling effects.</li>
					<li><strong>Evaluation of MLLMs as Judge Models:</strong> We introduce a framework to assess MLLMs' capabilities as judges in multi-turn, multimodal tasks, focusing on scoring, pair comparison, and crucial step recognition.</li>
				</ul>
			</div>
		</div>
	</div>
</section>
<!-- DATASET SECTION -->
<section class="hero is-light is-small">
	<div class="hero-body has-text-centered">
		<!-- <h1 class="title is-1 mathvista"><img src="static/images/mathvista.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>MathVista Dataset</h1> -->
		<h1 class="title is-1 mathvista">
			<!--<img src="static/images/mathvista.png" style="width:1em;vertical-align: middle" alt="Logo"/>-->
			<span class="mathvista" style="vertical-align: middle" id="Dataset">InterMT Dataset</span>
		</h1>
	</div>
</section>
<section class="section">
	<div class="container">
		<center>
			<h2 class="title is-3">Overview</h2>
		</center>
		<div class="columns is-centered">
			<div class="column is-half">
				<div class="content has-text-justified">
					<section id="dataset">
						<h2>Dataset</h2>
						<p>Our core contribution is the introduction of a human preference dataset designed for <span style="color:#8B0000;">multi-turn</span>, multimodal <span style="color:#00008B;">understanding</span> and <span style="color:#006400;">generation</span> tasks. This section outlines
							the dataset's composition, the collection of prompts and multi-turn QA instances, and the human annotation process.</p>
						<section id="composition">
							<h3>Dataset Composition</h3>
							<p>The dataset comprises two primary data types: (1) <em>Seed Questions</em> for initiating <span style="color:#8B0000;">multi-turn</span> conversations, and (2) human preference annotations at both the local and global levels. Prompts are refined using principles from
								linguistics and cognitive psychology to simulate realistic multimodal <span style="color:#8B0000;">multi-turn</span> tasks effectively.</p>
						</section>
						<section id="qa-construction">
							<h3>Multi-turn QA Construction</h3>
							<p>A tool-augmented workflow is employed to simulate human-like dialogues across various <span style="color:#8B0000;">multi-turn</span> tasks, integrating large models (e.g., GPT-4o, Qwen2.5-VL) with multimodal tools for image generation, editing, and retrieval.</p>
						</section>
						<section id="annotation">
							<h3>Preference Annotation</h3>
							<p>Human annotation includes local and global evaluations of conversation quality, focusing on <span style="color:#00008B;">understanding</span>-<span style="color:#006400;">generation</span> alignment, coherence, and completeness.</p>
						</section>
					</section>
				</div>
			</div>
			<div class="column is-half">
				<div class="carousel results-carousel">
					<div class="carousel-item">
						<div class="box m-5">
							<div class="content has-text-centered">
								<img src="./assets/Figure2.png" style="width: 100%;">
								<p style="margin-bottom: 30px;">
									<span class="icon">
										<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
									</span>
									<b>InterMT</b> includes over 15 tasks in vision-language scenarios, capturing communication examples across diverse multi-turn settings.
								</p>
							</div>
						</div>
					</div>
					<div class="carousel-item">
						<div class="box m-5">
							<div class="content has-text-centered">
								<img src="./assets/Figure3-0515_00.png" style="width: 100%;">
								<p style="margin-bottom: 30px;">Overview of the four-stage pipeline for constructing <span class="icon">
										<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
									</span>
									<b>InterMT</b>. The pipeline covers data harvesting, model-generated expansion, human annotation, and final QA tree assembly.
								</p>
							</div>
						</div>
					</div>
				</div>
			</div>
		</div>
	</div>
</section>
<section class="hero is-light is-small">
	<div class="hero-body has-text-centered">
		<!-- <h1 class="title is-1 mathvista"><img src="static/images/mathvista.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>MathVista Dataset</h1> -->
		<h1 class="title is-1 mathvista">
			<!--<img src="static/images/mathvista.png" style="width:1em;vertical-align: middle" alt="Logo"/>-->
			<span class="mathvista" style="vertical-align: middle" id="Experiment Analysis">Experiment Analysis</span>
		</h1>
	</div>
</section>
<section class="section">
	<div class="container">
		<div class="columns is-centered has-text-centered">
			<div class="column is-four-fifths" style="width: 100%;">
				<h2 class="title is-3">Main Analysis</h2>
				<div class="content has-text-justified">
					<div id="results-carousel" class="carousel results-carousel">
						<div class="content has-text-centered">
							<img src="./assets/Figure5-1.png" style="width: 65%;">
							<p style="text-align: justify; margin-left: auto; margin-right: auto; width: 75%;"> Since the dataset captures <em>real</em> human preferences across multiple dimensions at both <em>global</em> and <em>local</em> levels, it is meaningful to analyze the correlations among
								these dimensions, examine the relationship between per-turn preferences and overall evaluation, and further compare human feedback with AI feedback. <br>
								<br>
								<strong>Correlation Analysis</strong>
								<br> Three key findings from the analysis: <br> 1. <strong>Modality perception precedes effective modality fusion:</strong> The evaluation of image-text consistency is strongly correlated with visual perceptual quality, indicating that clear perception of individual
								modalities is a prerequisite for reliable multimodal judgment. <br> 2. <strong>Long-horizon evaluations hinge on coherence and temporal consistency:</strong> Metrics such as helpfulness and completeness strongly align with context awareness and global visual
								consistency, underscoring the importance of maintaining coherent semantics across extended interactions. <br> 3. <strong>Intent grounding drives long-horizon crucial step recognition:</strong> In multi-turn scenarios, models may produce plausible outputs but deviate
								from core user intentions, leading to stylistic drift and omission of key steps. <br>
								<br>
								<strong>Human Feedback vs. AI Feedback</strong>
								<br> Our analysis reveals that while AI annotators achieve reasonable agreement with human annotators at the local level (60%), their agreement drops significantly at the global level, particularly in long-horizon, multi-turn tasks. This indicates a current gap in
								AI's ability to align with human judgments in extended interactions.
							</p>
						</div>
						<div class="content has-text-centered">
							<img src="./assets/Figure5.png" style="width: 65%;">
							<p style="text-align: justify; margin-left: auto; margin-right: auto; width: 75%;"> This figure presents an analysis of the agreement among human annotators, AI annotators, and human verifiers. Our observations reveal that while AI annotators achieved an average score
								agreement of approximately 60\% on \textit{local} evaluation tasks, their agreement with humans is considerably lower for <em>global</em>, namely, longer-horizon, evaluation tasks. This finding partly indicates that current advanced models struggle to reach consensus
								with human judgments when performing score evaluations in multi-turn, multi-modal conversation tasks. We further conduct a broader investigation of the ability of MLLMs to assist human judges in three types of tasks. Consequently, until further validation of AI
								feedback's efficacy, we maintain a conservative position, considering the replacement of human annotation to be presently challenging. </p>
						</div>
						<div class="content has-text-centered">
							<img src="./assets/Figure6_00.jpg" style="width: 65%;">
							<p style="text-align: justify; margin-left: auto; margin-right: auto; width: 75%;"> Our findings suggest that modeling fine-grained <em>local (turn-level)</em> preferences is more effective in capturing human values and achieving better alignment. In contrast, directly
								modeling <em>global (conversation-level)</em> preferences often fails to reflect these nuanced preferences, especially in complex, long-horizon dialogue scenarios.</p>
						</div>
					</div>
				</div>
			</div>
		</div>
	</div>
	<section class="section">
		<div class="container">
			<div class="columns is-centered has-text-centered">
				<div class="column is-four-fifths" style="width: 100%;">
					<h2 class="title is-3">More Results</h2>
					<div class="content has-text-justified">
						<div id="results-carousel" class="carousel results-carousel">
							<div class="content has-text-centered">
								<img src="./assets/Figure7_00.jpg" style="width: 55%;">
								<p style="text-align: justify; margin-left: auto; margin-right: auto; width: 75%;"> As shown in the Figure, for evaluation turn \( k \), as the number of preceding turns increases from 1 to \( k-1 \), the model's accuracy continues to improve, with average future
									performance rising, indicating that training on multi-turn data with a limited number of turns can generalize to longer horizons.</p>
								<p style="text-align: justify; margin-left: auto; margin-right: auto; width: 75%;"> Regarding the number of turns in the training data, the generalization effect shows a diminishing trend. As demonstrated in the Figure, training with \( k \) turns does improve
									performance for \( k+1 \rightarrow T \) turns, but this effect diminishes as the number of turns increases. This decline can be attributed to three factors:</p>
								<ul style="text-align: justify; margin-left: auto; margin-right: auto; width: 100%;">
									<li style="text-align: justify; margin-left: auto; margin-right: auto; width: 75%;"> <em>Diminishing returns:</em> As the number of turns increases, the model's ability to capture long-term preferences weakens, leading to a performance plateau beyond a certain
										threshold.</li>
									<li style="text-align: justify; margin-left: auto; margin-right: auto; width: 75%;"> <em>Contextual drift:</em> As the conversation progresses, earlier turns lose relevance, causing the model to lose context and negatively affecting later predictions.</li>
									<li style="text-align: justify; margin-left: auto; margin-right: auto; width: 75%;"> <em>Intention-preference interaction:</em> The evolving relationship between user intentions (e.g., task-oriented goals) and latent preferences (e.g., personal preferences) may
										complicate the model's ability to distinguish between the two, with the conversational structure influencing predictive accuracy.</li>
								</ul>
							</div>
							<div class="content has-text-centered">
								<img src="./assets/Figure8-v1_00.jpg" style="width: 75%;">
								<p style="text-align: justify; margin-left: auto; margin-right: auto; width: 75%;"> The dataset includes multi-turn multimodal interleaved communication histories and human-annotated ground truth. Evaluated models must assess the conversation at both the turn and
									conversation levels across nine dimensions, following a set of guidelines.</p>
								<p style="text-align: justify; margin-left: auto; margin-right: auto; width: 75%;"><em>Scoring Evaluation</em> requires the model to assign scores on a 0-3 scale, with evaluation based on agreement and Pearson similarity.</p>
								<p style="text-align: justify; margin-left: auto; margin-right: auto; width: 75%;"><em>Pair Comparison</em> directly compares two individual turns or entire conversations, without considering ties, and is evaluated for accuracy against human judgments.</p>
								<p style="text-align: justify; margin-left: auto; margin-right: auto; width: 75%;"><em>Crucial Step Recognition</em> addresses a key challenge in multi-turn conversations: accurately identifying the user‚Äôs intent and determining whether it has been fulfilled,
									evaluated by the score provided by the judge according to the human-annotated reference answers.</p>
							</div>
						</div>
					</div>
				</div>
			</div>
		</div>
		<section class="section">
			<div class="container">
				<div class="columns is-centered">
					<div class="column is-full has-text-centered content">
						<h2 class="title is-3" id="Evaluation">
							<span class="icon">
								<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
							</span>
							<b>InterMT</b> Performance Evaluation
						</h2>
						<div class="content">
							<p class="mt-3">What does <span class="icon">
									<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
								</span>
								<b>InterMT</b> include and overview of the four-stage pipeline for constructing <span class="icon">
									<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
								</span>
								<b>InterMT</b>
							</p>
							<style>
								/* ÂÖ®Â±ÄÊ†∑Âºè */
								body {
									font-family: Arial, sans-serif;
									margin: 20px;
								}
								
								/* Ë°®Ê†ºÂü∫Á°ÄÊ†∑Âºè */
								.data-table {
									width: 100%;
									border-collapse: collapse;
									margin: 20px 0;
									background-color: white;
								}
								
								/* Ë°®Ê†ºÂ∏ÉÂ±Ä */
								.data-table th,
								.data-table td {
									padding: 12px 15px;
									text-align: center;
									vertical-align: middle;
									border: 1px solid #e1e1e1;
								}
								
								/* Ë°®Â§¥Ê†∑Âºè */
								.data-table thead {
									background-color: #2c3e50;
								}
								
								.data-table th {
									font-weight: bold;
									font-size: 14px;
									color: #ffffff;
								}
								
								/* Ë°®Ê†ºË°åÊ†∑Âºè */
								.data-table tbody tr:nth-child(even) {
									background-color: #f9f9f9;
								}
								
								.data-table tbody tr:hover {
									background-color: #f1f1f1;
								}
								
								/* ÂçïÂÖÉÊ†ºÂ≠ó‰ΩìÂ§ßÂ∞è */
								.data-table td {
									font-size: 13px;
								}
								
								/* ÁâπÊÆäÊ†∑ÂºèÁ±ª */
								.bold {
									font-weight: bold;
								}
								
								.highlight {
									background-color: #fffde7;
								}
								
								/* Ë°®Ê†ºÂØπÈΩê */
								.center-table {
									margin-left: auto;
									margin-right: auto;
								}
							</style>
							<table class="data-table center-table">
								<thead>
									<tr>
										<th rowspan="2" style="color: #ffffff;">Settings</th>
										<th rowspan="2" style="color: #ffffff;">MLLMs</th>
										<th colspan="5" style="color: #ffffff;">Local Setting</th>
										<th colspan="6" style="color: #ffffff;">Global Setting</th>
									</tr>
									<tr>
										<th style="color: #ffffff;">L1</th>
										<th style="color: #ffffff;">L2</th>
										<th style="color: #ffffff;">L3</th>
										<th style="color: #ffffff;">L4</th>
										<th style="color: #ffffff;">Avg.</th>
										<th style="color: #ffffff;">G1</th>
										<th style="color: #ffffff;">G2</th>
										<th style="color: #ffffff;">G3</th>
										<th style="color: #ffffff;">G4</th>
										<th style="color: #ffffff;">G5</th>
										<th style="color: #ffffff;">Avg.</th>
									</tr>
								</thead>
								<tbody>
									<tr>
										<td rowspan="12" class="bold">Scoring Evaluation</td>
										<td>Gemini-Flash* ‚Ä†</td>
										<td>0.346</td>
										<td>0.107</td>
										<td>0.119</td>
										<td>0.173</td>
										<td>0.186</td>
										<td>0.163</td>
										<td>0.042</td>
										<td>0.051</td>
										<td>
											<strong>0.246</strong>
										</td>
										<td>0.005</td>
										<td>0.101</td>
									</tr>
									<tr>
										<td>Gemini-Flash* (+reason)</td>
										<td>
											<strong>0.361</strong>
										</td>
										<td>0.072</td>
										<td>0.122</td>
										<td>0.168</td>
										<td>0.181</td>
										<td>-0.038</td>
										<td>0.083</td>
										<td>0.139</td>
										<td>0.199</td>
										<td>0.048</td>
										<td>0.086</td>
									</tr>
									<tr>
										<td>GPT-4.1</td>
										<td>0.264</td>
										<td>0.095</td>
										<td>0.242</td>
										<td>0.269</td>
										<td>0.218</td>
										<td>0.215</td>
										<td>0.216</td>
										<td>0.084</td>
										<td>0.044</td>
										<td>0.049</td>
										<td>0.122</td>
									</tr>
									<tr>
										<td>GPT-4.1 (+reason)</td>
										<td>0.281</td>
										<td>0.094</td>
										<td>0.272</td>
										<td>0.271</td>
										<td>0.229</td>
										<td>0.215</td>
										<td>0.255</td>
										<td>0.217</td>
										<td>0.216</td>
										<td>0.050</td>
										<td>
											<strong>0.191</strong>
										</td>
									</tr>
									<tr>
										<td>GPT-4o</td>
										<td>0.291</td>
										<td>
											<strong>0.131</strong>
										</td>
										<td>0.277</td>
										<td>0.268</td>
										<td>
											<strong>0.242</strong>
										</td>
										<td>0.254</td>
										<td>0.167</td>
										<td>0.137</td>
										<td>0.139</td>
										<td>0.069</td>
										<td>0.153</td>
									</tr>
									<tr>
										<td>GPT-4o (+reason)</td>
										<td>0.290</td>
										<td>0.091</td>
										<td>0.252</td>
										<td>
											<strong>0.280</strong>
										</td>
										<td>0.228</td>
										<td>0.183</td>
										<td>0.243</td>
										<td>0.194</td>
										<td>0.086</td>
										<td>0.072</td>
										<td>0.156</td>
									</tr>
									<tr>
										<td>Gemini-Pro*</td>
										<td>0.273</td>
										<td>0.079</td>
										<td>0.258</td>
										<td>0.168</td>
										<td>0.194</td>
										<td>
											<strong>0.285</strong>
										</td>
										<td>0.240</td>
										<td>-0.024</td>
										<td>0.235</td>
										<td>
											<strong>0.145</strong>
										</td>
										<td>0.176</td>
									</tr>
									<tr>
										<td>Gemini-Pro* (+reason)</td>
										<td>0.274</td>
										<td>0.070</td>
										<td>0.304</td>
										<td>0.211</td>
										<td>0.215</td>
										<td>0.239</td>
										<td>
											<strong>0.267</strong>
										</td>
										<td>0.195</td>
										<td>0.129</td>
										<td>0.060</td>
										<td>0.178</td>
									</tr>
									<tr>
										<td>Claude-thinking*</td>
										<td>0.299</td>
										<td>0.044</td>
										<td>0.262</td>
										<td>0.229</td>
										<td>0.209</td>
										<td>0.172</td>
										<td>0.140</td>
										<td>0.175</td>
										<td>0.150</td>
										<td>0.069</td>
										<td>0.141</td>
									</tr>
									<tr>
										<td>Claude-thinking* (+reason)</td>
										<td>0.291</td>
										<td>0.023</td>
										<td>0.254</td>
										<td>0.214</td>
										<td>0.196</td>
										<td>0.207</td>
										<td>0.260</td>
										<td>0.183</td>
										<td>0.155</td>
										<td>-0.001</td>
										<td>0.161</td>
									</tr>
									<tr>
										<td>o4-mini</td>
										<td>0.334</td>
										<td>0.062</td>
										<td>0.306</td>
										<td>0.134</td>
										<td>0.209</td>
										<td>0.169</td>
										<td>0.161</td>
										<td>0.120</td>
										<td>0.096</td>
										<td>0.028</td>
										<td>0.115</td>
									</tr>
									<tr>
										<td>o4-mini (+reason)</td>
										<td>0.326</td>
										<td>0.056</td>
										<td>
											<strong>0.322</strong>
										</td>
										<td>0.151</td>
										<td>0.214</td>
										<td>0.215</td>
										<td>0.229</td>
										<td>
											<strong>0.347</strong>
										</td>
										<td>0.137</td>
										<td>0.016</td>
										<td>0.189</td>
									</tr>
									<tr>
										<td rowspan="10" class="bold">Pair Comparison</td>
										<td>GPT-4.1</td>
										<td>0.541</td>
										<td>
											<strong>0.589</strong>
										</td>
										<td>0.508</td>
										<td>0.484</td>
										<td>0.531</td>
										<td>0.540</td>
										<td>0.520</td>
										<td>0.530</td>
										<td>
											<strong>0.590</strong>
										</td>
										<td>
											<strong>0.563</strong>
										</td>
										<td>
											<strong>0.549</strong>
										</td>
									</tr>
									<tr>
										<td>GPT-4.1 (+reason)</td>
										<td>0.550</td>
										<td>0.584</td>
										<td>0.501</td>
										<td>0.521</td>
										<td>
											<strong>0.539</strong>
										</td>
										<td>0.520</td>
										<td>0.520</td>
										<td>0.477</td>
										<td>0.513</td>
										<td>0.540</td>
										<td>0.514</td>
									</tr>
									<tr>
										<td>GPT-4o</td>
										<td>0.513</td>
										<td>0.488</td>
										<td>0.499</td>
										<td>0.510</td>
										<td>0.503</td>
										<td>0.560</td>
										<td>0.517</td>
										<td>
											<strong>0.550</strong>
										</td>
										<td>0.543</td>
										<td>0.470</td>
										<td>0.528</td>
									</tr>
									<tr>
										<td>GPT-4o (+reason)</td>
										<td>0.500</td>
										<td>0.537</td>
										<td>
											<strong>0.511</strong>
										</td>
										<td>0.509</td>
										<td>0.514</td>
										<td>0.542</td>
										<td>0.490</td>
										<td>0.545</td>
										<td>0.522</td>
										<td>0.528</td>
										<td>0.525</td>
									</tr>
									<tr>
										<td>Gemini-Pro*</td>
										<td>0.533</td>
										<td>0.521</td>
										<td>0.496</td>
										<td>0.533</td>
										<td>0.521</td>
										<td>
											<strong>0.562</strong>
										</td>
										<td>
											<strong>0.566</strong>
										</td>
										<td>0.523</td>
										<td>0.505</td>
										<td>0.505</td>
										<td>0.532</td>
									</tr>
									<tr>
										<td>Gemini-Pro* (+reason)</td>
										<td>0.526</td>
										<td>0.528</td>
										<td>0.513</td>
										<td>0.514</td>
										<td>0.520</td>
										<td>0.548</td>
										<td>0.562</td>
										<td>0.495</td>
										<td>0.522</td>
										<td>0.538</td>
										<td>0.533</td>
									</tr>
									<tr>
										<td>Claude-thinking*</td>
										<td>0.561</td>
										<td>0.568</td>
										<td>0.508</td>
										<td>0.502</td>
										<td>0.535</td>
										<td>0.539</td>
										<td>0.523</td>
										<td>0.518</td>
										<td>0.521</td>
										<td>0.528</td>
										<td>0.526</td>
									</tr>
									<tr>
										<td>Claude-thinking* (+reason)</td>
										<td>
											<strong>0.567</strong>
										</td>
										<td>0.550</td>
										<td>0.506</td>
										<td>0.519</td>
										<td>0.536</td>
										<td>0.512</td>
										<td>0.522</td>
										<td>0.512</td>
										<td>0.547</td>
										<td>0.512</td>
										<td>0.521</td>
									</tr>
									<tr>
										<td>o4-mini</td>
										<td>0.556</td>
										<td>0.549</td>
										<td>0.508</td>
										<td>
											<strong>0.536</strong>
										</td>
										<td>0.537</td>
										<td>0.552</td>
										<td>0.498</td>
										<td>0.522</td>
										<td>0.518</td>
										<td>0.495</td>
										<td>0.517</td>
									</tr>
									<tr>
										<td>o4-mini (+reason)</td>
										<td>0.521</td>
										<td>0.564</td>
										<td>0.522</td>
										<td>0.513</td>
										<td>0.530</td>
										<td>0.534</td>
										<td>0.510</td>
										<td>0.507</td>
										<td>0.512</td>
										<td>0.483</td>
										<td>0.509</td>
									</tr>
								</tbody>
							</table>
						</div>
					</div>
				</div>
			</div>
			</div>
			<section>
				<style>
					body {
					    font-family: Arial, sans-serif;
					    margin: 20px;
					  }
					  h1 {
					    margin-bottom: 20px;
					  }
					  .dialog-sample-group {
					    margin-bottom: 20px;
					  }
					  .dialog-sample-header {
						background: #0b6293;
						padding: 10px;
						cursor: pointer;
						user-select: none;
						width: 100%;       /* ÂÆΩÂ∫¶ÊíëÊª°Áà∂ÂÆπÂô® */
						color: white;      /* Â≠ó‰ΩìÈ¢úËâ≤ÁôΩËâ≤ */
						border-radius: 4px;
						margin-bottom: 10px;
						font-weight: bold; /* ËøôÈáåÊàëÂä†‰∏™Â≠ó‰ΩìÂä†Á≤óÔºåÁúãËµ∑Êù•Êõ¥ÈÜíÁõÆ */
						box-sizing: border-box; /* Á°Æ‰øùpaddingÁÆóÂú®ÂÆΩÂ∫¶ÂÜÖ */
						}
					
					  .dialog-content {
					    display: flex;
					    overflow-x: auto; /* Ê®™ÂêëÊªöÂä® */
					    padding-bottom: 10px;
					  }
					  .dialog-sample-item {
					    flex: 0 0 280px; /* Âõ∫ÂÆöÂÆΩÂ∫¶ */
					    max-height: 400px;
					    overflow-y: auto; /* ÂûÇÁõ¥ÊªöÂä® */
					    border: 1px solid #ccc;
					    border-radius: 6px;
					    padding: 10px;
					    margin-right: 10px;
					    background: #fff;
					    box-sizing: border-box;
					  }
					  .dialog-sample-item p {
					    margin: 8px 0;
					    white-space: normal;
					    word-break: break-word;
					  }
					  .dialog-sample-item ol {
					    padding-left: 20px;
					    margin-top: 5px;
					  }
					  .dialog-sample-item img {
					    max-width: 100%;
					    margin-top: 8px;
					    border-radius: 4px;
					  }
				</style>
				<h2 class="title is-3" id="Evaluation" style="text-align: center;"> Data Sample </h2>
				<div id="dialog-sample-container"></div>
				<script>
					const data = [
					    {
					        "group": "Group 1",
					        "examples": [
					          {
					            "model": "Janus",
					            "title": "Example 1",
					            "prompt": "What immediate actions would you recommend for the local authorities to take in response to the situation depicted in the image, and why?",
					            "response": "The image depicts a flooded area, with water covering the street and reflecting surrounding trees and buildings. The floodwater appears to be at a significant depth, potentially affecting traffic and access to homes. Immediate actions for local authorities to take in response to this situation would include: 1. Evacuation and Safety Measures: Ensure the safety of residents by providing alternative shelters and evacuation routes. This includes setting up emergency shelters and coordinating with local emergency services. 2. Flood Control and Drainage: Activate flood control measures such as opening storm drains and releasing water from nearby reservoirs or flood management systems to reduce water accumulation. 3. Communication and Coordination: Establish communication channels with affected residents to provide updates on the flood situation, safety instructions, and resources available. 4. Damage Assessment and Repair: Send teams to assess the extent of the damage and initiate repair efforts to restore infrastructure, such as roads, bridges, and utilities. 5. Environmental Monitoring: Monitor water quality and potential contamination risks to ensure public health and safety.",
					            "image": ".\\assets\\diag_sample\\image1.jpg"
					          },
					          {
					            "model": "Janus",
					            "title": "Example 2",
					            "prompt": "Please tell me how to build temporary roads or bridges, and generate one in the diagram.",
					            "response": "To build temporary roads or bridges, follow these steps: 1. Assessment and Planning: Evaluate the flood-affected area to determine the most suitable locations for temporary roads or bridges. Consider factors such as water depth, flow velocity, and structural stability. 2. Materials and Equipment: Gather materials like steel beams, concrete blocks, and construction tools. Ensure that the equipment is suitable for the water conditions, such as using waterproof materials and equipment. 3. Foundation Preparation: Prepare the ground by clearing debris and leveling the surface. Ensure that the foundation is stable and can support the weight of the temporary structure. 4. Construction: Construct the temporary road or bridge using the prepared materials. Use strong materials to ensure durability and safety. 5. Testing and Maintenance: Test the temporary structure for stability and safety. Regularly maintain it to ensure it remains functional and safe for use.",
					            "image": ".\\assets\\diag_sample\\image2.jpg"
					          },
					          {
					            "model": "Janus",
					            "title": "Example 3",
					            "prompt": "How long does it take to build such a bridge?",
					            "response": "Building a temporary bridge or road in flood-affected areas requires careful planning and execution. The time taken depends on several factors, including the complexity of the structure, the depth of the water, and the availability of materials and equipment. 1. Planning and Assessment: This phase typically takes a few days to a week. It involves assessing the flood conditions, determining the best location for the bridge or road, and planning the construction process. 2. Material Procurement: Gathering materials such as steel beams, concrete blocks, and construction tools can take a few days. 3. Construction: Building the bridge or road can take several weeks to a few months, depending on the complexity and the number of workers involved. 4. Testing and Maintenance: Once completed, the bridge or road needs to be tested for safety and durability. Regular maintenance is required to ensure its longevity. Overall, the construction of a temporary bridge or road in flood-affected areas can take anywhere from a few weeks to several months.",
					            "image": ".\\assets\\diag_sample\\image3.png"
					          },
					          {
					            "model": "Gemini",
					            "title": "Example 4",
					            "prompt": "What immediate actions would you recommend for the local authorities to take in response to the situation depicted in the image, and why?",
					            "response": "1. Issue Emergency Alerts and Warnings: Utilize all available channels (sirens, SMS, social media, radio, television) to inform residents about the dangerous flooding and advise them to seek higher ground immediately. Emphasize the risks of driving or walking through floodwaters. 2. Activate Emergency Response Teams: Deploy police, fire department, and emergency medical services to the affected areas. Their immediate priorities should be: Life Safety: Conducting search and rescue operations for individuals trapped in homes or vehicles; Medical Assistance: Providing aid to anyone injured; Security: Securing the flooded areas to prevent looting and ensure public safety. 3. Establish Shelters: Open designated emergency shelters (schools, community centers, etc.) for displaced residents, providing them with temporary housing, food, water, and essential supplies. 4. Assess the Situation and Infrastructure: Begin assessing the extent of the damage to roads, bridges, power lines, and other critical infrastructure. This will help in understanding the scope of the disaster and planning the subsequent response. 5. Block Off Affected Roads: Close roads leading into and within the flooded areas to prevent further traffic and potential accidents in the dangerous conditions.",
					            "image": ".\\assets\\diag_sample\\image4.png"
					          },
					          {
					            "model": "Gemini",
					            "title": "Example 5",
					            "prompt": "In addition to these immediate actions, what long-term strategies could the local authorities implement to prevent or mitigate future flooding incidents in this area?",
					            "response": "Implementing flood-prevention measures such as improving drainage systems, raising roads, emergency response plans, and regular maintenance checks.",
					            "image": ".\\assets\\diag_sample\\image5.png"
					          },
					          {
					            "model": "Gemini",
					            "title": "Example 6",
					            "prompt": "Could you elaborate on how the local authorities might go about 'improving the city‚Äôs drainage system'?",
					            "response": "1. Upgrading and Expanding Drainage Infrastructure, 2. Implementing Green Infrastructure, 3. Advanced Hydrological Modeling, 4. Sensor-Based Monitoring Systems, 5. Public Awareness and Engagement.",
					            "image": ".\\assets\\diag_sample\\image6.png"
					          }
					        ]
					    },
					    {
					        "group": "Group 2",
					        "examples": [
					          {
					            "model": "From Dataset",
					            "title": "Example 7",
					            "prompt": "What immediate actions would you recommend for the local authorities to take in response to the situation depicted in the image, and why?",
					            "response": "The image shows a flooded street with water reaching the curb and partially submerging parked vehicles. Prompt and coordinated evacuation is essential to ensure resident safety. Emergency services should be deployed to assist vulnerable populations, while barriers and signage can prevent vehicles from entering hazardous areas. In parallel, assessing drainage system blockages and deploying pumps could help reduce water levels and restore access. Continuous monitoring of water quality is also necessary to prevent health risks.",
					            "image": "#"
					          },
					          {
					            "model": "From Dataset",
					            "title": "Example 8",
					            "prompt": "In addition to these immediate actions, what long-term strategies could the local authorities implement to prevent or mitigate future flooding incidents in this area?",
					            "response": "Based on the previous response, I would recommend a combination of gray and green infrastructure upgrades. This includes enlarging stormwater drains, constructing retention basins, and restoring wetlands to act as natural buffers. Implementing permeable pavements and urban green spaces can help absorb runoff. Regular maintenance schedules for clearing debris and public education campaigns on proper waste disposal will further ensure the resilience of the flood management system.",
					            "image": "#"
					          },
					          {
					            "model": "From Dataset",
					            "title": "Example 9",
					            "prompt": "That's a great overview. Could you elaborate on how the local authorities might go about 'improving the city's drainage system'? What specific improvements or technologies might be considered?",
					            "response": "To improve the city's drainage system, authorities can retrofit existing pipes with larger-diameter conduits and install smart sensors to monitor flow rates in real time. Incorporating automated pump stations that activate during peak flows ensures rapid response. Advanced hydrological modeling can identify bottlenecks, guiding targeted upgrades. Finally, integrating rainwater harvesting systems in public buildings can reduce the burden on sewers during heavy storms.",
					            "image": ".\\assets\\diag_sample\\image6.png"
					          },
					          {
					            "model": "Gemini",
					            "title": "Example 10",
					            "prompt": "Considering the cleanliness and arrangement, what can you infer about the maintenance of this bathroom?",
					            "response": "The bathroom appears meticulously cleaned and well-organized, with spotless surfaces and neatly arranged toiletries. The grout lines are free of mold, and no clutter is visible on countertops, indicating regular upkeep by diligent maintenance staff or homeowners. Such attention to detail suggests a strong commitment to hygiene and order.",
					            "image":".\\assets\\diag_sample\\image7.png"
					          },
					          {
					            "model": "Gemini",
					            "title": "Example 11",
					            "prompt": "The bathroom image you generated looks a bit cluttered and cramped. What additions could enhance the bathroom's functionality or aesthetics?",
					            "response": "To improve functionality and aesthetics, consider adding wall-mounted shelving or recessed niches for storage to clear countertop space. A larger mirror with integrated LED lighting can visually expand the space and boost illumination. Slimline cabinets under the sink would hide essentials, while decorative accents‚Äîsuch as a stylish soap dispenser or potted plant‚Äîadd personality without overcrowding.",
					            "image": ".\\assets\\diag_sample\\image8.png"
					          },
					          {
					            "model": "Gemini",
					            "title": "Example 12",
					            "prompt": "That's helpful! Considering the potential for adding items, how would you prioritize these additions based on cost-effectiveness and impact on the bathroom's overall appeal?",
					            "response": "First, install wall-mounted shelves or storage niches‚Äîthese are low-cost but free up valuable counter space. Next, upgrade lighting with an LED-backlit mirror to enhance brightness and modernize the look. Then, add slimline cabinetry under the sink for hidden storage. Decorative touches like a soap dispenser and small plant come last, as they have lower impact on functionality but boost aesthetic appeal.",
					            "image": ".\\assets\\diag_sample\\image9.png"
					          },
					          {
					            "model": "Janus",
					            "title": "Example 13",
					            "prompt": "Considering the cleanliness and arrangement, what can you infer about the maintenance of this bathroom?",
					            "response": "The bathroom appears to be well-maintained, with uniformly clean surfaces and minimal clutter. The arrangement of toiletries and towels is orderly, suggesting that regular cleaning routines are followed. Overall, the space reflects a high standard of hygiene and organization.",
					            "image": ".\\assets\\diag_sample\\image10.png"
					          },
					          {
					            "model": "Janus",
					            "title": "Example 14",
					            "prompt": "The toilet picture you generated looks dark and plain. What additions could enhance the bathroom's functionality or aesthetics?",
					            "response": "To enhance the space:\n1. **Lighting:** Add a brighter, modern fixture above the mirror to improve visibility.\n2. **Storage Solutions:** Install floating shelves or a slim cabinet for toiletries.\n3. **Decorative Elements:** Introduce a framed mirror or artwork to add character.\n4. **Color Scheme:** Apply an accent tile backsplash or fresh paint to liven the walls.\n5. **Mirror Frame:** Add a decorative frame around the mirror for a cohesive, elegant look.",
					            "image": ".\\assets\\diag_sample\\image11.png"
					          },
					          {
					            "model": "Janus",
					            "title": "Example 15",
					            "prompt": "Please present the complete scene of this toilet to me, not just the sink.",
					            "response": "The complete scene includes a compact vanity with a sleek faucet, a pedestal toilet with a soft-close lid, and a corner-mounted towel rack. The floor features light-gray porcelain tiles, while walls are painted in a soft off-white. A small potted plant sits on the windowsill to add a touch of greenery, making the space both functional and inviting.",
					            "image":".\\assets\\diag_sample\\image12.png"
					          },
					          {
					            "model": "From Dataset",
					            "title": "Example 16",
					            "prompt": "Considering the cleanliness and arrangement, what can you infer about the maintenance of this bathroom?",
					            "response": "Based on the cleanliness of the sink area and the orderly arrangement of items, the bathroom seems to be maintained regularly. Surfaces are free of stains, and fixtures show no signs of wear, indicating consistent care and upkeep.",
					            "image": ".\\assets\\diag_sample\\image13.png"
					          },
					          {
					            "model": "From Dataset",
					            "title": "Example 17",
					            "prompt": "You mentioned that the bathroom might need to be updated. What additions could enhance the bathroom's functionality or aesthetics?",
					            "response": "Updating or finishing the empty wall space with floating shelves or a slim cabinet could add storage without crowding the floor. A new mirror above the sink would improve both function and visual space. Simple decorative elements‚Äîlike a patterned rug or matching soap dispenser‚Äîcan add warmth and personality.",
					            "image": ".\\assets\\diag_sample\\image14.png"
					          },
					          {
					            "model": "From Dataset",
					            "title": "Example 18",
					            "prompt": "That's helpful! Considering the potential for adding items, how would you prioritize these additions based on cost-effectiveness and impact on the bathroom's overall appeal?",
					            "response": "1. **Additional Storage:** Floating shelves offer high utility at low cost.\n2. **Mirror Above Sink:** Increases perceived space and brightness.\n3. **Decorative Elements:** Small accents like soap dispensers add style affordably.\n4. **Lighting Upgrade:** LED fixtures save energy and improve ambiance.\n5. **Tile Accents:** A backsplash refreshes the look without full renovation.",
					            "image": ".\\assets\\diag_sample\\image15.jpg"
					          }
					        ]
					      }
					      
					];
					
					// Ëß£ÊûêÂÜÖÂµåÁºñÂè∑ÂàóË°®Ôºà‰æãÂ¶Ç‚Äú1. xxx 2. xxx‚ÄùÔºâ
					function splitInlineNumberedList(text) {
					  const listStart = text.search(/\d+\.\s/);
					  if (listStart === -1) return { intro: text.trim(), list: null };
					
					  const intro = text.slice(0, listStart).trim();
					  const pointsStr = text.slice(listStart);
					
					  // ‰ΩøÁî®Ê≠£ÂàôÂåπÈÖçÁºñÂè∑È°πÔºåÊîØÊåÅÊç¢Ë°åÂíå‰∏≠ÊñáÈÄóÂè∑ÁªìÂ∞æÊ∏ÖÁêÜ
					  const regex = /\d+\.\s([\s\S]*?)(?=\d+\.|$)/g;
					  let match;
					  const list = [];
					  while ((match = regex.exec(pointsStr)) !== null) {
					    list.push(match[1].trim().replace(/[,Ôºå]\s*$/, ''));
					  }
					  return { intro, list };
					}
					
					// ÊâìÂ≠óÊú∫ÊïàÊûúÔºåÊîØÊåÅÁÆÄÂçïHTMLÊ†áÁ≠æÔºà‰∏çÊãÜÂàÜÊ†áÁ≠æÔºå‰ªÖÂ§ÑÁêÜÊñáÊú¨Ôºâ
					function typeWriter(target, htmlText, speed = 20, callback = null) {
					  let i = 0;
					  const text = htmlText;
					  target.innerHTML = '';
					
					  function type() {
					    if (i < text.length) {
					      target.innerHTML += text.charAt(i);
					      i++;
					      setTimeout(type, speed);
					    } else if (callback) {
					      callback();
					    }
					  }
					  type();
					}
					
					// Ê∏≤ÊüìÂçïÊù°ÂØπËØùÔºåÊâìÂ≠óÊú∫ÂêéÊòæÁ§∫ÂõæÁâáÂíåÂàóË°®
					function renderDialogItem(item, container, doneCallback) {
					  const dialogItem = document.createElement('div');
					  dialogItem.className = 'dialog-sample-item';
					
					  const promptP = document.createElement('p');
					  promptP.innerHTML = `<strong>Q:</strong> ${item.prompt}`;
					  dialogItem.appendChild(promptP);
					
					  const answerDiv = document.createElement('div');
					  answerDiv.innerHTML = '<strong>A:</strong> ';
					  dialogItem.appendChild(answerDiv);
					
					  const { intro, list } = splitInlineNumberedList(item.response);
					
					  const span = document.createElement('span');
					  answerDiv.appendChild(span);
					
					  typeWriter(span, intro + (list ? ' ' : ''), 20, () => {
					    if (list) {
					      const ol = document.createElement('ol');
					      list.forEach(point => {
					        const li = document.createElement('li');
					        li.textContent = point;
					        ol.appendChild(li);
					      });
					      answerDiv.appendChild(ol);
					    }
					    if (item.image && item.image !== "#") {
					      const img = document.createElement('img');
					      img.src = item.image;
					      img.alt = 'Response Image';
					      dialogItem.appendChild(img);
					    }
					    doneCallback();
					  });
					
					  container.appendChild(dialogItem);
					}
					
					// Ê†πÊçÆÊ®°ÂûãÂàÜÁ±ªÔºåÁîüÊàêÊäòÂè†Èù¢ÊùøÔºåÁÇπÂáªÂä†ËΩΩÂÜÖÂÆπ
					function renderDialogs() {
					  const container = document.getElementById('dialog-sample-container');
					  container.innerHTML = '';
					
					  // Êî∂ÈõÜÊâÄÊúâÁ§∫‰æãÂà∞‰∏Ä‰∏™Êï∞ÁªÑÔºåÊñπ‰æøËøáÊª§
					  let allExamples = [];
					  data.forEach(group => {
					    group.examples.forEach(ex => {
					      allExamples.push(ex);
					    });
					  });
					
					  // ÈúÄË¶ÅÂ±ïÁ§∫ÁöÑÊ®°ÂûãÂàóË°®
					  const models = ["Janus", "Gemini"];
					
					  models.forEach(model => {
					    const groupDiv = document.createElement('div');
					    groupDiv.className = 'dialog-sample-group';
					
					    const header = document.createElement('div');
					    header.className = 'dialog-sample-header';
					    header.textContent = `${model} - Click to Expand`;
					
					    const content = document.createElement('div');
					    content.className = 'dialog-content';
					    content.style.display = 'none'; // ÈªòËÆ§ÊäòÂè†
					
					    groupDiv.appendChild(header);
					    groupDiv.appendChild(content);
					    container.appendChild(groupDiv);
					
					    let loaded = false;  // Èò≤Ê≠¢Â§öÊ¨°Âä†ËΩΩ
					
					    header.addEventListener('click', () => {
					      if (content.style.display === 'flex') {
					        content.style.display = 'none';
					      } else {
					        content.style.display = 'flex';
					        if (!loaded) {
					          // ÊåâÊ®°ÂûãËøáÊª§Êï∞ÊçÆ
					          const filteredExamples = allExamples.filter(e => e.model === model);
					          let index = 0;
					
					          function next() {
					            if (index >= filteredExamples.length) return;
					            renderDialogItem(filteredExamples[index], content, () => {
					              index++;
					              next();
					            });
					          }
					          next();
					          loaded = true;
					        }
					      }
					    });
					  });
					}
					
					// È°µÈù¢DOMÂä†ËΩΩÂÆåÊàêÂêéÊâßË°å
					document.addEventListener('DOMContentLoaded', renderDialogs);
				</script>
			</section>
			<!-- @PAN TODO: bibtex -->
			<section class="section" id="BibTeX">
				<div class="container is-max-desktop content">
					<h2 class="title is-3 has-text-centered">BibTeX</h2>
					<pre><code>@article{chen2025intermt,
  title={InterMT: Multi-Turn Interleaved Preference Alignment with Human Feedback},
  author={Boyuan Chen and Donghai Hong and Jiaming Ji and Jiacheng Zheng and Bowen Dong and Jiayi Zhou and Kaile Wang and Josef Dai and Xuyao Wang and Wenqi Chen and Qirui Zheng and Wenxin Li and Sirui Han and Yike Guo and Yaodong Yang},
  year={2025},
  institution={Peking University and Hong Kong University of Science and Technology},
  url={https://pku-intermt.github.io},
  keywords={Multimodal Learning, Multi-Turn Interaction, Human Feedback, Preference Alignment}
}</code></pre>
				</div>
			</section>
			<section>
				<div class="section" id="org-banners" style="display: flex; align-items: center;">
					<a href="https://www.pku.edu.cn/" target="_blank" rel="external" style="margin-right: 20px;">
						<img class="center-block org-banner" src="./assets/PKU_logo.png" style="width: 150px; height: 150px;">
					</a>
					<a href="https://hkust.edu.hk/" target="_blank" class="ext-link" style="margin-left: 20px;">
						<img class="center-block org-banner" src="./assets/HKUST-STD.svg" style="width: 400px; height: 200px;">
					</a>
				</div>
			</section>
			<footer class="footer">
				<!-- <div class="container"> -->
				<div class="content has-text-centered">
				</div>
				<div class="columns is-centered">
					<div class="column is-8">
						<div class="content">
							<p> This website is website adapted from <a href="https://nerfies.github.io/">Nerfies</a>, licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>. </p>
						</div>
					</div>
				</div>
				<!-- </div> -->
			</footer>
			</div>
		</section>
		</body>
</html>
