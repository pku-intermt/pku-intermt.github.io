<!DOCTYPE html>
<html class="fontawesome-i2svg-active fontawesome-i2svg-complete">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<style data-merge-styles="true"></style>
	<style data-merge-styles="true"></style>
	<style data-merge-styles="true"></style>
	<meta name="description" content="Multi-Turn Interleaved Preference Alignment with Human Feedback">
	<meta name="keywords" content="Multi-Turn, Multi-Modality">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>InterMT: Multi-Turn Interleaved Preference Alignment with Human Feedback</title>
	<link rel="icon" href="./assets/logo_transp.png" type="image/png">
	<!-- ÂºïÂÖ•cssÊñá‰ª∂ -->
	<link href="./assets/css" rel="stylesheet">
	<link rel="stylesheet" href="./assets/bulma.min.css">
	<link rel="stylesheet" href="./assets/bulma-carousel.min.css">
	<link rel="stylesheet" href="./assets/font-face.css">
	<link rel="stylesheet" href="./assets/bulma-slider.min.css">
	<link rel="stylesheet" href="./assets/fontawesome.all.min.css">
	<link rel="stylesheet" href="./assets/academicons.min.css">
	<link rel="stylesheet" href="./assets/index.css">
	<link rel="stylesheet" href="./assets/leaderboard.css">
	<style>
		p, ul {
		text-align: justify;
		margin-left: auto;
		margin-right: auto;
		width: 75%;
		list-style:disc;
		margin-bottom: 10px; /* ËÆæÁΩÆÊÆµËêΩ‰πãÈó¥ÁöÑÈó¥Ë∑ù */
		}
		li {
		margin-bottom: 10px;
		}
	</style>
	<style>
		.data-card {
		background-color: #f8f9fa;
		border: 1px solid #e1e5e9;
		border-radius: 12px;
		padding: 25px;
		margin: 25px 0;
		box-shadow: 0 4px 8px rgba(0,0,0,0.1);
		transition: transform 0.3s ease, box-shadow 0.3s ease;
		}
		.data-card:hover {
		transform: translateY(-5px);
		box-shadow: 0 8px 16px rgba(0,0,0,0.15);
		}
		.data-card h4 {
		margin-top: 0;
		color: #2c3e50;
		border-left: none;
		padding-left: 0;
		display: flex;
		align-items: center;
		}
		.data-card h4::before {
		content: 'üìä';
		margin-right: 10px;
		font-size: 1.2em;
		}
		.data-card ul {
		margin: 15px 0;
		padding-left: 20px;
		}
		.data-card li {
		margin-bottom: 12px;
		text-align: justify;
		position: relative;
		}
		.data-card li::marker {
		color: #1b2f3d;
		font-weight: bold;
		}
		.dimensions-table {
		margin: 25px 0;
		background-color: #f8f9fa;
		padding: 20px;
		border-radius: 12px;
		box-shadow: 0 4px 8px rgba(0,0,0,0.1);
		overflow: hidden;
		}
		.dimensions-table h4 {
		margin-top: 0;
		text-align: center;
		color: #2c3e50;
		border-left: none;
		padding-left: 0;
		}
		.dimensions-table table {
		width: 100%;
		border-collapse: collapse;
		background-color: white;
		border-radius: 8px;
		overflow: hidden;
		box-shadow: 0 2px 4px rgba(0,0,0,0.1);
		}
		.dimensions-table th {
		background: linear-gradient(135deg, #3498db, #2980b9);
		color: white;
		padding: 15px;
		text-align: left;
		font-weight: bold;
		border-bottom: 3px solid #2980b9;
		position: relative;
		}
		.dimensions-table th::after {
		content: '';
		position: absolute;
		top: 0;
		left: 0;
		right: 0;
		bottom: 0;
		background: rgba(255,255,255,0.1);
		opacity: 0;
		transition: opacity 0.3s ease;
		}
		.dimensions-table th:hover::after {
		opacity: 1;
		}
		.dimensions-table td {
		padding: 12px 15px;
		border-bottom: 1px solid #ecf0f1;
		color: #2c3e50;
		font-weight: 500;
		transition: background-color 0.3s ease;
		}
		.dimensions-table tr:nth-child(even) {
		background-color: #f8f9fa;
		}
		.dimensions-table tr:hover {
		background-color: #e8f4f8;
		transform: scale(1.02);
		box-shadow: 0 2px 8px rgba(52, 152, 219, 0.2);
		}
		.dimensions-table tr:last-child td {
		border-bottom: none;
		}
		.section-intro {
		background: linear-gradient(135deg, #e8f4f8, #dbeafe);
		padding: 25px;
		border-radius: 12px;
		margin-bottom: 30px;
		border-left: 6px solid #3498db;
		position: relative;
		overflow: hidden;
		}
		.section-intro::before {
		content: '';
		position: absolute;
		top: 0;
		left: 0;
		right: 0;
		bottom: 0;
		background: url('data:image/svg+xml;utf8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 100 100"><defs><pattern id="grid" width="10" height="10" patternUnits="userSpaceOnUse"><path d="M 10 0 L 0 0 0 10" fill="none" stroke="%23ffffff" stroke-width="0.5" opacity="0.1"/></pattern></defs><rect width="100" height="100" fill="url(%23grid)"/></svg>');
		pointer-events: none;
		}
		.math-formula {
		background-color: #f8f9fa;
		padding: 15px;
		border-radius: 8px;
		border-left: 4px solid #3498db;
		margin: 20px 0;
		font-family: 'Courier New', monospace;
		}
		.warning {
		background-color: #fff3cd;
		border: 1px solid #ffeaa7;
		color: #856404;
		padding: 15px;
		border-radius: 8px;
		margin: 20px 0;
		border-left: 4px solid #f39c12;
		}
		.info {
		background-color: #d1ecf1;
		border: 1px solid #bee5eb;
		color: #0c5460;
		padding: 15px;
		border-radius: 8px;
		margin: 20px 0;
		border-left: 4px solid #17a2b8;
		}
		.success {
		background-color: #d4edda;
		border: 1px solid #c3e6cb;
		color: #155724;
		padding: 15px;
		border-radius: 8px;
		margin: 20px 0;
		border-left: 4px solid #28a745;
		}
		/* Âä†ËΩΩÂä®Áîª */
		.loading {
		display: inline-block;
		width: 20px;
		height: 20px;
		border: 3px solid rgba(52, 152, 219, 0.3);
		border-radius: 50%;
		border-top-color: #3498db;
		animation: spin 1s ease-in-out infinite;
		}
		@keyframes spin {
		to { transform: rotate(360deg); }
		}
		/* ÂìçÂ∫îÂºèËÆæËÆ° */
		@media (max-width: 768px) {
		body {
		padding: 15px;
		}
		.container {
		padding: 20px;
		}
		.dimensions-table table {
		font-size: 14px;
		}
		.dimensions-table th,
		.dimensions-table td {
		padding: 10px 8px;
		}
		.data-card {
		padding: 15px;
		}
		}
		@media (max-width: 480px) {
		.dimensions-table {
		overflow-x: auto;
		}
		.dimensions-table table {
		min-width: 500px;
		}
		}
		/* ÊâìÂç∞Ê†∑Âºè */
		@media print {
		body {
		background-color: white;
		padding: 0;
		}
		.container {
		box-shadow: none;
		padding: 20px;
		}
		.data-card,
		.dimensions-table {
		box-shadow: none;
		border: 1px solid #ccc;
		}
		.data-card:hover,
		.dimensions-table tr:hover {
		transform: none;
		box-shadow: none;
		}
		}
	</style>
	<style>
		/* Custom styles for the modal */
		.modal {
		display: none;
		position: fixed;
		z-index: 50;
		left: 0;
		top: 0;
		width: 100%;
		height: 100%;
		overflow: auto;
		background-color: rgba(0, 0, 0, 0.5);
		}
		.modal-content {
		background-color: white;
		margin: 5% auto;
		padding: 20px;
		border: 1px solid #888;
		width: 90%;
		max-width: 800px;
		border-radius: 8px;
		max-height: 80vh;
		overflow-y: auto;
		}
		.close {
		color: #aaa;
		float: right;
		font-size: 28px;
		font-weight: bold;
		cursor: pointer;
		}
		.close:hover {
		color: #000;
		}
		td {
		cursor: pointer;
		}
		td:hover {
		background-color: #e5e7eb;
		}
	</style>
	<!-- Ë°®Ê†ºÊ†∑ÂºècssÊñá‰ª∂ -->
	<style>
		/* Ë°®Ê†ºÂü∫Á°ÄÊ†∑Âºè */
		.data-table {
		width: 100%;
		border-collapse: collapse;
		margin: 20px 0;
		background-color: white;
		border: none;
		}
		/* Ë°®Ê†ºÂ∏ÉÂ±Ä */
		.data-table th,
		.data-table td {
		padding: 12px 15px;
		text-align: center;
		vertical-align: middle;
		border: none;
		}
		/* Ë°®Â§¥Ê†∑Âºè */
		.data-table thead {
		background-color: #2c3e50;
		}
		.data-table th {
		font-weight: bold;
		font-size: 14px;
		color: #ffffff;
		}
		/* Ë°®Ê†ºË°åÊ†∑Âºè */
		.data-table tbody tr:nth-child(even) {
		background-color: #f9f9f9;
		}
		.data-table tbody tr:hover {
		background-color: #f1f1f1;
		}
		/* ÂçïÂÖÉÊ†ºÂ≠ó‰ΩìÂ§ßÂ∞è */
		.data-table td {
		font-size: 13px;
		}
		/* ÁâπÊÆäÊ†∑ÂºèÁ±ª */
		.bold {
		font-weight: bold;
		}
		.highlight {
		color: #d02907;
		}
		/* Ë°®Ê†ºÂØπÈΩê */
		.center-table {
		margin-left: auto;
		margin-right: auto;
		}
	</style>
	<!-- ÂØπËØùcssÊñá‰ª∂ -->
	<style>
		.diag-dropdown-container {
		margin-bottom: 15px;
		padding: 10px;
		background-color: #fff;
		border-radius: 8px;
		box-shadow: 0 2px 4px rgba(0,0,0,0.1);
		}
		select {
		padding: 8px 12px;
		font-size: 16px;
		border-radius: 4px;
		border: 1px solid #ccc;
		}
		.diag-container {
		display: flex;
		gap: 15px;
		}
		.diag-section {
		flex: 1;
		border: 1px solid #ccc;
		border-radius: 8px;
		overflow: hidden;
		max-height: 85vh;
		display: flex;
		flex-direction: column;
		box-shadow: 0 2px 4px rgba(0,0,0,0.1);
		}
		.diag-section.diag-orange { background: #fff4e5; border-color: orange; }
		.diag-section.diag-default { background: #f9f9f9; border-color: #999; }
		.diag-section.diag-blue { background: #e7f3ff; border-color: #99c2ff; }
		.diag-section-header {
		padding: 12px 15px;
		font-weight: bold;
		cursor: pointer;
		user-select: none;
		background-color: #ddd;
		border-bottom: 1px solid #bbb;
		display: flex;
		align-items: center;
		}
		.diag-section-header svg, .diag-section-header img { /* SVGs/images within header */
		margin-right: 8px;
		width: 20px;
		height: 20px;
		vertical-align: middle;
		}
		.diag-section-header.diag-orange { background-color: #f7d399; border-color: orange; }
		.diag-section-header.diag-default { background-color: #e0e0e0; border-color: #ccc; }
		.diag-section-header.diag-blue { background-color: #b3d7ff; border-color: #99c2ff;}
		.diag-content {
		padding: 15px;
		overflow-y: auto;
		flex-grow: 1;
		background-color: #fff;
		}
		.diag-dialogue {
		margin-bottom: 20px;
		}
		.diag-prompt {
		text-align: right;
		background: #d0e6ff;
		padding: 10px 14px;
		border-radius: 15px 15px 0 15px;
		margin-bottom: 8px;
		max-width: 85%;
		margin-left: auto;
		font-weight: 600;
		word-wrap: break-word;
		}
		.diag-response {
		text-align: left;
		background: #eaeaea;
		padding: 10px 14px;
		border-radius: 15px 15px 15px 0;
		max-width: 85%;
		position: relative;
		white-space: normal;
		font-size: 14px;
		word-wrap: break-word;
		}
		.diag-response-text-area {
		white-space: pre-wrap;
		}
		.diag-prompt img, .diag-response img {
		max-width: 100%;
		margin-top: 10px;
		border-radius: 6px;
		display: block;
		}
		ol {
		padding-left: 25px;
		margin-top: 5px;
		margin-bottom: 5px;
		}
		ol li {
		margin-bottom: 5px;
		}
		p {
		margin-top: 5px;
		margin-bottom: 10px;
		}
		strong {
		font-weight: bold;
		}
		.diag-typing-caret::after {
		content: '|';
		animation: blink 1s step-end infinite;
		margin-left: 2px;
		font-weight: bold;
		}
		@keyframes blink {
		50% { opacity: 0; }
		}
	</style>
	<style>
		.modal {
		display: none;
		position: fixed;
		z-index: 1;
		left: 0;
		top: 0;
		width: 100%;
		height: 100%;
		overflow: auto;
		background-color: rgb(0,0,0);
		background-color: rgba(0,0,0,0.9);
		}
		.modal-content {
		margin: 15% auto;
		padding: 20px;
		width: 80%;
		max-width: 700px;
		}
		.close {
		color: #f1f1f1;
		float: right;
		font-size: 28px;
		font-weight: bold;
		}
		.close:hover,
		.close:focus {
		color: #000;
		text-decoration: none;
		cursor: pointer;
		}
	</style>
	<style>
		p, li, strong {
		text-align: left;
		}
	</style>
	<!-- ÂºïÂÖ•jsÊñá‰ª∂ -->
	<script>
		window.MathJax = {
		    tex: {
		        inlineMath: [['$', '$'], ['\\(', '\\)']],
		        displayMath: [['$$', '$$'], ['\\[', '\\]']],
		        packages: {'[+]': ['ams', 'newcommand', 'configmacros']}
		    },
		    options: {
		        processHtmlClass: 'tex2jax_process',
		        processScriptType: 'text/tex',
		        ignoreHtmlClass: 'tex2jax_ignore'
		    },
		    startup: {
		        ready: function () {
		            MathJax.startup.defaultReady();
		            console.log('MathJax is loaded and ready to go!');
		        }
		    }
		};
	</script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js"></script>
	<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
	<script type="text/javascript" src="./assets/sort-table.js" defer=""></script>
	<script src="./assets/jquery.min.js"></script>
	<script defer="" src="./assets/fontawesome.all.min.js"></script>
	<script src="./assets/bulma-carousel.min.js"></script>
	<script src="./assets/bulma-slider.min.js"></script>
	<script src="./assets/explorer-index.js"></script>
	<script src="./assets/question_card.js"></script>
	<script src="./assets/leaderboard_testmini.js"></script>
	<script src="./assets/output_folders.js" defer=""></script>
	<script src="./assets/model_scores.js" defer=""></script>
	<script src="./assets/data_public.js" defer=""></script>
</head>
<nav2 class="navbar" role="navigation" aria-label="main navigation">
	<div class="navbar-brand">
		<a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
			<span aria-hidden="true"></span>
			<span aria-hidden="true"></span>
			<span aria-hidden="true"></span>
		</a>
	</div>
	<div class="navbar-menu">
		<div class="navbar-start" style="flex-grow: 1; justify-content: center;">
			<!-- @PAN TODO: consider adding links? -->
			<div class="navbar-item has-dropdown is-hoverable">
				<a class="navbar-link">
					<span class="icon">
						<svg xmlns="http://www.w3.org/2000/svg" xml:space="preserve" width="512" height="501" shape-rendering="geometricPrecision" text-rendering="geometricPrecision" image-rendering="optimizeQuality" fill-rule="evenodd" clip-rule="evenodd" viewBox="0 0 512 500.59">
							<path fill-rule="nonzero"
								d="m107.22 313.44-23.58 55.2a7.338 7.338 0 0 1-6.57 4.47l-59.32 5.33 45.22 39.48c2.1 1.84 2.9 4.62 2.32 7.17l-13.36 58.52 51.51-30.8c2.4-1.44 5.3-1.34 7.54 0l51.53 30.8-5.87-25.66c4.88.55 9.91.19 14.8-1.15l6.68 29.31c.19 1.27.19 2.58-.02 3.89a12.8 12.8 0 0 1-1.62 4.47 12.559 12.559 0 0 1-7.71 5.73c-3.1.78-6.5.37-9.49-1.41l-52.07-31.13-51.62 30.88c-1.12.74-2.38 1.29-3.73 1.63l-.94.23c-1.45.26-2.99.26-4.55-.04l-.5-.09c-3.34-.78-6.08-2.85-7.78-5.56a12.573 12.573 0 0 1-1.67-9.24l13.57-59.39-45.68-39.89a12.566 12.566 0 0 1-2.94-3.78 12.6 12.6 0 0 1-1.29-4.2l-.05-.65c-.22-3.31.91-6.45 2.94-8.87 2.01-2.39 4.9-4.05 8.2-4.42l60.67-5.41 23.78-55.71c.62-1.47 1.54-2.85 2.7-4.02.94-.95 2.04-1.76 3.3-2.39.44-.24.91-.44 1.4-.6 2.96-1.06 6.12-.91 8.87.19 2.75 1.11 5.12 3.19 6.54 6.01l18.82 44.01-5.47 23.93c-.4-.5-.72-1.05-.97-1.64l-23.59-55.2zm137.19-302.2c0-6.2 5.03-11.24 11.24-11.24 6.2 0 11.24 5.04 11.24 11.24V62c0 6.2-5.04 11.24-11.24 11.24-6.21 0-11.24-5.04-11.24-11.24V11.24zM40.94 151.78c-5.7-2.4-8.38-8.96-5.98-14.66 2.4-5.7 8.97-8.38 14.67-5.98l46.73 19.8c5.7 2.4 8.38 8.97 5.98 14.66-2.4 5.7-8.97 8.38-14.67 5.98l-46.73-19.8zm47.52-72.84c-4.36-4.39-4.34-11.49.05-15.85 4.39-4.36 11.49-4.34 15.85.05l35.7 36.08c4.36 4.39 4.34 11.49-.05 15.85-4.39 4.36-11.49 4.34-15.85-.05l-35.7-36.08zm71.65-49.34c-2.35-5.72.38-12.27 6.1-14.62 5.73-2.35 12.27.38 14.63 6.1l19.3 46.95c2.35 5.72-.39 12.27-6.11 14.62-5.72 2.35-12.27-.38-14.62-6.1l-19.3-46.95zm302.26 101.54c5.7-2.4 12.27.28 14.67 5.98 2.4 5.7-.28 12.26-5.98 14.66l-46.73 19.8c-5.7 2.4-12.27-.28-14.67-5.98-2.4-5.69.28-12.26 5.98-14.66l46.73-19.8zm-54.73-68c4.36-4.39 11.46-4.41 15.85-.05 4.39 4.36 4.41 11.46.05 15.85l-35.7 36.08c-4.36 4.39-11.46 4.41-15.85.05-4.39-4.36-4.41-11.46-.05-15.85l35.7-36.08zm-76.48-42.06c2.36-5.72 8.9-8.45 14.63-6.1 5.72 2.35 8.45 8.9 6.1 14.62l-19.3 46.95c-2.35 5.72-8.9 8.45-14.62 6.1-5.72-2.35-8.46-8.9-6.11-14.62l19.3-46.95zm-53.09 110.8 33.5 78.45 83.93 7.53c.77 0 1.53.06 2.27.16 6.17.76 11.54 3.87 15.3 8.34l1.22 1.67c2.97 4.22 4.6 9.39 4.32 14.82l-.2 2.03c-.37 2.69-1.19 5.23-2.36 7.54l-.93 1.6c-.84 1.36-1.81 2.62-2.89 3.79-.82.97-1.76 1.85-2.79 2.6l-63.56 55.5 19.33 84.72c1.09 6.02-.21 11.99-3.28 16.87a23.685 23.685 0 0 1-14.82 10.51c-3.12.74-6.32.78-9.41.23l-.01.06c-3.19-.57-6.23-1.79-8.91-3.54L256 381.22l-74.01 44.25a23.716 23.716 0 0 1-17.57 2.45c-5.92-1.49-11.25-5.23-14.61-10.81l-.9-1.71a23.7 23.7 0 0 1-2.18-6.9c-.48-3.03-.37-6.08.29-8.96l19.1-83.63-64.6-56.4a23.796 23.796 0 0 1-8.09-16.36c-.39-6 1.52-12.21 5.85-17.16a23.87 23.87 0 0 1 7.47-5.73c2.81-1.4 5.93-2.19 9.12-2.37l84.56-7.56 33.69-78.88a23.734 23.734 0 0 1 13.02-12.75c12.18-4.91 25.86 1.03 30.83 12.93l.1.25zM416.44 303.3l23.72 55.56 60.67 5.41c3.3.37 6.19 2.03 8.2 4.42 2.03 2.42 3.16 5.56 2.94 8.87l-.05.65a12.6 12.6 0 0 1-1.29 4.2c-.72 1.43-1.73 2.72-2.94 3.78l-45.68 39.89 13.57 59.39c.68 3.29 0 6.58-1.67 9.24-1.7 2.71-4.44 4.78-7.78 5.56l-.5.09c-1.56.3-3.1.3-4.55.04l-.94-.23c-1.35-.34-2.61-.89-3.73-1.63l-51.63-30.88-52.06 31.13a12.513 12.513 0 0 1-9.49 1.41c-3.1-.78-5.93-2.76-7.71-5.73-.84-1.41-1.38-2.93-1.63-4.45l-.01-.08c-.21-1.25-.2-2.54 0-3.81l6.69-29.36c1.09.3 2.21.55 3.33.75 3.75.68 7.62.84 11.46.41l-5.87 25.68 51.52-30.8c2.25-1.34 5.14-1.44 7.54 0l51.52 30.8-13.37-58.52c-.57-2.55.23-5.33 2.33-7.17l45.22-39.48-59.32-5.33a7.338 7.338 0 0 1-6.57-4.47l-23.58-55.2-23.57 55.2c-.26.61-.61 1.18-1.02 1.69l-5.46-23.91 18.84-44.08c1.42-2.82 3.79-4.9 6.54-6.01 2.75-1.1 5.91-1.25 8.87-.19.49.16.96.36 1.4.6 1.26.63 2.36 1.44 3.3 2.39 1.16 1.17 2.08 2.55 2.7 4.02l.06.15z" />
							<path fill="#F7D345" fill-rule="nonzero"
								d="m263.04 137.82 37.45 87.7 95.01 8.52c4.19.37 7.29 4.06 6.93 8.25a7.59 7.59 0 0 1-2.57 5.07v.01l-71.87 62.75 21.26 93.02c.94 4.11-1.63 8.2-5.74 9.15-2.1.47-4.2.04-5.88-1.06L256 362.43l-81.89 48.96a7.627 7.627 0 0 1-10.46-2.64 7.602 7.602 0 0 1-.89-5.61h-.01l21.26-93.02-71.87-62.75c-3.17-2.77-3.49-7.59-.72-10.76a7.621 7.621 0 0 1 5.33-2.59l94.75-8.5 37.48-87.75c1.65-3.88 6.13-5.68 10.01-4.03 1.91.81 3.31 2.31 4.05 4.08z" />
						</svg>
					</span> &nbsp;&nbsp;More Research </a>
				<div class="navbar-dropdown">
					<a class="navbar-item" href="https://github.com/PKU-Alignment/safe-rlhf">
						<b>SafeRLHF (ICLR 2024 Spotlight)</b>
					</a>
					<a class="navbar-item" href="https://huggingface.co/datasets/PKU-Alignment/PKU-SafeRLHF">
						<b>PKU-SafeRLHF (ACL2025 Main)</b>
					</a>
					<a class="navbar-item" href="https://github.com/PKU-Alignment/aligner">
						<b>Aligner (NeurIPS 2024 Oral)</b>
					</a>
					<a class="navbar-item" href="https://github.com/PKU-Alignment/safe-sora">
						<b>SafeSora (NeurIPS 2024 DB Track)</b>
					</a>
					<a class="navbar-item" href="https://github.com/PKU-Alignment/beavertails">
						<b>BeaverTails (NeurIPS 2024 DB Track)</b>
					</a>
					<a class="navbar-item" href="https://align-anything.readthedocs.io/en/latest/index.html">
						<b>Align-Anythingüî•üî•üî•</b>
					</a>
				</div>
			</div>
		</div>
	</div>
</nav2>
<section class="hero" style="background-color: #1f5ceb;">
	<div class="hero-body">
		<div class="container is-max-desktop">
			<div class="columns is-centered">
				<div class="column has-text-centered">
					<h1 class="title is-1 publication-title" style="color: white;">
						<span class="img">
							<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
						</span>InterMT: Multi-Turn Interleaved Preference Alignment with Human Feedback
					</h1>
					<div class="is-size-5 publication-authors">
						<div class="is-size-5 publication-authors">
							<span class="author-block" style="color: white;"> Boyuan Chen<sup>1*</sup>Ôºå </span>
							<span class="author-block" style="color: white;"> Donghai Hong<sup>1*</sup>Ôºå </span>
							<span class="author-block" style="color: white;"> Jiaming Ji<sup>1*</sup>Ôºå </span>
							<span class="author-block" style="color: white;"> Jiacheng Zheng<sup>2</sup>Ôºå </span>
							<span class="author-block" style="color: white;"> Bowen Dong<sup>1</sup>Ôºå </span>
							<span class="author-block" style="color: white;"> Jiayi Zhou<sup>1</sup>Ôºå </span>
							<span class="author-block" style="color: white;"> Kaile Wang<sup>1</sup>Ôºå </span>
							<span class="author-block" style="color: white;"> Josef Dai<sup>1</sup>Ôºå </span>
							<span class="author-block" style="color: white;"> Xuyao Wang<sup>1</sup>Ôºå </span>
							<span class="author-block" style="color: white;"> Wenqi Chen<sup>1</sup>Ôºå </span>
							<span class="author-block" style="color: white;"> Qirui Zheng<sup>1</sup>Ôºå </span>
							<span class="author-block" style="color: white;"> Wenxin Li<sup>1</sup>Ôºå </span>
							<span class="author-block" style="color: white;"> Sirui Han<sup>2</sup>Ôºå </span>
							<span class="author-block" style="color: white;"> Yike Guo<sup>2</sup>Ôºå </span>
							<span class="author-block" style="color: white;"> Yaodong Yang<sup>1‚Ä†</sup>Ôºå </span>
						</div>
						<div class="is-size-5 publication-authors">
							<span class="author-block" style="color: white;">
								<sup>1</sup>
								<em>Peking University</em>
							</span>
							<span class="author-block" style="color: white;">
								<sup>2</sup>
								<em>The Hong Kong University of Science and Technology</em>
							</span>
						</div>
						<p style="text-align: center; color: white;">
							<small> * Equal contribution, <sup>‚Ä†</sup> corresponding author. </small>
						</p>
						<br>
					</div>
					<div class="column has-text-centered">
						<div class="publication-links">
							<!-- PDF Link. -->
							<span class="link-block">
								<a href="paper.pdf" class="external-link button is-normal is-rounded is-dark">
									<span class="icon">
										<svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg="">
											<path fill="currentColor"
												d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z">
											</path>
										</svg>
										<!-- <i class="fas fa-file-pdf"></i> Font Awesome fontawesome.com -->
									</span>
									<span>Paper</span>
								</a>
							</span>
							<span class="link-block">
								<a href="https://huggingface.co/datasets/PKU-Alignment/INTERMT" class="external-link button is-normal is-rounded is-dark">
									<span class="icon">
										<img src="./assets/huggingface-color.svg" alt="HuggingFace" style="width: 1em; height: 1em;" />
									</span>
									<span>InterMT</span>
								</a>
							</span>
							<!-- Video Link. -->
							<span class="link-block">
								<a href="https://github.com/cby-pku/INTERMT" class="external-link button is-normal is-rounded is-dark">
									<span class="icon">
										<svg class="svg-inline--fa fa-github fa-w-16" aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" data-fa-i2svg="">
											<path fill="currentColor"
												d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z">
											</path>
										</svg>
										<!-- <i class="fab fa-github"></i> Font Awesome fontawesome.com -->
									</span>
									<span>InterMT-Bench</span>
								</a>
							</span>
							<!-- Code Link. -->
							<span class="link-block">
								<a href="https://huggingface.co/PKU-Alignment/INTERMT-Judge" class="external-link button is-normal is-rounded is-dark">
									<span class="icon">
										<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512">
											<!--!Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2025 Fonticons, Inc.-->
											<path fill="#ffffff"
												d="M0 96C0 60.7 28.7 32 64 32l320 0c35.3 0 64 28.7 64 64l0 320c0 35.3-28.7 64-64 64L64 480c-35.3 0-64-28.7-64-64L0 96zm144 4c-24.3 0-44 19.7-44 44l0 48c0 24.3 19.7 44 44 44l32 0c24.3 0 44-19.7 44-44l0-48c0-24.3-19.7-44-44-44l-32 0zm-4 44c0-2.2 1.8-4 4-4l32 0c2.2 0 4 1.8 4 4l0 48c0 2.2-1.8 4-4 4l-32 0c-2.2 0-4-1.8-4-4l0-48zm140-44c-11 0-20 9-20 20c0 9.7 6.9 17.7 16 19.6l0 76.4c0 11 9 20 20 20s20-9 20-20l0-96c0-11-9-20-20-20l-16 0zM132 296c0 9.7 6.9 17.7 16 19.6l0 76.4c0 11 9 20 20 20s20-9 20-20l0-96c0-11-9-20-20-20l-16 0c-11 0-20 9-20 20zm96 24l0 48c0 24.3 19.7 44 44 44l32 0c24.3 0 44-19.7 44-44l0-48c0-24.3-19.7-44-44-44l-32 0c-24.3 0-44 19.7-44 44zm44-4l32 0c2.2 0 4 1.8 4 4l0 48c0 2.2-1.8 4-4 4l-32 0c-2.2 0-4-1.8-4-4l0-48c0-2.2 1.8-4 4-4z" />
										</svg>
									</span>
									<span>Model</span>
								</a>
							</span>
						</div>
					</div>
				</div>
			</div>
		</div>
	</div>
</section>
<br>
<section class="hero teaser">
	<div class="container is-max-desktop">
		<div class="content has-text-centered">
			<img src="./assets/figure1-0516_00.jpg" alt="geometric reasoning" width="100%" style="box-shadow: 0 4px 10px rgba(0, 0, 0, 0.3); border-radius: 8px;">
			<h2 style="text-align: center;">Abstract</h2>
			<p> As multimodal large models (MLLMs) continue to advance across challenging tasks, a key question emerges: <strong><em>What essential capabilities are still missing?</em></strong> A critical aspect of human learning is continuous interaction with the environment -- not limited to
				language, but also involving multimodal understanding and generation. To move closer to human-level intelligence, models must similarly support <strong>multi-turn</strong>, <strong>multimodal interaction</strong>. In particular, they should comprehend interleaved multimodal contexts
				and respond coherently in ongoing exchanges. In this work, we present <strong>an initial exploration</strong> through the <span class="icon">
					<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
				</span><strong>InterMT</strong> -- <strong>the first preference dataset for <em>multi-turn</em> multimodal interaction</strong>, grounded in real human feedback. In this exploration, we particularly emphasize the importance of human oversight, introducing expert annotations to guide
				the process, motivated by the fact that current MLLMs lack such complex interactive capabilities. <span class="icon">
					<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
				</span><strong>InterMT</strong> captures human preferences at both global and local levels into nine sub-dimensions, consists of 15.6k prompts, 52.6k multi-turn dialogue instances, and 32.4k human-labeled preference pairs. To compensate for the lack of capability for multi-modal
				understanding and generation, we introduce an agentic workflow that leverages tool-augmented MLLMs to construct multi-turn QA instances. To further this goal, we introduce <span class="icon">
					<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
				</span><strong>InterMT-Bench</strong> to assess the ability of MLLMs in assisting judges with multi-turn, multimodal tasks. We demonstrate the utility of <span class="icon">
					<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
				</span><strong>InterMT</strong> through applications such as judge moderation and further reveal the <em>multi-turn scaling law</em> of judge model. We hope the open-source of our data can help facilitate further research on aligning current MLLMs to the next step. </p>
		</div>
	</div>
</section>
<br>
<br>
<section class="hero is-light is-small">
	<div class="hero-body has-text-centered">
		<!-- <h1 class="title is-1 mathvista"><img src="static/images/mathvista.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>MathVista Dataset</h1> -->
		<h1 class="title is-1 mathvista">
			<!--<img src="static/images/mathvista.png" style="width:1em;vertical-align: middle" alt="Logo"/>-->
			<span class="mathvista" style="vertical-align: middle" id="introduction">Introduction</span>
		</h1>
	</div>
</section>
<section class="section">
	<div class="container" style="margin-bottom: 2vh;">
		<div class="columns is-centered has-text-centered">
			<div class="column is-four-fifths">
				<p>Humans perceive the world through dynamic, multimodal interactions involving text, images, audio, video, and more. Building on the success of multimodal large language models (MLLMs), recent efforts aim to develop general-purpose AI assistants that handle multiple mixed
					modalities. A key feature of such general-purpose assistants is to engage in natural multi-turn conversations, perceive and generate any modality, to enable more smooth interaction and grounded understanding. </p>
				<p>Recent years have seen community efforts in transplanting alignment techniques, such as Reinforcement Learning from Human Feedback (RLHF), from the text modality to multiple modalities settings. Within this line of research, most studies focus exclusively on either understanding
					or generation. The lack of alignment considerations for multimodal mixed input-output settings exacerbates the imbalance across modalities, i.e., modality disequilibrium. Furthermore, existing methods primarily focus on single-turn interactions, where an LLM generates a response
					from a prompt and receives immediate alignment feedback. However, real-world interactions typically occur in long-horizon conversations and often feature interleaved multimodal inputs and outputs. </p>
				<blockquote style="border:1px solid #ccc; padding:10px; background:#f9f9f9; margin:20px 0; text-align:center; font-style:italic; font-size:regular; "> How to improve <strong style="color:#8B0000;">multi-turn</strong> interleaved <strong
						style="color:#00008B;">understanding</strong>-<strong style="color:#006400;">generation</strong> alignment via human feedback? </blockquote>
				<p>
					<strong>
						<u style="color:#C6011F"> Our reflections highlight several key issues in the alignment of MLLMs: <svg class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="currentColor" viewBox="0 0 24 24">
								<path
									d="M13.849 4.22c-.684-1.626-3.014-1.626-3.698 0L8.397 8.387l-4.552.361c-1.775.14-2.495 2.331-1.142 3.477l3.468 2.937-1.06 4.392c-.413 1.713 1.472 3.067 2.992 2.149L12 19.35l3.897 2.354c1.52.918 3.405-.436 2.992-2.15l-1.06-4.39 3.468-2.938c1.353-1.146.633-3.336-1.142-3.477l-4.552-.36-1.754-4.17Z" />
							</svg>
						</u>
					</strong>
				</p>
				<ul>
					<li><strong>Modality Fusion via Harmonizing Understanding and Generation.</strong> To build general-purpose AI assistants, high-fidelity perception and understanding alone are not sufficient. The system should also support the selective generation of multimodal outputs to
						effectively communicate, instruct, or interact with users in a natural and contextually appropriate manner. </li>
					<li><strong>Modeling Long‚ÄêHorizon, Interleaved Multimodal Interactions.</strong> Real‚Äêworld user‚ÄìAI exchanges typically span many turns and interleave text, vision, and other modalities. Such interactions demand not only precise instruction following but also sustained attention
						and reasoning over an evolving context, approaching near‚Äêhuman in‚Äêcontext reasoning capabilities. </li>
					<li><strong>Dynamic Human‚Äêin‚Äêthe‚ÄêLoop Alignment.</strong> In extended, multimodal interactions, user preferences continually evolve. Capturing and aligning with these emergent, dynamic preferences calls for genuine, iterative human feedback throughout the interaction.</li>
				</ul>
				<p>In response, we introduce <span class="icon">
						<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
					</span>InterMT, a human preference dataset designed to capture the complexity and diversity of human intent in multi-turn settings. Specifically, <span class="icon">
						<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
					</span>InterMT targets vision-language interaction scenarios involving interleaved understanding and generation. To model dynamic human preferences, <span class="icon">
						<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
					</span>InterMT comprises <strong>
						<font color="red">15604</font>
					</strong> seed questions that elicit multi-turn, multimodal conversations spanning <strong>
						<font color="red">15+</font>
					</strong> domains. Helpfulness is then decomposed into <strong>
						<font color="red">9</font>
					</strong> sub-dimensions, capturing both global (conversation-level) and local (turn-level) aspects of human feedback. </p>
				<p>
					<strong>
						<u style="color:#C6011F"> Our key contributions are summarized as follows: <svg class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="currentColor" viewBox="0 0 24 24">
								<path
									d="M13.09 3.294c1.924.95 3.422 1.69 5.472.692a1 1 0 0 1 1.438.9v9.54a1 1 0 0 1-.562.9c-2.981 1.45-5.382.24-7.25-.701a38.739 38.739 0 0 0-.622-.31c-1.033-.497-1.887-.812-2.756-.77-.76.036-1.672.357-2.81 1.396V21a1 1 0 1 1-2 0V4.971a1 1 0 0 1 .297-.71c1.522-1.506 2.967-2.185 4.417-2.255 1.407-.068 2.653.453 3.72.967.225.108.443.216.655.32Z" />
							</svg>
						</u>
					</strong>
				</p>
				<ul>
					<li><strong>The First Multi-turn Interleaved Preference Dataset:</strong> To the best of our knowledge, <span class="icon">
							<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
						</span>InterMT is the first dataset that captures real human preferences for tasks involving multi-turn and interleaved multimodal understanding and generation. It contains <strong>
							<font color="red">15604</font>
						</strong> unique seed questions across diverse categories, <strong>
							<font color="red">52.6k</font>
						</strong> multi-turn interleaved vision-language QA instances, and <strong>
							<font color="red">32459</font>
						</strong> sets of multi-dimensional human preference annotations. </li>
					<li><strong>Agent-based Construction Workflow:</strong> <span class="icon">
							<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
						</span>InterMT employs a carefully designed agent-based multi-turn QA construction workflow that leverages strong MLLMs augmented with external tools to simulate high-quality real multi-turn interactions. </li>
					<li><strong>Decoupled Helpfulness in Multi-turn Multimodal Scenarios:</strong> <span class="icon">
							<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
						</span>InterMT decomposes the concept of helpfulness for multi-turn, multimodal interactions into two distinct levels: local (turn-level) and global (conversation-level). Furthermore, <span class="icon">
							<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
						</span>InterMT breaks down helpfulness into <strong>
							<font color="red">9</font>
						</strong> specific dimensions, allowing for a detailed and nuanced evaluation of multi-turn, multi-modal interactions. </li>
					<li><strong>Effective for Multi-turn Alignment:</strong> Building on <span class="icon">
							<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
						</span>InterMT, we investigate methods to model long-horizon values and align dynamic human values. Our findings reveal the phenomenon of preference transfer in multi-turn multimodal interactions, which facilitates preference modeling for predicting human judgments.
						Additionally, we identify a scaling phenomenon in multi-turn multimodal judge moderation. </li>
					<li><strong>One More Thing</strong> We introduce InterMT-Bench to evaluate the ability of MLLMs in assisting judges across multi-turn, multimodal tasks, encompassing three parts: Scoring Evaluation, Pair Comparison, and Crucial Step Recognition. Despite strong reasoning
						capabilities, advanced MLLMs fail to align with human values in judgment tasks. However, they show potential in identifying crucial steps in long-context scenarios. </li>
				</ul>
			</div>
		</div>
	</div>
</section>
<!-- DATASET SECTION -->
<section class="hero is-light is-small">
	<div class="hero-body has-text-centered">
		<!-- <h1 class="title is-1 mathvista"><img src="static/images/mathvista.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>MathVista Dataset</h1> -->
		<h1 class="title is-1 mathvista">
			<!--<img src="static/images/mathvista.png" style="width:1em;vertical-align: middle" alt="Logo"/>-->
			<span class="mathvista" style="vertical-align: middle" id="Dataset"><span class="img">
					<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
				</span>InterMT Dataset</span>
		</h1>
	</div>
</section>
<section class="section">
	<center>
		<h2 class="title is-3">Data Overview</h2>
	</center>
	<div class="columns is-centered has-text-centered">
		<div class="column is-four-fifths">
			<div class="content has-text-justified">
				<div class="carousel results-carousel">
					<div class="carousel-item">
						<div class="box m-5">
							<div class="content has-text-centered">
								<img src="./assets/Figure2.png" style="width: 60%;" onclick="openFullscreenImage(this.src)">
								<p style="margin-bottom: 30px;">
									<span class="icon">
										<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
									</span><b>InterMT</b> includes over 15 tasks in vision-language scenarios, capturing communication examples across diverse settings. These examples demonstrate <span style="color: #8B0000;">multi-turn</span>, interleaved <span
										style="color: #00008B;">understanding</span> and <span style="color: #006400;">generation</span> in six representative domains.
								</p>
							</div>
						</div>
					</div>
					<div class="carousel-item">
						<div class="box m-5">
							<div class="content has-text-centered">
								<img src="./assets/Figure3-0515_00.png" style="width: 60%;" onclick="openFullscreenImage(this.src)">
								<p style="margin-bottom: 30px;">
								<p> Overview of the four‚Äêstage pipeline for constructing<span class="icon">
										<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
									</span><b>InterMT</b>.<br>
									<strong style="font-weight: bold;">Stage I:</strong> seed questions are harvested from open‚Äêsource corpora, websites, and human writing, then filtered for perceptual quality, diversity, and multi‚Äêturn potential. <strong style="font-weight: bold;">Stage
										II:</strong> iterative calls to large models and external tools (<em>e.g.</em> retrieve, generate) produce answer expansions and follow-up questions, forming a candidate QA tree. <strong style="font-weight: bold;">Stage III:</strong> human annotators perform
									per‚Äêturn (local) and conversation-level (global) evaluations‚Äîcovering quality, coherence, context awareness, and completeness‚Äîto prune and select preferred branches. <strong style="font-weight: bold;">Stage IV:</strong> the retained branches are reassembled into
									deep, coherent QA trees (depth &ge; 5) yielding the final multi‚Äêturn QA pairs for model training.
								</p>
							</div>
						</div>
					</div>
				</div>
			</div>
			<div class="container">
				<div class="container" style="margin-bottom: 2vh;">
					<div class="columns is-centered has-text-centered">
						<div class="column is-four-fifths">
							<p><strong>Dataset Composition</strong></p>
							<P>The dataset includes: <strong>(1)</strong> carefully crafted seed questions for multi-turn, multimodal conversations, and <strong>(2)</strong> fine-grained human preference annotations at both local and global conversation levels. The seed questions are rigorously
								selected and refined to enable more faithful simulation of real-world tasks. </p>
							<P> We collect preference data through score evaluations and pairwise comparisons of multi-modal responses at each conversation turn, based on four sub-dimensions. Global conversation helpfulness is then evaluated via five sub-dimensions. Incorporating natural language
								feedback further improves annotation quality and alignment with human intent.</p1>
							<p><strong>Data Card</strong></p>
							<ul>
								<li>The dataset is built from a corpus of <span class="highlight">100k</span> image-text examples, comprising <span class="highlight">72.1%</span> from open-source vision-language datasets, <span class="highlight">22.8%</span> from web data, and <span
										class="highlight">5.1%</span> from human-written content. All prompts are refined following constitutional guidelines to improve multi-turn compatibility, resulting in <span class="highlight">15604</span> unique seed questions.</li>
								<li>Each seed question is expanded via an agent-based multi-turn QA construction workflow, producing at least <span class="highlight">8</span> multi-turn QA instances per prompt. After pruning and filtering, we obtain <span class="highlight">52.6k</span> high-quality
									multi-turn QA instances, with <span class="highlight">41.92%</span> containing five or more turns. </li>
								<li>The resulting <span class="highlight">52.6k</span> QA instances cover <span class="highlight">15+</span> vision-language <span class="highlight">understanding</span> and <span class="highlight">generation</span> tasks, such as image editing and visual tutorials.
									Each instance features interleaved textual and visual content in both inputs and outputs, with an average of <span class="highlight">5.33</span> images per conversation. </li>
								<li>The dataset features <span class="highlight">32,459</span> human preference annotations, organized as score evaluation pairwise comparisons at both the local and global levels. Preferences are decomposed into <span class="highlight">9</span> dimensions of
									helpfulness, accompanied by human-written critiques, refinement suggestions, and rationales. </li>
							</ul>
							<p style="font-size: 1.5em; text-align: center;"><strong>Human Annotation</strong></p>
							<P> Defining high-quality multi-turn multimodal dialogues is inherently challenging, as it requires assessing response correctness, the coherence of image-text interleaving, and the dynamic nature of human preferences throughout the conversation. We conduct multiple
								rounds of in-depth discussions with our annotation team regarding existing open-source datasets and prior work on MLLMs.</p>
							<P> We then identify the following <span class="highlight">9</span> annotation dimensions:<em><u>(click table elements for detailed definition.)</u></em></p>
							<div class="dimensions-table">
								<table class="w-full border-collapse bg-white shadow-md rounded-lg">
									<thead>
										<tr class="bg-gray-800 text-white">
											<th class="p-4 text-left">Global Dimensions</th>
											<th class="p-4 text-left">Local Dimensions</th>
										</tr>
									</thead>
									<tbody>
										<tr>
											<td class="p-4 border-t" data-dimension="G1">G1: Context Awareness</td>
											<td class="p-4 border-t" data-dimension="L1">L1: Local Image-Text Consistency</td>
										</tr>
										<tr>
											<td class="p-4 border-t" data-dimension="G2">G2: Helpfulness and Completeness</td>
											<td class="p-4 border-t" data-dimension="L2">L2: Visual Perceptual Quality</td>
										</tr>
										<tr>
											<td class="p-4 border-t" data-dimension="G3">G3: Crucial Step Recognition</td>
											<td class="p-4 border-t" data-dimension="L3">L3: Contextual Coherence</td>
										</tr>
										<tr>
											<td class="p-4 border-t" data-dimension="G4">G4: Global Image-Text Consistency</td>
											<td class="p-4 border-t" data-dimension="L4">L4: Text Quality</td>
										</tr>
										<tr>
											<td class="p-4 border-t" data-dimension="G5">G5: Style Coherence</td>
											<td class="p-4 border-t"></td>
										</tr>
									</tbody>
								</table>
							</div>
							<!-- Modal for displaying definitions -->
							<div id="definitionModal" class="modal">
								<div class="modal-content">
									<span class="close">&times;</span>
									<div id="definitionContent" class="prose"></div>
								</div>
							</div>
							<script>
								// Definitions object
								const definitions = {
								    G1: {
								        title: "Context Awareness",
								        definition: "The model should retain and understand the dialogue history to ensure contextual coherence, rather than treating each turn as an isolated interaction.",
								        examples: [
								            "In <i>visual storytelling</i> tasks, the model should maintain consistent characters, settings, and plot lines.",
								            "In <i>design revision</i> tasks, the model should remember the user's previous requests and avoid repeating suggestions that were previously rejected."
								        ],
								        scoring: [
								            "<b>0 points</b>: The model completely ignores the context; responses are irrelevant or contradict the dialogue history.",
								            "<b>1 point</b>: The model partially recalls context but exhibits noticeable information loss or inconsistency in roles.",
								            "<b>2 points</b>: Context is mostly preserved, with occasional minor inconsistencies.",
								            "<b>3 points</b>: Full understanding of context; responses are logically coherent, with no information loss or contradictions."
								        ]
								    },
								    G2: {
								        title: "Helpfulness and Completeness",
								        definition: "Measures how well the model's textual and visual outputs follow task instructions and provide complete information to fulfill the user's request. This also includes the logical structure of the response. In multi-turn image-text interactions, the model should accurately follow all instructions and ultimately deliver a complete solution.",
								        examples: [
								            "<b>Task</b>: Cake design improvement",
								            "&nbsp;&nbsp;- <i>User</i>: Please help me improve this design (uploads image)",
								            "&nbsp;&nbsp;- <i>AI</i>: Suggests adding frosting (but no new image generated) ‚Üí Deduct points",
								            "&nbsp;&nbsp;- <i>User</i>: Please show me the modified 3D rendering",
								            "&nbsp;&nbsp;- <i>AI</i>: [Generates an image of the cake with frosting] ‚Üí Full score"
								        ],
								        scoring: [
								            "<b>0 points</b>: The model fails to meet the user's needs; responses are irrelevant or severely incorrect, making the task unachievable.",
								            "<b>1 point</b>: Partially satisfies the user's request but lacks critical content or contains major errors that hinder task completion.",
								            "<b>2 points</b>: Largely completes the task but has minor omissions or inaccuracies that affect the final outcome.",
								            "<b>3 points</b>: Fully and accurately fulfills all user requirements; information is comprehensive, logically structured, and free from errors or omissions."
								        ]
								    },
								    G3: {
								        title: "Crucial Step Recognition",
								        definition: "In multi-turn interactions, the model must accurately identify and complete crucial steps, avoiding irrelevant or incorrect information.",
								        examples: [
								            "<b>Task</b>: Step-by-step guidance for drawing a cat",
								            "&nbsp;&nbsp;- <b>Crucial steps</b>: Sketch outline ‚Üí Refine facial features ‚Üí Adjust proportions ‚Üí Apply color",
								            "&nbsp;&nbsp;- <i>Incorrect</i>: Model asks the user to color before the outline is drawn",
								            "&nbsp;&nbsp;- <i>Correct</i>: Model guides the user through steps in a logical order"
								        ],
								        scoring: [
								            "<b>0 points</b>: Key steps are entirely incorrect or omitted, preventing task completion.",
								            "<b>1 point</b>: Some steps are inaccurate, though the task may still proceed with effort.",
								            "<b>2 points</b>: Overall step sequence is reasonable, with minor deviations or logical flaws.",
								            "<b>3 points</b>: All crucial steps are correctly identified and ordered, with no redundancy or omissions."
								        ]
								    },
								    G4: {
								        title: "Global Image-Text Consistency",
								        definition: "In multi-turn image-text interactions, textual descriptions should align closely with the generated images. Inconsistencies between text and images, or failing to generate images when required, result in lower scores.",
								        examples: [
								            "<b>Task</b>: AI-generated interior design plan",
								            "&nbsp;&nbsp;- <i>User</i>: Please provide a modern-style living room design",
								            "&nbsp;&nbsp;- <i>AI</i>: [Generates image, but the style does not match] ‚Üí Deduct points",
								            "&nbsp;&nbsp;- <i>User</i>: Please change the sofa color to dark grey",
								            "&nbsp;&nbsp;- <i>AI</i>: [Generates image with dark grey sofa] ‚Üí Full score"
								        ],
								        scoring: [
								            "<b>0 points</b>: Images are completely unrelated to the text, or necessary images are missing.",
								            "<b>1 point</b>: Partial relevance, but with significant mismatches (e.g., incorrect color or structure).",
								            "<b>2 points</b>: Largely consistent, with minor deviations.",
								            "<b>3 points</b>: Perfect alignment between text and images, with no inconsistencies."
								        ]
								    },
								    G5: {
								        title: "Style Coherence",
								        definition: "Assesses the consistency of style and subject representation across generated images, including texture, color harmony, lighting, rendering style, physical properties, clothing, and behavior. It penalizes visual repetition, such as overly similar outputs or duplicated elements within a single image. In multi-turn interactions, generated images should exhibit stylistic coherence across turns, with smooth transitions and no abrupt changes.",
								        examples: [
								            "<b>Special Case</b>: If only one turn includes an image while the other does not, visual style coherence is <b>not</b> affected. In such cases, assign a <b>default score of 3 points</b>."
								        ],
								        scoring: [
								            "<b>‚Äì1 point</b>: The task required image generation, but none was provided.",
								            "<b>0 points</b>: Images exhibit entirely different styles, tones, rendering, or subject traits, resulting in visual dissonance.",
								            "<b>1 point</b>: Some stylistic or subject consistency, but with clear discrepancies (e.g., sudden tone changes, mismatched rendering, or inconsistent subject traits).",
								            "<b>2 points</b>: Style, tone, and subject representation are generally consistent, with minor variations that do not affect overall coherence.",
								            "<b>3 points</b>: All images are highly consistent in style, tone, quality, and subject representation; visual transitions are smooth and contextually appropriate."
								        ]
								    },
								    L1: {
								        title: "Local Image-Text Consistency",
								        definition: "In a single dialogue turn, the textual description should closely match the generated image(s), ensuring the text accurately reflects the visual content without ambiguity or misleading information.",
								        examples: [
								            "If the turn includes multiple images, evaluate the overall consistency of the text with all images. Individual image feedback can be added as needed (e.g., [3,1] Image 1: accurate; Image 2: inconsistent).",
								            "If no image is generated, evaluate based on task requirements:",
								            "&nbsp;&nbsp;- If image generation was expected but omitted, assess the inconsistency between the text and the missing visual content.",
								            "&nbsp;&nbsp;- If the task (e.g., Visual Analysis) does not require image generation, assess consistency between the input image and the text.",
								            "Otherwise, default evaluation compares the answer text with the image(s) generated in that turn."
								        ],
								        scoring: [
								            "<b>0 points</b>: Text is irrelevant to the image(s) or contains major factual errors; Key descriptions are missing or completely incorrect (e.g., referencing nonexistent objects or scenes); Text may cause significant misunderstanding.",
								            "<b>1 point</b>: Text is partially related to the image(s), but includes clear errors or misleading descriptions; Covers part of the image content but omits or misrepresents key details or relationships; Reader must infer or adjust understanding to align with the image(s).",
								            "<b>2 points</b>: Text generally matches the image(s), with minor local inaccuracies (e.g., imprecise attribute descriptions or slight omissions); Does not hinder overall comprehension, but lacks precision upon close inspection.",
								            "<b>3 points</b>: Text is highly aligned with the image(s), covering all key elements and details; Free from factual errors or ambiguity; the description is natural and coherent."
								        ]
								    },
								    L2: {
								        title: "Visual Perceptual Quality",
								        definition: "Evaluates the visual realism, naturalness, and absence of distortion or artifacts in the generated image(s). Focuses on whether the image structure, colors, and composition realistically simulate the physical world, avoiding unnatural artifacts.",
								        examples: [
								            "In multi-image outputs, assign a unified score for overall quality. If image quality varies significantly, provide per-image feedback as needed (e.g., [3,1] Image 1: good; Image 2: distorted).",
								            "If no image is generated, assess any image provided in the user prompt. If the image has issues, point them out in the textual answer."
								        ],
								        scoring: [
								            "<b>0 points</b>: Obvious artifacts (e.g., disconnections, misalignments), severe distortions (e.g., highly unrealistic shapes), or structural errors (e.g., unbalanced proportions, illogical composition); Unnatural color rendering (e.g., harsh color blocks, abnormal tones); Lighting does not follow physical laws, severely affecting image recognizability.",
								            "<b>1 point</b>: Image is mostly recognizable but contains localized severe flaws; Examples: anatomical errors (e.g., limb dislocation), inconsistent local color (e.g., banding, strong noise), or small rendering failures; Overall naturalness is compromised, affecting visual coherence.",
								            "<b>2 points</b>: Image is generally natural and coherent; structure, color, and lighting are mostly reasonable; Minor local imperfections such as rough edges, small artifacts, or slight blurring that do not affect overall perceptual quality.",
								            "<b>3 points</b>: Image is visually realistic and natural; Well-structured, smooth color transitions, physically consistent lighting; No visible artifacts, distortions, or flaws; overall aesthetics and details are excellent."
								        ]
								    },
								    L3: {
								        title: "Contextual Coherence",
								        definition: "Assesses whether the response in this turn logically continues the dialogue history and remains consistent with prior content, avoiding contradictions.",
								        examples: [],
								        scoring: [
								            "<b>0 points</b>: Completely irrelevant or logically inconsistent with previous context.",
								            "<b>1 point</b>: Partially relevant but includes clear inconsistencies.",
								            "<b>2 points</b>: Mostly coherent, with minor deviations.",
								            "<b>3 points</b>: Fully consistent with prior dialogue; no contradictions."
								        ]
								    },
								    L4: {
								        title: "Text Quality",
								        definition: "Measures the clarity, coherence, and correctness of the output text. Includes grammar, spelling, readability, consistency with instructions and context, and absence of redundancy. Responses should be logically sound, well-structured, and clearly expressed, avoiding abrupt transitions or repetition.",
								        examples: [],
								        scoring: [
								            "<b>0 points</b>: Text is disorganized, lacks logic, and is hard to understand; may contain numerous grammar or spelling errors or repetitive content.",
								            "<b>1 point</b>: Some parts are logically clear, but the text includes noticeable jumps, omissions, or contradictions that hurt overall readability; may include frequent language errors or redundant expressions.",
								            "<b>2 points</b>: The overall logic is reasonable and the flow mostly smooth, but there are minor incoherences; some sentences may require optimization to improve readability.",
								            "<b>3 points</b>: Text is logically rigorous, clearly expressed, well-organized, and naturally structured; no obvious jumps or repetition; grammar and spelling are correct, providing a good reading experience."
								        ]
								    }
								};
								
								// Get modal elements
								const modal = document.getElementById('definitionModal');
								const modalContent = document.getElementById('definitionContent');
								const closeBtn = document.getElementsByClassName('close')[0];
								
								// Add click event listeners to table cells
								document.querySelectorAll('td[data-dimension]').forEach(cell => {
								    cell.addEventListener('click', () => {
								        const dimension = cell.getAttribute('data-dimension');
								        const def = definitions[dimension];
								        if (def) {
								            // Build HTML for the definition
								            let html = `<h1 class="text-xl font-bold mb-2" style="font-size: 36px;"><b><u>${def.title}</u></b></h1>`;
								            html += `<h3 class="text-lg font-semibold"><b>Definition</b></h3>`;
								            html += `<p class="mb-4">${def.definition}</p>`;
								            if (def.examples && def.examples.length > 0) {
								                html += `<h3 class="text-lg font-semibold"><b>Examples</b></h3>`;
								                html += `<ul class="list-disc pl-5 mb-4">`;
								                def.examples.forEach(example => {
								                    html += `<li>${example}</li>`;
								                });
								                html += `</ul>`;
								            }
								            html += `<h3 class="text-lg font-semibold"><b>Scoring Criteria</b></h3>`;
								            html += `<ul class="list-disc pl-5">`;
								            def.scoring.forEach(criterion => {
								                html += `<li>${criterion}</li>`;
								            });
								            html += `</ul>`;
								            modalContent.innerHTML = html;
								            modal.style.display = 'block';
								        }
								    });
								});
								
								// Close modal when clicking the close button
								closeBtn.addEventListener('click', () => {
								    modal.style.display = 'none';
								});
								
								// Close modal when clicking outside the modal content
								window.addEventListener('click', (event) => {
								    if (event.target === modal) {
								        modal.style.display = 'none';
								    }
								});
							</script>
							<br>
							<p> Crowdworkers first rate individual turns and then evaluate entire conversations from both local and global perspectives. A <span class="bold">Dual Verification</span> stage combines dedicated annotator efforts with professional quality control reviews to ensure
								guideline adherence. Structured <span class="bold">Language Feedback</span>, which offers concise explanations of scoring rationale, focused critiques, and refinement suggestions, further guides response improvement and substantially enhances annotation reliability.
							</p>
						</div>
					</div>
				</div>
			</div>
			<!-- Fullscreen Image Modal -->
			<div id="fullscreen-image-modal" class="modal" style="display: none;">
				<div class="modal-content" style="position: relative;">
					<span class="close" onclick="closeFullscreenImage()" style="position: absolute; top: 10px; right: 10px; cursor: pointer; font-size: 24px;">&times;</span>
					<img id="fullscreen-image" src="" style="width: 100%; height: auto;">
				</div>
			</div>
			<script>
				function openFullscreenImage(imageSrc) {
				    const modal = document.getElementById('fullscreen-image-modal');
				    const fullscreenImage = document.getElementById('fullscreen-image');
				    fullscreenImage.src = imageSrc;
				    modal.style.display = 'block';
				}
				
				function closeFullscreenImage() {
				    const modal = document.getElementById('fullscreen-image-modal');
				    modal.style.display = 'none';
				}
				
				// Close the modal when clicking outside the modal content
				window.onclick = function(event) {
				    const modal = document.getElementById('fullscreen-image-modal');
				    if (event.target === modal) {
				        modal.style.display = 'none';
				    }
				}
			</script>
</section>
<section class="hero is-light is-small">
	<div class="hero-body has-text-centered">
		<!-- <h1 class="title is-1 mathvista"><img src="static/images/mathvista.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>MathVista Dataset</h1> -->
		<h1 class="title is-1 mathvista">
			<!--<img src="static/images/mathvista.png" style="width:1em;vertical-align: middle" alt="Logo"/>-->
			<span class="mathvista" style="vertical-align: middle" id="Data Experiment Analysis">Dataset Main Analysis</span>
		</h1>
	</div>
</section>
<section class="section">
	<div class="container">
		<div class="columns is-centered has-text-centered">
			<div class="column is-four-fifths" style="width: 100%;">
				<!-- <h2 class="title is-3">Dataset Main Analysis</h2> -->
				<div class="content has-text-justified">
					<div id="results-carousel" class="carousel results-carousel">
						<div class="content has-text-centered">
							<img src="./assets/Figure5-1.png" style="width: 55%;">
							<ol style="margin-left: auto; margin-right: auto; width: 75%;">
								<li> Each seed question is expanded via an agent-based multi-turn QA construction workflow, producing at least 8 multi-turn QA instances per prompt. After pruning and filtering, we obtain 52.6k high-quality multi-turn QA instances, with 41.92% containing five or more
									turns. </li>
								<li> The resulting 52.6k QA instances cover 15+ vision-language understanding and generation tasks, such as image editing and visual tutorials. Each instance features interleaved textual and visual content in both inputs and outputs, with an average of 5.33 images per
									conversation. </li>
							</ol>
						</div>
						<div class="content has-text-centered">
							<img src="./assets/Figure4-0516_00.jpg" style="width: 55%;">
							<p>This figure illustrates the relationship between global and local preference annotation dimensions. We identify three key findings:
							<ol style="margin-left: auto; margin-right: auto; width: 75%;">
								<li>
									<strong>Modality perception precedes effective modality fusion</strong>: for both the local-local and local-global correlation, the evaluation of image-text consistency is strongly correlated with visual perceptual quality (up to 0.81). This suggests that before
									assessing multimodal information, human evaluators tend to prioritize a clear understanding of each individual modality, indicating that a clear perception of individual modalities is a prerequisite for reliable multimodal judgment.
								</li>
								<li>
									<strong>Long-horizon evaluations hinge on coherence and temporal consistency</strong>: for the global-global correlation, metrics such as helpfulness and completeness strongly align with context awareness and global visual consistency, underscoring the importance
									of maintaining coherent semantics, multimodal information, and consistency with prior conversational context over extended interactions.
								</li>
								<li>
									<strong>Intent grounding drives long-horizon crucial step recognition</strong>: in multi-turn scenarios, models may deviate from the user's core intentions, producing self-directed responses. Despite locally high-scoring and plausible outputs, this leads to
									stylistic drift and omission of key steps over extended interactions, as demonstrated in the local-global setting.
								</li>
							</ol>
							</p>
						</div>
						<div class="content has-text-centered">
							<img src="./assets/Figure5.png" style="width: 55%;">
							<p style="margin-left: auto; margin-right: auto; width: 75%;"> This figure presents an analysis of the agreement among human annotators, AI annotators, and human verifiers. Our observations reveal that while AI annotators achieved an average score agreement of
								approximately 60% on <em>local</em> evaluation tasks, their agreement with humans is considerably lower for <em>global</em>, namely, longer-horizon, evaluation tasks. This finding partly indicates that current advanced models struggle to reach consensus with human
								judgments when performing score evaluations in multi-turn, multi-modal conversation tasks. We further conduct a broader investigation of the ability of MLLMs to assist human judges in three types of tasks. Consequently, until further validation of AI feedback's
								efficacy, we maintain a conservative position, considering the replacement of human annotation to be presently challenging. </p>
						</div>
						<!-- <div class="content has-text-centered">
		<img src="./assets/Figure6_00.jpg" style="width: 65%;">
		<p style="margin-left: auto; margin-right: auto; width: 75%;">Our findings, presented in this figure, suggest that modeling fine-grained <em>local (turn-level)</em> preferences is more effective in capturing human values and achieving better alignment. In contrast,
			directly modeling <em>global (conversation-level)</em> preferences often fails to reflect these nuanced preferences, especially in complex, long-horizon scenarios. We examine the bidirectional transfer between turn-level (<em>local</em>) and conversation-level
			(<em>global</em>) human preferences. Both <em>local-to-global</em> and <em>global-to-local</em> transfers are effective, since multi-turn questions typically hinge on the seed question's intent. However, <em>global-to-local</em> transfer is consistently easier and
			better aligned with actual preferences. We attribute this to the greater stability of global preferences‚Äîreflecting users' overarching tendencies‚Äîwhereas local preferences are short-term and more context-dependent, making <em>local-to-global</em> transfer more
			challenging.</p>
		</div> -->
					</div>
				</div>
			</div>
		</div>
	</div>
</section>
<div>
	<section class="hero is-light is-small">
		<div class="hero-body has-text-centered">
			<h1 class="title is-1 mathvista">
				<span class="mathvista" style="vertical-align: middle" id="Inspiring Future Research">Inspiring Future Research</span>
			</h1>
		</div>
	</section>
	<div class="container" style="margin-bottom: 2vh;">
		<div class="columns is-centered has-text-centered">
			<div class="column is-four-fifths">
				<p>
					<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" /> InterMT lays the groundwork for advancing research on aligning fine-grained human values in <em>multi-turn multimodal understanding and generation</em> tasks, potentially inspiring new
					research directions. Building on real human data provided by our dataset, we identify several promising directions for future research:
				</p>
				<ul>
					<li>
						<strong style="color:#8B0000;">Modeling long-horizon values.</strong> <em>How can we model long-horizon, interleaved multimodal preferences by leveraging the <em>local</em> and <em>global</em> human annotations in <img src="./assets/logo_transp.png" alt="HuggingFace"
								style="width: 1em; height: 1em;" />InterMT?</em>
					</li>
					<li>
						<strong style="color:#8B0000;">Aligning dynamic human values:</strong> <em>How can we design algorithms that effectively incorporate real human feedback from <img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />InterMT to assess and enhance
							the performance of MLLMs?</em>
					</li>
				</ul>
			</div>
		</div>
	</div>
</div>
<br>
<div class="container" style="margin-bottom: 2vh;">
	<div class="columns is-centered has-text-centered">
		<div class="column is-four-fifths">
			<!-- Á¨¨‰∏ÄÈ°µÂÜÖÂÆπÔºöPreference Modeling -->
			<div class="content has-text-centered">
				<h4>Preference Modeling for Multi-turn Interleaved Multimodal Scenarios</h4>
				<p> We investigate two strategies for modeling long-horizon preferences in multi-turn multimodal scenarios: <em>prefix preference</em> and <em>chain-based preference</em>. Our findings, presented in this Figure, suggest that modeling fine-grained <em>local (turn-level)</em>
					preferences is more effective in capturing human values and achieving better alignment. In contrast, directly modeling <em>global (conversation-level)</em> preferences often fails to reflect these nuanced preferences, especially in complex, long-horizon scenarios. </p>
				<figure style="width: 70%; margin: 0 auto;">
					<img src="./assets/Figure6_00.jpg" style="width: 100%;">
				</figure>
				<p> We examine the bidirectional transfer between turn-level (<em>local</em>) and conversation-level (<em>global</em>) human preferences. As shown in the figure, both <em>local-to-global</em> and <em>global-to-local</em> transfers are effective, since multi-turn questions typically
					hinge on the seed question's intent. However, <em>global-to-local</em> transfer is consistently easier and better aligned with actual preferences. We attribute this to the greater stability of global preferences‚Äîreflecting users' overarching tendencies‚Äîwhereas local preferences
					are short-term and more context-dependent, making <em>local-to-global</em> transfer more challenging. </p>
			</div>
			<!-- Á¨¨‰∫åÈ°µÂÜÖÂÆπÔºöScaling Laws -->
			<div class="content has-text-centered">
				<h4>Multi-turn Scaling Law of Turn-based Judge Moderation</h4>
				<p>
					<em>Can we accurately capture users' intentions and latent preferences with a limited number of conversational turns, thereby improving the modeling of long-term values?</em> Such capabilities are crucial for building general-purpose AI assistants, which need to understand and
					predict users' needs across diverse contexts, adapting to changing preferences over time. We investigate whether the discriminative power of judge models, trained on the first <code style="color:#000000;">k</code> turns, improves in subsequent turns (from <code
						style="color:#000000;">k+1</code> to <code style="color:#000000;">N</code>) and exhibits <em>scaling laws</em>.
				</p>
				<figure style="width: 70%; margin: 0 auto;">
					<img src="./assets/Figure7_00.jpg" style="width: 100%;">
				</figure>
				<p style="margin-left: auto; margin-right: auto; width: 75%;"> The results reveal two key insights: <strong>(1)</strong> Multi-turn judge moderation exhibits a generalization effect linked to the number of turns. As shown in this figure (a), for evaluation turn <code
						style="color:#000000;">k</code>, as the number of preceding turns increases from 1 to <code style="color:#000000;">k-1</code>, the model's accuracy continues to improve, with average future performance rising, indicating that training on multi-turn data with a limited number
					of turns can generalize to longer horizons. <strong>(2)</strong> Regarding the number of turns in the training data, the generalization effect shows a diminishing trend. As demonstrated in this figure (b), training with <code style="color:#000000;">k</code> turns does improve
					performance for <em>k+1 ‚Üí T</em> turns, but this effect diminishes as the number of turns increases. The decline is due to three factors: diminishing returns as the model struggles with long-term preferences, contextual drift as earlier turns lose relevance, and the evolving
					interaction between user intentions and latent preferences. </p>
			</div>
		</div>
	</div>
</div>
</div>
</div>
<section class="section">
	<div class="container" style="margin-bottom: 2vh;">
		<div class="columns is-centered has-text-centered">
			<div class="column is-four-fifths">
				<h2 class="title is-3" id="Evaluation">
					<span class="icon">
						<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
					</span>
					<b>InterMT</b> Performance Evaluation
				</h2>
				<div class="content has-text-justified">
					<p>
						<em>Do MLLMs truly understand what is desirable in multi-turn, multimodal interactions and how to align with human values?</em> This task is particularly challenging due to the absence of multimodal benchmarks that capture human preferences in multi-turn settings. Leveraging
						genuine feedback from <strong><img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />InterMT</strong>, we introduce <strong>InterMT-Bench</strong> to assess MLLMs' alignment with human values in multi-turn, multimodal tasks.
					</p>
					<p>
						<strong>InterMT-Bench</strong> comprises three distinct tasks: <em>Scoring Evaluation</em>, <em>Pair Comparison</em>, and <em>Crucial Step Recognition</em>.
					</p>
				</div>
				<div class="content">
					<table class="data-table center-table">
						<thead>
							<tr>
								<th rowspan="2" style="color: #ffffff;">Settings</th>
								<th rowspan="2" style="color: #ffffff;">MLLMs</th>
								<th colspan="5" style="color: #ffffff;">Local Setting</th>
								<th colspan="6" style="color: #ffffff;">Global Setting</th>
							</tr>
							<tr>
								<th style="color: #ffffff;">L1</th>
								<th style="color: #ffffff;">L2</th>
								<th style="color: #ffffff;">L3</th>
								<th style="color: #ffffff;">L4</th>
								<th style="color: #ffffff;">Avg.</th>
								<th style="color: #ffffff;">G1</th>
								<th style="color: #ffffff;">G2</th>
								<th style="color: #ffffff;">G3</th>
								<th style="color: #ffffff;">G4</th>
								<th style="color: #ffffff;">G5</th>
								<th style="color: #ffffff;">Avg.</th>
							</tr>
						</thead>
						<tbody>
							<tr>
								<td rowspan="12" class="bold">Scoring Evaluation</td>
								<td>Gemini-Flash* ‚Ä†</td>
								<td>0.346</td>
								<td>0.107</td>
								<td>0.119</td>
								<td>0.173</td>
								<td>0.186</td>
								<td>0.163</td>
								<td>0.042</td>
								<td>0.051</td>
								<td>
									<strong style="color:#C6011F">0.246</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
										<path fill-rule="evenodd"
											d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
											clip-rule="evenodd" />
									</svg>
								</td>
								<td>0.005</td>
								<td>0.101</td>
							</tr>
							<tr>
								<td>Gemini-Flash* (+reason)</td>
								<td>
									<strong style="color:#C6011F">0.361</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
										<path fill-rule="evenodd"
											d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
											clip-rule="evenodd" />
									</svg>
								</td>
								<td>0.072</td>
								<td>0.122</td>
								<td>0.168</td>
								<td>0.181</td>
								<td>-0.038</td>
								<td>0.083</td>
								<td>0.139</td>
								<td>0.199</td>
								<td>0.048</td>
								<td>0.086</td>
							</tr>
							<tr>
								<td>GPT-4.1</td>
								<td>0.264</td>
								<td>0.095</td>
								<td>0.242</td>
								<td>0.269</td>
								<td>0.218</td>
								<td>0.215</td>
								<td>0.216</td>
								<td>0.084</td>
								<td>0.044</td>
								<td>0.049</td>
								<td>0.122</td>
							</tr>
							<tr>
								<td>GPT-4.1 (+reason)</td>
								<td>0.281</td>
								<td>0.094</td>
								<td>0.272</td>
								<td>0.271</td>
								<td>0.229</td>
								<td>0.215</td>
								<td>0.255</td>
								<td>0.217</td>
								<td>0.216</td>
								<td>0.050</td>
								<td>
									<strong style="color:#C6011F">0.191</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
										<path fill-rule="evenodd"
											d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
											clip-rule="evenodd" />
									</svg>
								</td>
							</tr>
							<tr>
								<td>GPT-4o</td>
								<td>0.291</td>
								<td>
									<strong style="color:#C6011F">0.131</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
										<path fill-rule="evenodd"
											d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
											clip-rule="evenodd" />
									</svg>
								</td>
								<td>0.277</td>
								<td>0.268</td>
								<td>
									<strong style="color:#C6011F">0.242</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
										<path fill-rule="evenodd"
											d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
											clip-rule="evenodd" />
									</svg>
								</td>
								<td>0.254</td>
								<td>0.167</td>
								<td>0.137</td>
								<td>0.139</td>
								<td>0.069</td>
								<td>0.153</td>
							</tr>
							<tr>
								<td>GPT-4o (+reason)</td>
								<td>0.290</td>
								<td>0.091</td>
								<td>0.252</td>
								<td>
									<strong style="color:#C6011F">0.280</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
										<path fill-rule="evenodd"
											d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
											clip-rule="evenodd" />
									</svg>
								</td>
								<td>0.228</td>
								<td>0.183</td>
								<td>0.243</td>
								<td>0.194</td>
								<td>0.086</td>
								<td>0.072</td>
								<td>0.156</td>
							</tr>
							<tr>
								<td>Gemini-Pro*</td>
								<td>0.273</td>
								<td>0.079</td>
								<td>0.258</td>
								<td>0.168</td>
								<td>0.194</td>
								<td>
									<strong style="color:#C6011F">0.285</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
										<path fill-rule="evenodd"
											d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
											clip-rule="evenodd" />
									</svg>
								</td>
								<td>0.240</td>
								<td>-0.024</td>
								<td>0.235</td>
								<td>
									<strong style="color:#C6011F">0.145</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
										<path fill-rule="evenodd"
											d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
											clip-rule="evenodd" />
									</svg>
								</td>
								<td>0.176</td>
							</tr>
							<tr>
								<td>Gemini-Pro* (+reason)</td>
								<td>0.274</td>
								<td>0.070</td>
								<td>0.304</td>
								<td>0.211</td>
								<td>0.215</td>
								<td>0.239</td>
								<td>
									<strong style="color:#C6011F">0.267</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
										<path fill-rule="evenodd"
											d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
											clip-rule="evenodd" />
									</svg>
								</td>
								<td>0.195</td>
								<td>0.129</td>
								<td>0.060</td>
								<td>0.178</td>
							</tr>
							<tr>
								<td>Claude-thinking*</td>
								<td>0.299</td>
								<td>0.044</td>
								<td>0.262</td>
								<td>0.229</td>
								<td>0.209</td>
								<td>0.172</td>
								<td>0.140</td>
								<td>0.175</td>
								<td>0.150</td>
								<td>0.069</td>
								<td>0.141</td>
							</tr>
							<tr>
								<td>Claude-thinking* (+reason)</td>
								<td>0.291</td>
								<td>0.023</td>
								<td>0.254</td>
								<td>0.214</td>
								<td>0.196</td>
								<td>0.207</td>
								<td>0.260</td>
								<td>0.183</td>
								<td>0.155</td>
								<td>-0.001</td>
								<td>0.161</td>
							</tr>
							<tr>
								<td>o4-mini</td>
								<td>0.334</td>
								<td>0.062</td>
								<td>0.306</td>
								<td>0.134</td>
								<td>0.209</td>
								<td>0.169</td>
								<td>0.161</td>
								<td>0.120</td>
								<td>0.096</td>
								<td>0.028</td>
								<td>0.115</td>
							</tr>
							<tr>
								<td>o4-mini (+reason)</td>
								<td>0.326</td>
								<td>0.056</td>
								<td>
									<strong style="color:#C6011F">0.322</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
										<path fill-rule="evenodd"
											d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
											clip-rule="evenodd" />
									</svg>
								</td>
								<td>0.151</td>
								<td>0.214</td>
								<td>0.215</td>
								<td>0.229</td>
								<td>
									<strong style="color:#C6011F">0.347</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
										<path fill-rule="evenodd"
											d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
											clip-rule="evenodd" />
									</svg>
								</td>
								<td>0.137</td>
								<td>0.016</td>
								<td>0.189</td>
							</tr>
							<tr>
								<td rowspan="10" class="bold">Pair Comparison</td>
								<td>GPT-4.1</td>
								<td>0.541</td>
								<td>
									<strong style="color:#C6011F">0.589</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
										<path fill-rule="evenodd"
											d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
											clip-rule="evenodd" />
									</svg>
								</td>
								<td>0.508</td>
								<td>0.484</td>
								<td>0.531</td>
								<td>0.540</td>
								<td>0.520</td>
								<td>0.530</td>
								<td>
									<strong style="color:#C6011F">0.590</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
										<path fill-rule="evenodd"
											d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
											clip-rule="evenodd" />
									</svg>
								</td>
								<td>
									<strong style="color:#C6011F">0.563</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
										<path fill-rule="evenodd"
											d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
											clip-rule="evenodd" />
									</svg>
								</td>
								<td>
									<strong style="color:#C6011F">0.549</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
										<path fill-rule="evenodd"
											d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
											clip-rule="evenodd" />
									</svg>
								</td>
							</tr>
							<tr>
								<td>GPT-4.1 (+reason)</td>
								<td>0.550</td>
								<td>0.584</td>
								<td>0.501</td>
								<td>0.521</td>
								<td>
									<strong style="color:#C6011F">0.539</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
										<path fill-rule="evenodd"
											d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
											clip-rule="evenodd" />
									</svg>
								</td>
								<td>0.520</td>
								<td>0.520</td>
								<td>0.477</td>
								<td>0.513</td>
								<td>0.540</td>
								<td>0.514</td>
							</tr>
							<tr>
								<td>GPT-4o</td>
								<td>0.513</td>
								<td>0.488</td>
								<td>0.499</td>
								<td>0.510</td>
								<td>0.503</td>
								<td>0.560</td>
								<td>0.517</td>
								<td>
									<strong style="color:#C6011F">0.550</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
										<path fill-rule="evenodd"
											d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
											clip-rule="evenodd" />
									</svg>
								</td>
								<td>0.543</td>
								<td>0.470</td>
								<td>0.528</td>
							</tr>
							<tr>
								<td>GPT-4o (+reason)</td>
								<td>0.500</td>
								<td>0.537</td>
								<td>
									<strong style="color:#C6011F">0.511</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
										<path fill-rule="evenodd"
											d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
											clip-rule="evenodd" />
									</svg>
								</td>
								<td>0.509</td>
								<td>0.514</td>
								<td>0.542</td>
								<td>0.490</td>
								<td>0.545</td>
								<td>0.522</td>
								<td>0.528</td>
								<td>0.525</td>
							</tr>
							<tr>
								<td>Gemini-Pro*</td>
								<td>0.533</td>
								<td>0.521</td>
								<td>0.496</td>
								<td>0.533</td>
								<td>0.521</td>
								<td>
									<strong style="color:#C6011F">0.562</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
										<path fill-rule="evenodd"
											d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
											clip-rule="evenodd" />
									</svg>
								</td>
								<td>
									<strong style="color:#C6011F">0.566</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
										<path fill-rule="evenodd"
											d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
											clip-rule="evenodd" />
									</svg>
								</td>
								<td>0.523</td>
								<td>0.505</td>
								<td>0.505</td>
								<td>0.532</td>
							</tr>
							<tr>
								<td>Gemini-Pro* (+reason)</td>
								<td>0.526</td>
								<td>0.528</td>
								<td>0.513</td>
								<td>0.514</td>
								<td>0.520</td>
								<td>0.548</td>
								<td>0.562</td>
								<td>0.495</td>
								<td>0.522</td>
								<td>0.538</td>
								<td>0.533</td>
							</tr>
							<tr>
								<td>Claude-thinking*</td>
								<td>0.561</td>
								<td>0.568</td>
								<td>0.508</td>
								<td>0.502</td>
								<td>0.535</td>
								<td>0.539</td>
								<td>0.523</td>
								<td>0.518</td>
								<td>0.521</td>
								<td>0.528</td>
								<td>0.526</td>
							</tr>
							<tr>
								<td>Claude-thinking* (+reason)</td>
								<td>
									<strong style="color:#C6011F">0.567</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
										<path fill-rule="evenodd"
											d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
											clip-rule="evenodd" />
									</svg>
								</td>
								<td>0.550</td>
								<td>0.506</td>
								<td>0.519</td>
								<td>0.536</td>
								<td>0.512</td>
								<td>0.522</td>
								<td>0.512</td>
								<td>0.547</td>
								<td>0.512</td>
								<td>0.521</td>
							</tr>
							<tr>
								<td>o4-mini</td>
								<td>0.556</td>
								<td>0.549</td>
								<td>0.508</td>
								<td>
									<strong style="color:#C6011F">0.536</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
										<path fill-rule="evenodd"
											d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
											clip-rule="evenodd" />
									</svg>
								</td>
								<td>0.537</td>
								<td>0.552</td>
								<td>0.498</td>
								<td>0.522</td>
								<td>0.518</td>
								<td>0.495</td>
								<td>0.517</td>
							</tr>
							<tr>
								<td>o4-mini (+reason)</td>
								<td>0.521</td>
								<td>0.564</td>
								<td>0.522</td>
								<td>0.513</td>
								<td>0.530</td>
								<td>0.534</td>
								<td>0.510</td>
								<td>0.507</td>
								<td>0.512</td>
								<td>0.483</td>
								<td>0.509</td>
							</tr>
						</tbody>
					</table>
				</div>
			</div>
		</div>
	</div>
	</div>
	<div class="container" style="margin-bottom: 2vh;">
		<div class="columns is-centered has-text-centered">
			<div class="column is-four-fifths">
				<div class="content has-text-centered">
					<div style="display: flex; align-items: center; justify-content: center; gap: 0em; flex-wrap: wrap;">
						<!-- Image on the left -->
						<div style="flex: 1; min-width: 400px; text-align: center;">
							<figure>
								<img src="./assets/crucial_step_recognition_score_00.jpg" alt="Crucial Step Recognition" style="width: 100%; max-width: 400px; border-radius: 8px;">
							</figure>
						</div>
						<!-- Text on the right -->
						<div style="flex: 2; min-width: 300px;">
							<h4 style="text-align: justify;">Results and Takeaways</h4>
							<p> We evaluated <span style="color:#C6011F">6</span> advanced MLLMs for their ability to assist in judgment for multi-turn multimodal interactions, considering the nine dimensions proposed above. The results reveal key observations: <strong>Existing models still face
									challenges in aligning with long-horizon human values, but they perform more accurately in evaluating local, fine-grained preferences</strong>. As shown in the table, all models exhibit significant gaps in performance compared to humans in both <em>Score
									Evaluation</em> and <em>Pair Comparison</em> tasks. However, the models demonstrate better accuracy when assessing local dimensions rather than global dimensions, suggesting that capturing fine-grained (<em>e.g.</em>, turn-level) human preferences is crucial for
								both evaluation and alignment with human dynamic and long-horizon values. </p>
							<p> However, there is cause for optimism: current MLLMs exhibit near-human-level performance (<span style="color:#C6011F">4.38/5</span>) in recognizing task completion and aligning with human intent (<em>i.e.</em>, <em>Crucial Step Recognition</em>), providing potential
								solutions for long-term value alignment. </p>
							<p>
						</div>
						<!-- Á¨¨‰∏âÈ°µÂÜÖÂÆπÔºöEvaluation Setup -->
						<div class="content has-text-centered">
							<h4>Induced Bias and Hallucination</h4>
							<p> We identified issues related to bias and hallucination: <strong>Position Bias</strong>, where models consistently favor responses in specific positions (<em>e.g.</em>, the first answer), often influenced by training data that places correct answers at the beginning or
								end of prompts; and <strong>High-Score Bias</strong>, where models tend to assign higher scores to entire multi-turn communications. </p>
							<figure style="width: 75%; margin: 0 auto;">
								<img src="./assets/Figure8-v1_00.jpg" style="width: 100%;">
							</figure>
							<p style="margin-left: auto; margin-right: auto; width: 75%;"> These issues, particularly in long-horizon tasks, may hinder the model's ability to capture differences between extended conversations, thereby posing challenges in modeling long-horizon human values and
								potentially leading to safety concerns. </p>
						</div>
					</div>
				</div>
			</div>
		</div>
	</div>
</section>
<section>
	<div class="container" style="margin-bottom: 2vh;">
		<div class="columns is-centered has-text-centered">
			<div class="column is-four-fifths">
				<h2 class="title is-3" id="Evaluation" style="text-align: center;">Examples</h2>
				<p style="text-align: justify;"> Grounded in theoretical frameworks from linguistics, human-computer interaction, and cognitive psychology, we identify five prototypical scenarios that commonly lead to multi-turn conversations in real-world multimodal contexts: (1) incomplete or unclear user cognition; (2) follow-up queries prompted by unsatisfactory initial responses; (3) complex tasks that require incremental, stepwise reasoning; (4) open-ended or companion-like dialogic interactions; and (5) cross-modal mismatches arising from latent inconsistencies between image and text modalities or the need for integrated cross-modal reasoning. </p>
				<div class="diag-dropdown-container"></div>
				<div id="Dataset Examples" class="diag-section diag-orange">
					<div class="diag-section-header diag-orange">
						<svg>
							<use xlink:href="#icon-dataset"></use>
						</svg>
						<span class="highlight">Dataset Examples</span>
					</div>
					<div class="diag-content" id="diag-content">
						<!-- Ê∑ªÂä†Ê†áÁ≠æÂç°ÂØºËà™ -->
						<div class="tabs-container" style="margin-top: 2em; margin-bottom: 1em;">
							<nav class="tabs" style="display: flex; justify-content: center;">
								<ul id="categoryTabs" style="list-style: none; display: flex; margin: 0; padding: 0; border-bottom: 2px solid #eee;">
									<!-- Ê†áÁ≠æÂç°Â∞ÜÈÄöËøáJavaScriptÂä®ÊÄÅÁîüÊàê -->
								</ul>
							</nav>
						</div>
						
						<div class="columns is-multiline" id="examplesContainer">
							<!-- ÂÜÖÂÆπÂ∞ÜÈÄöËøáJavaScriptÂä®ÊÄÅÁîüÊàê -->
						</div>
					</div>
				</div>
			</div>
		</div>
	</div>
</section>
<script>
	// Â≠òÂÇ®ÊâÄÊúâÁ±ªÂà´ÁöÑÊï∞ÊçÆ
	let allCategoriesData = {};
	let currentCategory = '';
	const categoriesConfig = [
		{
			name: 'unclear_cognition',
			path: './multi_turn_qa_examples/unclear_cognition/dataset_examples.json',
			displayName: 'Unclear Cognition'
		},
		{
			name: 'exploratory_or_companion_like_interaction',
			path: './multi_turn_qa_examples/exploratory_or_companion_like_interaction/dataset_examples.json',
			displayName: 'Exploratory or Companion-like Interaction'
		}
	];

	// ÊñáÊú¨È¢ÑÂ§ÑÁêÜÂáΩÊï∞ÔºåÂ∞Ü<>ÊõøÊç¢‰∏∫[]
	function preprocessText(text) {
		if (typeof text !== 'string') return text;
		return text.replace(/</g, '[').replace(/>/g, ']');
	}

	// È¢ÑÂ§ÑÁêÜÊï∞ÊçÆÈ°π
	function preprocessDataItem(item) {
		return {
			...item,
			prompt: preprocessText(item.prompt),
			response: preprocessText(item.response)
		};
	}

	// ÂºÇÊ≠•Âä†ËΩΩÊâÄÊúâÁ±ªÂà´ÁöÑÊï∞ÊçÆ
	async function loadAllCategoriesData() {
		try {
			for (const config of categoriesConfig) {
				const response = await fetch(config.path);
				if (!response.ok) {
					throw new Error(`HTTP error! status: ${response.status} for ${config.name}`);
				}
				const data = await response.json();
				// È¢ÑÂ§ÑÁêÜÊï∞ÊçÆÔºåÊõøÊç¢<>‰∏∫[]
				const processedData = data.map(preprocessDataItem);
				// ‰ΩøÁî®È¢ÑÂÆö‰πâÁöÑdisplayName
				allCategoriesData[config.name] = {
					data: processedData,
					displayName: config.displayName
				};
			}
			console.log('All dataset examples loaded successfully');
			
			// Êï∞ÊçÆÂä†ËΩΩÂÆåÊàêÂêéÂàùÂßãÂåñÁïåÈù¢
			initializeTabs();
			// ÈªòËÆ§ÊòæÁ§∫Á¨¨‰∏Ä‰∏™Á±ªÂà´
			if (categoriesConfig.length > 0) {
				currentCategory = categoriesConfig[0].name;
				renderCategoryExamples(currentCategory);
			}
		} catch (error) {
			console.error('Error loading dataset examples:', error);
			// Â¶ÇÊûúÂä†ËΩΩÂ§±Ë¥•ÔºåÊòæÁ§∫ÈîôËØØ‰ø°ÊÅØ
			allCategoriesData = {};
		}
	}

	// ÂàùÂßãÂåñÊ†áÁ≠æÂç°
	function initializeTabs() {
		const tabsContainer = document.getElementById('categoryTabs');
		if (!tabsContainer) return;
		
		tabsContainer.innerHTML = '';
		
		categoriesConfig.forEach((config, index) => {
			const categoryData = allCategoriesData[config.name];
			if (!categoryData) return;
			
			const tabItem = document.createElement('li');
			tabItem.style.marginRight = '1em';
			
			const tabLink = document.createElement('a');
			tabLink.textContent = categoryData.displayName;
			tabLink.style.padding = '1em 2em';
			tabLink.style.textDecoration = 'none';
			tabLink.style.borderBottom = '3px solid transparent';
			tabLink.style.cursor = 'pointer';
			tabLink.style.transition = 'all 0.3s ease';
			
			// ËÆæÁΩÆÈªòËÆ§ÈÄâ‰∏≠Áä∂ÊÄÅ
			if (index === 0) {
				tabLink.style.borderBottomColor = '#ff6600';
				tabLink.style.color = '#ff6600';
				tabLink.style.fontWeight = 'bold';
			} else {
				tabLink.style.color = '#666';
			}
			
			// Ê∑ªÂä†ÁÇπÂáª‰∫ã‰ª∂
			tabLink.addEventListener('click', () => {
				switchCategory(config.name, tabLink);
			});
			
			// Ê∑ªÂä†ÊÇ¨ÂÅúÊïàÊûú
			tabLink.addEventListener('mouseenter', () => {
				if (currentCategory !== config.name) {
					tabLink.style.color = '#ff6600';
				}
			});
			
			tabLink.addEventListener('mouseleave', () => {
				if (currentCategory !== config.name) {
					tabLink.style.color = '#666';
				}
			});
			
			tabItem.appendChild(tabLink);
			tabsContainer.appendChild(tabItem);
		});
	}

	// ÂàáÊç¢Á±ªÂà´
	function switchCategory(categoryName, clickedTab) {
		if (currentCategory === categoryName) return;
		
		// Êõ¥Êñ∞Ê†áÁ≠æÂç°Ê†∑Âºè
		const allTabs = document.querySelectorAll('#categoryTabs a');
		allTabs.forEach(tab => {
			tab.style.borderBottomColor = 'transparent';
			tab.style.color = '#666';
			tab.style.fontWeight = 'normal';
		});
		
		clickedTab.style.borderBottomColor = '#ff6600';
		clickedTab.style.color = '#ff6600';
		clickedTab.style.fontWeight = 'bold';
		
		currentCategory = categoryName;
		renderCategoryExamples(categoryName);
	}

	// Ê∏≤ÊüìÊåáÂÆöÁ±ªÂà´ÁöÑÁ§∫‰æã
	function renderCategoryExamples(categoryName) {
		console.log('renderCategoryExamples called for category:', categoryName);
		const categoryData = allCategoriesData[categoryName];
		console.log('Category data:', categoryData);
		if (!categoryData) {
			console.warn('No data found for category:', categoryName);
			return;
		}
		const container = document.getElementById('examplesContainer');
		if (!container) return;
		
		container.innerHTML = '';
		
		// ÊåâÁªÑÂàÜÁªÑÊï∞ÊçÆ
		const groupedData = {};
		const groupNames = {};
		
		categoryData.data.forEach(item => {
			const groupId = item.group;
			if (!groupedData[groupId]) {
				groupedData[groupId] = [];
			}
			groupedData[groupId].push(item);
			groupNames[groupId] = item.group_name;
		});
		
		// ÂÆö‰πâÈ¢úËâ≤ÊñπÊ°à
		const colors = ['#ff6600', '#007acc', '#5cb45c', '#9b59b6', '#e74c3c'];
		
		// ‰∏∫ÊØè‰∏™ÁªÑÂàõÂª∫Âàó
		Object.keys(groupedData).sort((a, b) => a - b).forEach((groupId, index) => {
			const groupData = groupedData[groupId];
			const groupName = groupNames[groupId];
			const color = colors[index % colors.length];
			
			// ÂàõÂª∫Âàó
			const column = document.createElement('div');
			column.className = 'column is-one-third';
			column.style.padding = '1em';
			
			// ÂàõÂª∫Âç°Áâá
			const card = document.createElement('div');
			card.className = 'card';
			card.style.borderRadius = '8px';
			card.style.padding = '1em';
			card.style.boxShadow = '0 2px 10px rgba(0,0,0,0.1)';
			card.style.backgroundColor = 'white';
			card.style.height = '600px';
			card.style.overflowY = 'auto';
			
			// ÂàõÂª∫Ê†áÈ¢ò
			const title = document.createElement('h3');
			title.textContent = groupName;
			title.style.color = color;
			title.style.textAlign = 'center';
			title.style.marginBottom = '1em';
			title.style.position = 'sticky';
			title.style.top = '0';
			title.style.backgroundColor = 'white';
			title.style.paddingBottom = '0.5em';
			title.style.borderBottom = '1px solid #eee';
			
			// ÂàõÂª∫ÂÜÖÂÆπÂÆπÂô®
			const content = document.createElement('div');
			
			// Ê∑ªÂä†ÊØèËΩÆÂØπËØù
			groupData.forEach((item, roundIndex) => {
				const dialogueElement = createDialogueElement(item, roundIndex + 1, categoryName);
				content.appendChild(dialogueElement);
			});
			
			card.appendChild(title);
			card.appendChild(content);
			column.appendChild(card);
			container.appendChild(column);
		});
	}
	
	// ÂáΩÊï∞ÔºöÂ§ÑÁêÜÂçï‰∏™ÂØπËØùÂπ∂ËøîÂõû HTML ÂÖÉÁ¥†
	function createDialogueElement(item, roundNumber, categoryName) {
		console.log('createDialogueElement called with:', { item, roundNumber, categoryName });
		const roundDiv = document.createElement('div');
		roundDiv.style.marginBottom = '1.5em';
		roundDiv.style.padding = '0.5em';
		roundDiv.style.borderBottom = '1px solid #eee';

		// HTMLËΩ¨‰πâÂáΩÊï∞
		function escapeHtml(text) {
			const div = document.createElement('div');
			div.textContent = text;
			return div.innerHTML;
		}

		// Ê†ºÂºèÂåñÊñáÊú¨ÔºåÂ§ÑÁêÜÊç¢Ë°å
		function formatText(text) {
			const escapedText = escapeHtml(text);
			return escapedText.replace(/\n/g, '<br>');
		}

		// Prompt Ê†áÈ¢ò
		const promptTitle = document.createElement('h4');
		promptTitle.textContent = `Round ${roundNumber} Prompt`;
		promptTitle.style.color = '#666';
		promptTitle.style.marginBottom = '0.3em';
		promptTitle.style.fontSize = '0.95em';

		// Prompt ÊñáÊú¨
		const promptText = document.createElement('div');
		promptText.innerHTML = formatText(item.prompt);
		promptText.style.fontWeight = 'bold';
		promptText.style.marginBottom = '0.5em';
		promptText.style.fontSize = '0.95em';

		// Response Ê†áÈ¢ò
		const responseTitle = document.createElement('h4');
		responseTitle.textContent = `Round ${roundNumber} Response`;
		responseTitle.style.color = '#666';
		responseTitle.style.marginBottom = '0.3em';
		responseTitle.style.fontSize = '0.95em';

		// Response ÊñáÊú¨
		const responseText = document.createElement('div');
		responseText.innerHTML = formatText(item.response);
		responseText.style.marginBottom = '0.5em';
		responseText.style.fontSize = '0.95em';
		responseText.style.lineHeight = '1.4';

		// ÁªÑË£Ö Prompt Âå∫Âüü
		roundDiv.appendChild(promptTitle);
		roundDiv.appendChild(promptText);

		// Â¶ÇÊûú Prompt ÊúâÂõæÁâáÔºåÊòæÁ§∫ÂõæÁâáÔºàÂõ∫ÂÆöÂ§ßÂ∞èÔºâ
		if (item.imageP && item.imageP !== '#') {
			console.log('Processing prompt image:', item.imageP, 'for category:', categoryName);
			const promptImg = document.createElement('img');
			// ÊûÑÂª∫Ê≠£Á°ÆÁöÑÂõæÁâáË∑ØÂæÑ
			let imagePath;
			if (item.imageP.startsWith('./images/')) {
				// Ê†ºÂºè: "./images/1.png" -> "./multi_turn_qa_examples/categoryName/images/1.png"
				imagePath = `./multi_turn_qa_examples/${categoryName}/images/${item.imageP.replace('./images/', '')}`;
			} else if (item.imageP.startsWith('./')) {
				// Ê†ºÂºè: "./xxx" -> "./multi_turn_qa_examples/categoryName/xxx"
				imagePath = `./multi_turn_qa_examples/${categoryName}/${item.imageP.replace('./', '')}`;
			} else {
				// ÂÖ∂‰ªñÊ†ºÂºè
				imagePath = item.imageP;
			}
			console.log(`Setting prompt image path: ${imagePath} (original: ${item.imageP})`);
			promptImg.src = imagePath;
			console.log('Final prompt img.src set to:', promptImg.src);
			promptImg.alt = `Round ${roundNumber} ÂõæÁâá`;
			promptImg.style.width = '100%';
			promptImg.style.height = '150px';
			promptImg.style.objectFit = 'cover';
			promptImg.style.marginBottom = '0.5em';
			promptImg.style.borderRadius = '4px';
			promptImg.onerror = function() {
				this.style.display = 'none';
				console.warn(`Image not found: ${this.src}`);
			};
			roundDiv.appendChild(promptImg);
		}

		// ÁªÑË£Ö Response Âå∫Âüü
		roundDiv.appendChild(responseTitle);
		roundDiv.appendChild(responseText);

		// Â¶ÇÊûú Response ÊúâÂõæÁâáÔºåÊòæÁ§∫ÂõæÁâáÔºàÂõ∫ÂÆöÂ§ßÂ∞èÔºâ
		if (item.imageR && item.imageR !== '#') {
			const responseImg = document.createElement('img');
			// ÊûÑÂª∫Ê≠£Á°ÆÁöÑÂõæÁâáË∑ØÂæÑ
			let imagePath;
			if (item.imageR.startsWith('./images/')) {
				// Ê†ºÂºè: "./images/1.png" -> "./multi_turn_qa_examples/categoryName/images/1.png"
				imagePath = `./multi_turn_qa_examples/${categoryName}/images/${item.imageR.replace('./images/', '')}`;
			} else if (item.imageR.startsWith('./')) {
				// Ê†ºÂºè: "./xxx" -> "./multi_turn_qa_examples/categoryName/xxx"
				imagePath = `./multi_turn_qa_examples/${categoryName}/${item.imageR.replace('./', '')}`;
			} else {
				// ÂÖ∂‰ªñÊ†ºÂºè
				imagePath = item.imageR;
			}
			console.log(`Setting response image path: ${imagePath} (original: ${item.imageR})`);
			responseImg.src = imagePath;
			responseImg.alt = `Round ${roundNumber} ÂõûÂ§çÂõæÁâá`;
			responseImg.style.width = '100%';
				responseImg.style.height = '150px';
				responseImg.style.objectFit = 'cover';
				responseImg.style.marginTop = '0.5em';
				responseImg.style.borderRadius = '4px';
				responseImg.onerror = function() {
					this.style.display = 'none';
					console.warn(`Image not found: ${this.src}`);
				};
			roundDiv.appendChild(responseImg);
		}

		return roundDiv;
	}

	// È°µÈù¢Âä†ËΩΩÂÆåÊàêÂêéÂä†ËΩΩÊï∞ÊçÆ
	document.addEventListener('DOMContentLoaded', loadAllCategoriesData);
</script>
<section>
	<div class="container" style="margin-bottom: 2vh;">
		<div class="columns is-centered has-text-centered">
			<div class="column is-four-fifths">
				<h2 class="title is-3" id="Evaluation" style="text-align: center;"> Case Study </h2>
				<p style="text-align: justify;">This case study revealed that the two models fall short in image-text consistency, instruction following, multi-turn interaction, and image quality. By contrast, our dataset enables <strong>better adherence to user intent through high-quality
						multimodal understanding and generation.</strong></p>
				<div id="dialog-sample-container"></div>
				<svg style="display:none;">
					<symbol id="icon-gemini" viewBox="0 0 24 24">
						<?xml version="1.0" ?><svg id="Apple" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg">
							<defs>
								<style>
									.cls-1{fill:#f0432c;}.cls-2{fill:#ffbf00;}.cls-3{fill:#24aa49;}.cls-4{fill:#3384fc;}
								</style>
							</defs>
							<title />
							<path class="cls-1" d="M90,93.25l65.06,65.06a140.31,140.31,0,0,1,201-1v0l65-65.09a232.33,232.33,0,0,0-331,1Z" />
							<path class="cls-2" d="M90.85,419.76l65-65a140.34,140.34,0,0,1,1-198.71L91.81,91a232.35,232.35,0,0,0-1,328.8Z" />
							<polygon class="cls-3" points="301.98 301.84 301.98 302.8 302.94 302.8 301.98 301.84" />
							<path class="cls-3" d="M255.5,488.21a231.65,231.65,0,0,0,164.55-68.3L355,354.84a140.3,140.3,0,0,1-199,0l-65,65A231.65,231.65,0,0,0,255.5,488.21Z" />
							<path class="cls-4" d="M484.36,210.33l-181.42.25V302.8h85.84a140.7,140.7,0,0,1-33.69,52.91l65.05,65.06a232.65,232.65,0,0,0,64.22-210.44Z" />
						</svg>
					</symbol>
					<symbol id="icon-janus" viewBox="0 0 24 24">
						<svg xmlns="http://www.w3.org/2000/svg" shape-rendering="geometricPrecision" text-rendering="geometricPrecision" image-rendering="optimizeQuality" fill-rule="evenodd" clip-rule="evenodd" viewBox="0 0 512 509.64">
							<path fill="#fff" d="M115.612 0h280.775C459.974 0 512 52.026 512 115.612v278.415c0 63.587-52.026 115.613-115.613 115.613H115.612C52.026 509.64 0 457.614 0 394.027V115.612C0 52.026 52.026 0 115.612 0z" />
							<path fill="#4D6BFE" fill-rule="nonzero"
								d="M440.898 139.167c-4.001-1.961-5.723 1.776-8.062 3.673-.801.612-1.479 1.407-2.154 2.141-5.848 6.246-12.681 10.349-21.607 9.859-13.048-.734-24.192 3.368-34.04 13.348-2.093-12.307-9.048-19.658-19.635-24.37-5.54-2.449-11.141-4.9-15.02-10.227-2.708-3.795-3.447-8.021-4.801-12.185-.861-2.509-1.725-5.082-4.618-5.512-3.139-.49-4.372 2.142-5.601 4.349-4.925 9.002-6.833 18.921-6.647 28.962.432 22.597 9.972 40.597 28.932 53.397 2.154 1.47 2.707 2.939 2.032 5.082-1.293 4.41-2.832 8.695-4.186 13.105-.862 2.817-2.157 3.429-5.172 2.205-10.402-4.346-19.391-10.778-27.332-18.553-13.481-13.044-25.668-27.434-40.873-38.702a177.614 177.614 0 00-10.834-7.409c-15.512-15.063 2.032-27.434 6.094-28.902 4.247-1.532 1.478-6.797-12.251-6.736-13.727.061-26.285 4.653-42.288 10.777-2.34.92-4.801 1.593-7.326 2.142-14.527-2.756-29.608-3.368-45.367-1.593-29.671 3.305-53.368 17.329-70.788 41.272-20.928 28.785-25.854 61.482-19.821 95.59 6.34 35.943 24.683 65.704 52.876 88.974 29.239 24.123 62.911 35.943 101.32 33.677 23.329-1.346 49.307-4.468 78.607-29.27 7.387 3.673 15.142 5.144 28.008 6.246 9.911.92 19.452-.49 26.839-2.019 11.573-2.449 10.773-13.166 6.586-15.124-33.915-15.797-26.47-9.368-33.24-14.573 17.235-20.39 43.213-41.577 53.369-110.222.8-5.448.121-8.877 0-13.287-.061-2.692.553-3.734 3.632-4.041 8.494-.981 16.742-3.305 24.314-7.471 21.975-12.002 30.84-31.719 32.933-55.355.307-3.612-.061-7.348-3.879-9.245v-.003zM249.4 351.89c-32.872-25.838-48.814-34.352-55.4-33.984-6.155.368-5.048 7.41-3.694 12.002 1.415 4.532 3.264 7.654 5.848 11.634 1.785 2.634 3.017 6.551-1.784 9.493-10.587 6.55-28.993-2.205-29.856-2.635-21.421-12.614-39.334-29.269-51.954-52.047-12.187-21.924-19.267-45.435-20.435-70.542-.308-6.061 1.478-8.207 7.509-9.307 7.94-1.471 16.127-1.778 24.068-.615 33.547 4.9 62.108 19.902 86.054 43.66 13.666 13.531 24.007 29.699 34.658 45.496 11.326 16.778 23.514 32.761 39.026 45.865 5.479 4.592 9.848 8.083 14.035 10.656-12.62 1.407-33.673 1.714-48.075-9.676zm15.899-102.519c.521-2.111 2.421-3.658 4.722-3.658a4.74 4.74 0 011.661.305c.678.246 1.293.614 1.786 1.163.861.859 1.354 2.083 1.354 3.368 0 2.695-2.154 4.837-4.862 4.837a4.748 4.748 0 01-4.738-4.034 5.01 5.01 0 01.077-1.981zm47.208 26.915c-2.606.996-5.2 1.778-7.707 1.88-4.679.244-9.787-1.654-12.556-3.981-4.308-3.612-7.386-5.631-8.679-11.941-.554-2.695-.247-6.858.246-9.246 1.108-5.144-.124-8.451-3.754-11.451-2.954-2.449-6.711-3.122-10.834-3.122-1.539 0-2.954-.673-4.001-1.224-1.724-.856-3.139-3-1.785-5.634.432-.856 2.525-2.939 3.018-3.305 5.6-3.185 12.065-2.144 18.034.244 5.54 2.266 9.727 6.429 15.759 12.307 6.155 7.102 7.263 9.063 10.773 14.39 2.771 4.163 5.294 8.451 7.018 13.348.877 2.561.071 4.74-2.341 6.277-.981.625-2.109 1.044-3.191 1.458z" />
						</svg>
					</symbol>
					<symbol id="icon-dataset" viewBox="0 0 24 24" fill="currentColor">
						<svg id="Layer_1" data-name="Layer 1" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 122.88 116.41">
							<defs>
								<style>
									.cls-1{fill-rule:evenodd;}
								</style>
							</defs>
							<title>database-table</title>
							<path class="cls-1"
								d="M16.49,24.88C24.05,27.41,34.57,29,46.26,29S68.48,27.41,76,24.88c6.63-2.22,10.73-4.9,10.73-7.52S82.67,12.06,76,9.84C68.48,7.33,58,5.75,46.27,5.75S24.06,7.33,16.49,9.84c-14.06,4.7-14.46,10.21,0,15ZM64.91,55.34h48.73a9.27,9.27,0,0,1,9.24,9.24v42.58a9.27,9.27,0,0,1-9.24,9.25H64.91a9.27,9.27,0,0,1-9.24-9.25V64.58a9.27,9.27,0,0,1,9.24-9.24ZM91.09,99.18H118v12H91.09v-12Zm-30.89,0H87.13v12H60.2v-12Zm0-31.89H87.13v12H60.2v-12Zm0,15.94H87.13v12H60.2v-12ZM91.09,67.29H118v12H91.09v-12Zm0,15.94H118v12H91.09v-12ZM5.82,45.77c.52,2.45,4.5,4.91,10.68,7,7.22,2.42,17.16,3.95,28.24,4.08v5.77c-11.67-.13-22.25-1.78-30.05-4.39A35.86,35.86,0,0,1,5.84,54V71.27c.52,2.45,4.5,4.91,10.68,7,7.22,2.4,17.15,3.94,28.22,4.07v5.75c-11.67-.14-22.25-1.78-30.05-4.4A36.08,36.08,0,0,1,5.83,79.5V96.75c.52,2.45,4.51,4.91,10.68,7,7.22,2.41,17.16,4,28.23,4.08v5.75c-11.67-.13-22.24-1.78-30-4.4C10.4,107.72,0,103,0,97.38V95.55C0,69.86,0,43.06,0,17.41c0-5.43,5.61-10,14.66-13C22.82,1.68,34,0,46.27,0S69.7,1.68,77.87,4.41s13.64,6.78,14.55,11.53a3,3,0,0,1,.16,1v28.6H86.8V26.09a36.69,36.69,0,0,1-8.93,4.22c-8.15,2.75-19.31,4.41-31.58,4.41S22.83,33,14.66,30.31A36.26,36.26,0,0,1,5.8,26.14V45.77Z" />
						</svg>
					</symbol>
				</svg>
				<div class="diag-dropdown-container">
					<label for="exampleSelect">Select Cases: </label>
					<select id="exampleSelect"></select>
				</div>
				<div class="diag-container">
					<div id="Gemini" class="diag-section diag-default">
						<div class="diag-section-header diag-default">
							<svg>
								<use xlink:href="#icon-gemini"></use>
							</svg>
							<span class="highlight">Gemini ‚ñº</span>
						</div>
						<div class="diag-content"></div>
					</div>
					<div id="Janus" class="diag-section diag-default">
						<div class="diag-section-header diag-default">
							<svg>
								<use xlink:href="#icon-janus"></use>
							</svg>
							<span class="highlight">Janus ‚ñº</span>
						</div>
						<div class="diag-content"></div>
					</div>
					<div id="Ours" class="diag-section diag-orange">
						<div class="diag-section-header diag-orange">
							<svg>
								<use xlink:href="#icon-dataset"></use>
							</svg>
							<span class="highlight">Ours ‚ñº</span>
						</div>
						<div class="diag-content"></div>
					</div>
				</div>
				<script>
					const data = [
								{
									"model": "Janus",
									"title": "Case 1",
									"figures": 1,
									"prompt": "What immediate actions would you recommend for the local authorities to take in response to the situation depicted in the image, and why?",
									"response": "The image depicts a flooded area, with water covering the street and reflecting surrounding trees and buildings. The floodwater appears to be at a significant depth, potentially affecting traffic and access to homes. Immediate actions for local authorities to take in response to this situation would include: 1. *Evacuation and Safety Measures*: Ensure the safety of residents by providing alternative shelters and evacuation routes. This includes setting up emergency shelters and coordinating with local emergency services. 2. *Flood Control and Drainage*: Activate flood control measures such as opening storm drains and releasing water from nearby reservoirs or flood management systems to reduce water accumulation. 3. *Communication and Coordination*: Establish communication channels with affected residents to provide updates on the flood situation, safety instructions, and resources available. 4. *Damage Assessment and Repair*: Send teams to assess the extent of the damage and initiate repair efforts to restore infrastructure, such as roads, bridges, and utilities. 5. *Environmental Monitoring*: Monitor water quality and potential contamination risks to ensure public health and safety.",
									"imageR": "./assets/diag_sample/image1.jpg",
									"imageP": "#",
									"color": "default"
								},
								{
									"model": "Janus",
									"title": "Case 1",
									"figures": 1,
									"prompt": "Please tell me how to build temporary roads or bridges, and generate one in the diagram.",
									"response": "To build temporary roads or bridges, follow these steps: 1. *Assessment and Planning*: Evaluate the flood-affected area to determine the most suitable locations for temporary roads or bridges. Consider factors such as water depth, flow velocity, and structural stability. 2. *Materials and Equipment*: Gather materials like steel beams, concrete blocks, and construction tools. Ensure that the equipment is suitable for the water conditions, such as using waterproof materials and equipment. 3. *Foundation Preparation*: Prepare the ground by clearing debris and leveling the surface. Ensure that the foundation is stable and can support the weight of the temporary structure. 4. *Construction*: Construct the temporary road or bridge using the prepared materials. Use strong materials to ensure durability and safety. 5. *Testing and Maintenance*: Test the temporary structure for stability and safety. Regularly maintain it to ensure it remains functional and safe for use.",
									"imageR": "./assets/diag_sample/image2.jpg",
									"imageP": "#",
									"color": "default"
								},
								{
									"model": "Janus",
									"title": "Case 1",
									"figures": 3,
									"prompt": "How long does it take to build such a bridge?",
									"response": "Building a temporary bridge or road in flood-affected areas requires careful planning and execution. The time taken depends on several factors, including the complexity of the structure, the depth of the water, and the availability of materials and equipment. 1. *Planning and Assessment*: This phase typically takes a few days to a week. It involves assessing the flood conditions, determining the best location for the bridge or road, and planning the construction process. 2. *Material Procurement*: Gathering materials such as steel beams, concrete blocks, and construction tools can take a few days. 3. *Construction*: Building the bridge or road can take several weeks to a few months, depending on the complexity and the number of workers involved. 4. *Testing and Maintenance*: Once completed, the bridge or road needs to be tested for safety and durability. Regular maintenance is required to ensure its longevity. Overall, the construction of a temporary bridge or road in flood-affected areas can take anywhere from a few weeks to several months.",
									"imageR": "#",
									"imageP": "#",
									"color": "default"
								},
								{
									"model": "Gemini",
									"title": "Case 1",
									"figures": 1,
									"prompt": "What immediate actions would you recommend for the local authorities to take in response to the situation depicted in the image, and why?",
									"response": "The image depicts a street scene with significant floodwater inundating buildings and vehicles. Some vehicles appear partially submerged. *Immediate Actions for Local Authorities:* 1. *Issue Emergency Alerts and Warnings:* Utilize all available channels (sirens, SMS, social media, radio, television) to inform residents about the dangerous flooding and advise them to seek higher ground immediately. Emphasize the risks of driving or walking through floodwaters. 2. *Activate Emergency Response Teams:* Deploy police, fire department, and emergency medical services to the affected areas. Their immediate priorities should be: * *Life Safety:* Conducting search and rescue operations for individuals trapped in homes or vehicles. * *Medical Assistance:* Providing aid to anyone injured. * *Security:* Securing the flooded areas to prevent looting and ensure public safety. 3. *Establish Shelters:* Open designated emergency shelters (schools, community centers, etc.) for displaced residents, providing them with temporary housing, food, water, and essential supplies. 4. *Assess the Situation and Infrastructure:* Begin assessing the extent of the damage to roads, bridges, power lines, and other critical infrastructure. This will help in understanding the scope of the disaster and planning the subsequent response. 5. *Block Off Affected Roads:* Close roads leading into and within the flooded areas to prevent further traffic and potential accidents in the dangerous conditions. *Why These Actions Are Crucial:* These immediate actions prioritize the safety and well-being of the affected population. Swift warnings and the deployment of emergency services are essential to minimize casualties and injuries. Providing shelter addresses the immediate needs of those displaced from their homes. Assessing the damage and securing the area lays the groundwork for a coordinated and effective longer-term recovery effort. Preventing access to flooded areas reduces the risk of further harm and allows emergency responders to work more efficiently.",
									"imageR": "./assets/diag_sample/image3.png",
									"imageP": "#",
									"color": "default"
								},
								{
									"model": "Gemini",
									"title": "Case 1",
									"figures": 1,
									"prompt": "In addition to these immediate actions, what long-term strategies could the local authorities implement to prevent or mitigate future flooding incidents in this area?",
									"response": "I will generate an image depicting a flooded street scene to help illustrate potential immediate actions for local authorities.",
									"imageR": "./assets/diag_sample/image4.png",
									"imageP": "#",
									"color": "default"
								},
								{
									"model": "Gemini",
									"title": "Case 1",
									"figures": 1,
									"prompt": "It seems that your image does not show any long-term strategies, please supplement with textual description.",
									"response": "Immediate Actions:\n* Emergency Response Activation: Deploy emergency services, including police, fire department, and ambulance, to the affected areas.\n* Evacuation and Shelter: Initiate evacuation procedures for residents in high-risk zones, providing safe shelter and essential supplies.\n* Road Closure and Traffic Management: Close flooded roads and manage traffic flow to ensure safety and allow access for emergency vehicles.\n* Damage Assessment: Begin preliminary assessments of the extent of the flooding and damage to infrastructure and property.\n* Communication and Information: Keep the public informed about the situation, safety measures, and available assistance through various channels.\n* Resource Mobilization: Activate and coordinate resources, including personnel, equipment, and supplies, for rescue and relief efforts.",
									"imageR": "./assets/diag_sample/image5.png",
									"imageP": "#",
									"color": "default"
								},
								{
									"model": "Ours",
									"title": "Case 1",
									"figures": 3,
									"prompt": "What immediate actions would you recommend for the local authorities to take in response to the situation depicted in the image, and why?",
									"response": "The image shows a flooded street with a traffic light and power lines submerged in water. Based on the current state, I would recommend that the local authorities take immediate actions to ensure the safety of residents and maintain traffic flow.\n\nFirst and foremost, they should deploy emergency personnel to the affected areas to rescue people that might be in danger and ensure their safety. Given the possible risk of electrocution from the submerged power lines, it is vital to shut off and secure the power supply to the area.\n\nNext, the local authorities should assess the traffic light's functionality and ensure it remains operational to manage the flow of vehicles and pedestrians. If necessary, they may need to replace or repair the damaged traffic light and lines to restore proper functionality.\n\nFurthermore, they should work on clearing and draining the water accumulated in the streets to minimize the risk of infections and to restore normality to daily life. They might consider using specialized machinery and personnel for this task, if the situation is severe or widespread.\n\nLastly, the local authorities should communicate their actions and plans effectively to the public via social media, emergency alerts, and notifications to keep residents informed about the situation and any assistance or resources available.\n\nConclusion: Look for opportunities to improve the traffic flow, safety, and well-being of the community during the recovery process.",
									"imageR": "#",
									"imageP": "#",
									"color": "yellow"
								},
								{
									"model": "Ours",
									"title": "Case 1",
									"figures": 3,
									"prompt": "In addition to these immediate actions, what long-term strategies could the local authorities implement to prevent or mitigate future flooding incidents in this area?",
									"response": "Based on the previous response, I would recommend that the local authorities in the area take immediate actions to ensure the safety of residents and maintain traffic flow in the flooded street. To achieve this, they should deploy emergency personnel to rescue people that may be in danger and ensure their safety. Given the possible risk of electrocution from the submerged power lines, it is crucial to shut off and secure the power supply to the area. \n\nIn addition to these immediate actions, long-term strategies could include implementing flood-prevention measures, such as improving the city's drainage system, raising roads and infrastructure to minimize the impact of flooding, creating emergency response plans, and conducting regular maintenance checks on traffic lights and power lines to reduce the risk of damage during extreme weather events.\n\nThe implementation of these measures can help prevent or mitigate future flooding incidents in the area, ensuring the safety and well-being of residents and minimizing the disruption to daily life.",
									"imageR": "#",
									"imageP": "#",
									"color": "yellow"
								},
								{
									"model": "Ours",
									"title": "Case 1",
									"figures": 1,
									"prompt": "That's a great overview. Could you elaborate on how the local authorities might go about 'improving the city's drainage system'? What specific improvements or technologies might be considered?",
									"response": "To improve the city's drainage system and mitigate future flooding effectively, local authorities can consider several specific improvements and technologies:\n\n1. *Upgrading and Expanding Drainage Infrastructure*: By increasing the capacity of existing drainage systems and constructing additional channels, the city can better accommodate heavy rainfall and reduce flood risk. This might involve widening culverts, enlarging stormwater pipes, and constructing detention basins to manage water flow more effectively.\n\n2. *Implementing Green Infrastructure*: Integrating green technology such as rain gardens, permeable pavements, and green roofs can help absorb and divert rainwater naturally. These solutions not only reduce immediate drainage loads but also improve urban aesthetics and biodiversity.\n\n3. *Advanced Hydrological Modeling*: Utilizing sophisticated software to simulate rainfall scenarios can help identify potential weaknesses in drainage and plan infrastructure improvements more strategically. This modeling aids in efficient planning and resource allocation.\n\n4. *Sensor-Based Monitoring Systems*: Implementing sensor technology in critical drainage locations can provide real-time data on water levels and system performance. This allows for proactive management and quick responses during heavy rain events.\n\n5. *Public Awareness and Engagement*: Educating citizens about flood risks and encouraging practices that reduce blockage in drainage systems, like proper disposal of waste, can significantly improve overall efficiency.\n\n6. *Regular Maintenance and Cleaning*: Conducting frequent checks and cleaning of drainage systems ensures they remain clear of debris, reducing the likelihood of blockages during storms.\n\n7. *Infrastructure Integration*: Consideration of multi-use designs that integrate drainage systems with urban features can both save space and enhance functionality, such as park spaces that double as flood basins.\n\nThese strategies, when combined, can create a robust drainage system tailored to the city's unique needs, reducing the impact of future flooding events. \n\nWould you like more information on the benefits of green infrastructure?",
									"imageR": "./assets/diag_sample/image6.png",
									"imageP": "#",
									"color": "yellow"
								},
								{
									"model": "Gemini",
									"title": "Case 2",
									"figures": 2,
									"prompt": "Considering the cleanliness and arrangement, what can you infer about the maintenance of this bathroom?",
									"response": "The bathroom appears to be meticulously maintained. The surfaces are clear of clutter and look clean, suggesting regular tidying. The arrangement of items seems deliberate and organized, further indicating a commitment to upkeep.",
									"imageR": "./assets/diag_sample/image8.png",
									"imageP": "./assets/diag_sample/image7.png",
									"color": "default"
								},
								{
									"model": "Gemini",
									"title": "Case 2",
									"figures": 0,
									"prompt": "The bathroom image you generated looks a bit cluttered. What kind of updates or additions could enhance the bathroom's functionality or aesthetics?",
									"response": "The current image shows a bathroom with some items on the counter and a few things in the shower area. Updates to enhance functionality and aesthetics could include adding closed storage like drawers or cabinets to reduce visible clutter, incorporating a consistent color scheme for towels and accessories, and perhaps a larger mirror to visually expand the space.",
									"imageR": "#",
									"imageP": "./assets/diag_sample/image9.png",
									"color": "default"
								},
								{
									"model": "Gemini",
									"title": "Case 2",
									"figures": 1,
									"prompt": "That's helpful! Considering the potential for adding storage or decorative elements, how would you prioritize these additions based on cost-effectiveness and impact on the bathroom's overall appeal?",
									"response": "The image displays a clean and relatively well-maintained bathroom. The sink area is tidy, and the surfaces appear to be clear of excessive clutter. This suggests a regular cleaning routine. The presence of what looks like a toothbrush holder and a neatly placed hand towel indicates attention to organization.",
									"imageR": "./assets/diag_sample/image10.png",
									"imageP": "#",
									"color": "default"
								},
								{
									"model": "Janus",
									"title": "Case 2",
									"figures": 2,
									"prompt": "Considering the cleanliness and arrangement, what can you infer about the maintenance of this bathroom?",
									"response": "The bathroom appears to be well-maintained, as evidenced by the cleanliness of the tiles, the organized placement of items, and the absence of visible dirt or clutter. The mirror is clean, and the sink and surrounding area are tidy, suggesting regular upkeep. The presence of a trash bin and a toilet paper holder further indicates attention to cleanliness and organization.",
									"imageR": "./assets/diag_sample/image12.png",
									"imageP": "./assets/diag_sample/image11.png",
									"color": "default"
								},
								{
									"model": "Janus",
									"title": "Case 2",
									"figures": 1,
									"prompt": "The toilet picture you generated looks dark and plain. What kind of updates or additions could enhance the bathroom's functionality or aesthetics?",
									"response": "To enhance the functionality and aesthetics of the bathroom, several updates or additions could be considered:\n\n1. Lighting: Adding a brighter or more modern light fixture above the mirror could improve visibility and create a more inviting atmosphere.\n2. Storage Solutions: Installing additional shelves or cabinets could provide more storage space for toiletries and other bathroom essentials.\n3. Decorative Elements: Introducing decorative elements such as a small plant, artwork, or a stylish soap dispenser could add personality and warmth to the space.\n4. Color Scheme: Updating the tiles or adding colorful accents could refresh the overall look and feel of the bathroom.\n5. Mirror Frame: A decorative frame for the mirror could add a touch of elegance and make the space feel more cohesive.",
									"imageR": "./assets/diag_sample/image13.png",
									"imageP": "#",
									"color": "default"
								},
								{
									"model": "Janus",
									"title": "Case 2",
									"figures": 3,
									"prompt": "Please present the complete scene of this toilet to me, not just the sink.",
									"response": "The complete scene of this toilet includes a white ceramic toilet with a standard flush mechanism, positioned against tiled walls. Above the toilet, there is a rectangular mirror with a simple frame, reflecting part of the room. To the right of the toilet, there is a wall-mounted metal rack with a wire basket, likely intended for holding toilet paper rolls. The floor is tiled in a light color, matching the walls, and there is a small trash bin with a lid located near the toilet. The overall design is functional and minimalistic, with a focus on cleanliness and practicality.",
									"imageR": "#",
									"imageP": "#",
									"color": "default"
								},
								{
									"model": "Ours",
									"title": "Case 2",
									"figures": 1,
									"prompt": "Considering the cleanliness and arrangement, what can you infer about the maintenance of this bathroom?",
									"response": "Based on the cleanliness of the sink area, bright white room, and mirror, it can be inferred that the bathroom is well-maintained. The white sink, white tiles, and white curtains contribute to a clean and organized appearance, which suggests that the space is cared for. Another notable aspect of the bathroom is the empty wall space, also shown in the image, which could indicate that the bathroom might need to be updated or finished. Overall, the cleanliness and arrangement of the bathroom seem to show good maintenance practices.",
									"imageR": "#",
									"imageP": "./assets/diag_sample/image14.png",
									"color": "yellow"
								},
								{
									"model": "Ours",
									"title": "Case 2",
									"figures": 1,
									"prompt": "You mentioned that the bathroom might need to be updated or finished due to the empty wall space. What kind of updates or additions could enhance the bathroom's functionality or aesthetics?",
									"response": "Updating or finishing the empty wall space in the bathroom could enhance its functionality and aesthetics. Some possible additions or updates could include installing additional storage, such as shelves or cabinets, to organize toiletries and accessories; installing a mirror above the sink to create the illusion of a larger space; or even incorporating decorative elements like artwork or plants to add visual interest and make the bathroom feel more inviting.",
									"imageR": "./assets/diag_sample/image15.jpg",
									"imageP": "#",
									"color": "yellow"
								},
								{
									"model": "Ours",
									"title": "Case 2",
									"figures": 3,
									"prompt": "That's helpful! Considering the potential for adding storage or decorative elements, how would you prioritize these additions based on cost-effectiveness and impact on the bathroom's overall appeal?",
									"response": "To prioritize additions based on cost-effectiveness and impact on the bathroom's overall appeal, I recommend starting with functional improvements that also enhance aesthetics. Here are some suggestions:\n\n1. Additional Storage: Installing floating shelves or a vanity with built-in cabinets can provide much-needed storage while maintaining a sleek look. This is both practical and visually appealing.\n\n2. Mirror Above Sink: Adding a mirror above the sink not only serves a practical purpose but also makes the space feel larger and brighter.\n\n3. Decorative Elements: Incorporating small, affordable items like a plant or a piece of artwork can add personality and warmth to the bathroom without breaking the bank.\n\n4. Lighting: Upgrading the lighting fixture can significantly improve the ambiance of the bathroom. Opt for energy-efficient LED lights that offer both cost savings and better illumination.\n\n5. Tile Accents: Adding a tile backsplash or border can give the bathroom a fresh look without requiring a full renovation.\n\nEach of these additions can be tailored to fit your budget and style preferences, ensuring a balanced approach to enhancing both functionality and aesthetics. Would you like to see some visual examples of these ideas?",
									"imageR": "#",
									"imageP": "#",
									"color": "yellow"
								}
								];
					
					const exampleSelect = document.getElementById('exampleSelect');
					const uniqueTitles = [...new Set(data.map(item => item.title))];
					uniqueTitles.forEach(title => {
					  const option = document.createElement('option');
					  option.value = title;
					  option.textContent = title;
					  exampleSelect.appendChild(option);
					});
					
					function parseBold(text) {
					  return text.replace(/\*\*(.*?)\*\*/g, '<strong>$1</strong>');
					}
					
					function parseOrderedList(text) {
					    const lines = text.split('\n');
					    let htmlOutput = '';
					    let currentListItemContent = [];
					    let inListScope = false;
					    let hasOrderedListPattern = false;
					
					    // È¶ñÂÖàÊ£ÄÊü•ÊòØÂê¶ÂåÖÂê´ÊúâÂ∫èÂàóË°®Ê®°Âºè
					    for (const line of lines) {
					        if (/^\d+\.\s/.test(line)) {
					            hasOrderedListPattern = true;
					            break;
					        }
					    }
					
					    // Â¶ÇÊûúÊ≤°ÊúâÊúâÂ∫èÂàóË°®Ê®°ÂºèÔºåÁõ¥Êé•ËøîÂõûÊÆµËêΩÊ†ºÂºè
					    if (!hasOrderedListPattern) {
					        return '<p>' + parseBold(text.replace(/\n/g, '<br>')) + '</p>';
					    }
					
					    function flushListItem() {
					        if (currentListItemContent.length > 0) {
					            htmlOutput += `<li>${parseBold(currentListItemContent.join('<br>'))}</li>`;
					            currentListItemContent = [];
					        }
					    }
					
					    function startList() {
					        if (!inListScope) {
					            htmlOutput += '<ol>';
					            inListScope = true;
					        }
					    }
					
					    function endList() {
					        if (inListScope) {
					            flushListItem();
					            htmlOutput += '</ol>';
					            inListScope = false;
					        }
					    }
					
					    for (const line of lines) {
					        if (/^\d+\.\s/.test(line)) {
					            startList();
					            flushListItem();
					            currentListItemContent.push(line.replace(/^\d+\.\s/, ''));
					        } else if (inListScope && line.trim() === '' && currentListItemContent.length > 0) {
					            currentListItemContent.push('');
					        } else if (inListScope && currentListItemContent.length > 0) {
					             currentListItemContent.push(line);
					        } else {
					            endList();
					            if (line.trim() !== '') {
					                htmlOutput += `<p>${parseBold(line)}</p>`;
					            }
					        }
					    }
					    endList();
					    
					    return htmlOutput;
					}
					
					function typeWriter(html, el, callback) {
					  el.innerHTML = '';
					  const tempDiv = document.createElement('div');
					  tempDiv.innerHTML = html;
					  let charDelay = 5;
					
					  let currentNode = tempDiv.firstChild;
					  function processNextRootNode() {
					    if (currentNode) {
					      const next = currentNode.nextSibling;
					      typeNodeRecursive(currentNode, el, () => {
					        currentNode = next;
					        processNextRootNode();
					      });
					    } else {
					      callback && callback();
					    }
					  }
					
					  function typeNodeRecursive(sourceNode, targetParent, doneCallback) {
					    if (sourceNode.nodeType === Node.TEXT_NODE) {
					      let text = sourceNode.textContent;
					      let i = 0;
					      function typeChar() {
					        if (i < text.length) {
					          targetParent.appendChild(document.createTextNode(text[i]));
					          i++;
					          setTimeout(typeChar, charDelay);
					        } else {
					          doneCallback();
					        }
					      }
					      typeChar();
					    } else if (sourceNode.nodeType === Node.ELEMENT_NODE) {
					      const newElement = document.createElement(sourceNode.nodeName);
					      for (let attr of sourceNode.attributes) {
					        newElement.setAttribute(attr.name, attr.value);
					      }
					      targetParent.appendChild(newElement);
					
					      let childNode = sourceNode.firstChild;
					      function processChildren() {
					        if (childNode) {
					          const nextChild = childNode.nextSibling;
					          typeNodeRecursive(childNode, newElement, () => {
					            childNode = nextChild;
					            processChildren();
					          });
					        } else {
					          doneCallback();
					        }
					      }
					      processChildren();
					    } else {
					      doneCallback();
					    }
					  }
					  processNextRootNode();
					}
					
					function createDialogueElement(item) {
					  const dialogueContainer = document.createElement('div');
					  dialogueContainer.className = 'diag-dialogue';
					
					  const promptDiv = document.createElement('div');
					  promptDiv.className = 'diag-prompt';
					  promptDiv.textContent = item.prompt;
					  dialogueContainer.appendChild(promptDiv);
					
					  if (item.imageP && item.imageP !== '#') {
					    const promptImg = document.createElement('img');
					    promptImg.src = item.imageP.replace(/\\/g, '/');
					    promptImg.alt = "Prompt image";
					    promptImg.onerror = function() {
					      this.style.display = 'none';
					      console.warn(`Image not found: ${this.src}`);
					    };
					    promptDiv.appendChild(promptImg);
					  }
					
					  const responseContainer = document.createElement('div');
					  responseContainer.className = 'diag-response';
					
					  const responseTextDiv = document.createElement('div');
					  responseTextDiv.className = 'diag-response-text-area diag-typing-caret';
					  
					  let responseImgElement = null;
					  if (item.imageR && item.imageR !== '#') {
					    responseImgElement = document.createElement('img');
					    responseImgElement.src = item.imageR.replace(/\\/g, '/');
					    responseImgElement.alt = "Response image";
					    responseImgElement.style.display = 'none';
					    responseImgElement.onerror = function() {
					      this.style.display = 'none';
					      console.warn(`Image not found: ${this.src}`);
					    };
					  }
					
					  responseContainer.appendChild(responseTextDiv);
					  if (responseImgElement) {
					      responseContainer.appendChild(responseImgElement);
					  }
					  
					  dialogueContainer.appendChild(responseContainer);
					
					  const processedResponseText = parseOrderedList(item.response);
					
					  typeWriter(processedResponseText, responseTextDiv, () => {
					    responseTextDiv.classList.remove('diag-typing-caret');
					    if (responseImgElement) {
					      responseImgElement.style.display = 'block';
					    }
					  });
					
					  return dialogueContainer;
					}
					
					function renderExample(title) {
					  const sections = {
					    Gemini: document.querySelector('#Gemini .diag-content'), // Prefixed
					    Janus: document.querySelector('#Janus .diag-content'),  // Prefixed
					    Ours: document.querySelector('#Ours .diag-content')     // Prefixed
					  };
					
					  document.getElementById('Gemini').className = 'diag-section diag-default'; // Prefixed
					  document.querySelector('#Gemini .diag-section-header').className = 'diag-section-header diag-default'; // Prefixed
					  document.querySelector('#Gemini .diag-section-header span').textContent = 'Gemini ‚ñº';
					
					  document.getElementById('Janus').className = 'diag-section diag-default'; // Prefixed
					  document.querySelector('#Janus .diag-section-header').className = 'diag-section-header diag-default'; // Prefixed
					  document.querySelector('#Janus .diag-section-header span').textContent = 'Janus ‚ñº';
					
					  document.getElementById('Ours').className = 'diag-section diag-orange'; // Prefixed
					  document.querySelector('#Ours .diag-section-header').className = 'diag-section-header diag-orange'; // Prefixed
					  document.querySelector('#Ours .diag-section-header span').textContent = 'Ours ‚ñº';
					
					  Object.values(sections).forEach(section => section.innerHTML = '');
					
					  const filtered = data.filter(d => d.title === title);
					
					  ['Gemini', 'Janus', 'Ours'].forEach(model => {
					    const items = filtered.filter(item => item.model === model);
					    const modelSectionContent = sections[model];
					    if (modelSectionContent) {
					        if (items.length === 0) {
					            modelSectionContent.innerHTML = '<p><i>No data for this example.</i></p>';
					        } else {
					            items.forEach(item => {
					                const el = createDialogueElement(item);
					                modelSectionContent.appendChild(el);
					            });
					        }
					    }
					  });
					}
					
					document.querySelectorAll('.diag-section-header').forEach(header => { // Prefixed
					  header.addEventListener('click', () => {
					    const content = header.nextElementSibling;
					    const currentSpan = header.querySelector('span');
					    if (content.style.display === 'none' || content.style.display === '') {
					      content.style.display = 'block';
					      if(currentSpan) currentSpan.textContent = currentSpan.textContent.replace('‚ñ∂', '‚ñº');
					    } else {
					      content.style.display = 'none';
					      if(currentSpan) currentSpan.textContent = currentSpan.textContent.replace('‚ñº', '‚ñ∂');
					    }
					  });
					});
					
					exampleSelect.addEventListener('change', e => {
					  renderExample(e.target.value);
					});
					
					if (uniqueTitles.length > 0) {
					    exampleSelect.value = uniqueTitles[0];
					    renderExample(uniqueTitles[0]);
					}
					
				</script>
			</div>
		</div>
	</div>
</section>
<!-- @PAN TODO: bibtex -->
<section class="section" id="BibTeX">
	<div class="container is-max-desktop content">
		<h2 class="title is-3 has-text-centered">BibTeX</h2>
		<pre><code style="color:#000000;">@article{chen2025intermt,
	title={InterMT: Multi-Turn Interleaved Preference Alignment with Human Feedback},
	author={Boyuan Chen and Donghai Hong and Jiaming Ji and Jiacheng Zheng and Bowen Dong and Jiayi Zhou and Kaile Wang and Josef Dai and Xuyao Wang and Wenqi Chen and Qirui Zheng and Wenxin Li and Sirui Han and Yike Guo and Yaodong Yang},
	year={2025},
	institution={Peking University and Hong Kong University of Science and Technology},
	url={https://pku-intermt.github.io},
	keywords={Multimodal Learning, Multi-Turn Interaction, Human Feedback, Preference Alignment}
	}</code></pre>
	</div>
</section>
<section>
	<div class="section" id="org-banners" style="display: flex; align-items: center;">
		<a href="https://www.pku.edu.cn/" target="_blank" rel="external" style="margin-right: 20px;">
			<img class="center-block org-banner" src="./assets/PKU_logo.png" style="width: 150px; height: 150px;">
		</a>
		<a href="https://hkust.edu.hk/" target="_blank" class="ext-link" style="margin-left: 20px;">
			<img class="center-block org-banner" src="./assets/HKUST-STD.svg" style="width: 400px; height: 200px;">
		</a>
	</div>
</section>
<footer class="footer">
	<!-- <div class="container"> -->
	<div class="content has-text-centered">
	</div>
	<div class="columns is-centered">
		<div class="column is-8">
			<div class="content">
				<p> This website is website adapted from <a href="https://nerfies.github.io/">Nerfies</a>, licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>. </p>
			</div>
		</div>
	</div>
	<!-- </div> -->
</footer>
</div>
</section>
</body>
</html>
