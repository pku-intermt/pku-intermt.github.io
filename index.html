<!DOCTYPE html>
<html class="fontawesome-i2svg-active fontawesome-i2svg-complete">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<style data-merge-styles="true"></style>
	<style data-merge-styles="true"></style>
	<style data-merge-styles="true"></style>
	<meta name="description" content="Multi-Turn Interleaved Preference Alignment with Human Feedback">
	<meta name="keywords" content="Multi-Turn, Multi-Modality">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Multi-Turn Interleaved Preference Alignment with Human Feedback</title>
	<link rel="icon" href="./assets/logo_transp.png" type="image/png">
	<!-- ÂºïÂÖ•cssÊñá‰ª∂ -->
	<link href="./assets/css" rel="stylesheet">
	<link rel="stylesheet" href="./assets/bulma.min.css">
	<link rel="stylesheet" href="./assets/bulma-carousel.min.css">
	<link rel="stylesheet" href="./assets/font-face.css">
	<link rel="stylesheet" href="./assets/bulma-slider.min.css">
	<link rel="stylesheet" href="./assets/fontawesome.all.min.css">
	<link rel="stylesheet" href="./assets/academicons.min.css">
	<link rel="stylesheet" href="./assets/index.css">
	<link rel="stylesheet" href="./assets/leaderboard.css">
	<style>
		p, ul {
		    text-align: justify;
		    margin-left: auto;
		    margin-right: auto;
		    width: 75%;
			margin-bottom: 10px; /* ËÆæÁΩÆÊÆµËêΩ‰πãÈó¥ÁöÑÈó¥Ë∑ù */
		}
		li {
		    margin-bottom: 10px;
		}
	</style>
	<!-- Ë°®Ê†ºÊ†∑ÂºècssÊñá‰ª∂ -->
	<style>
		/* Ë°®Ê†ºÂü∫Á°ÄÊ†∑Âºè */
		.data-table {
			width: 100%;
			border-collapse: collapse;
			margin: 20px 0;
			background-color: white;
			border: none;
		}
		
		/* Ë°®Ê†ºÂ∏ÉÂ±Ä */
		.data-table th,
		.data-table td {
			padding: 12px 15px;
			text-align: center;
			vertical-align: middle;
			border: none;
		}
		
		/* Ë°®Â§¥Ê†∑Âºè */
		.data-table thead {
			background-color: #2c3e50;
		}
		
		.data-table th {
			font-weight: bold;
			font-size: 14px;
			color: #ffffff;
		}
		
		/* Ë°®Ê†ºË°åÊ†∑Âºè */
		.data-table tbody tr:nth-child(even) {
			background-color: #f9f9f9;
		}
		
		.data-table tbody tr:hover {
			background-color: #f1f1f1;
		}
		
		/* ÂçïÂÖÉÊ†ºÂ≠ó‰ΩìÂ§ßÂ∞è */
		.data-table td {
			font-size: 13px;
		}
		
		/* ÁâπÊÆäÊ†∑ÂºèÁ±ª */
		.bold {
			font-weight: bold;
		}
		
		.highlight {
			background-color: #fffde7;
		}
		
		/* Ë°®Ê†ºÂØπÈΩê */
		.center-table {
			margin-left: auto;
			margin-right: auto;
		}
	</style>
	<!-- ÂØπËØùcssÊñá‰ª∂ -->
	<style>
		/* body {
		  font-family: Arial, sans-serif;
		  margin: 20px;
		  background-color: #f0f0f0;
		} */
		.diag-dropdown-container {
		  margin-bottom: 15px;
		  padding: 10px;
		  background-color: #fff;
		  border-radius: 8px;
		  box-shadow: 0 2px 4px rgba(0,0,0,0.1);
		}
		select {
		  padding: 8px 12px;
		  font-size: 16px;
		  border-radius: 4px;
		  border: 1px solid #ccc;
		}
		.diag-container {
		  display: flex;
		  gap: 15px;
		}
		.diag-section {
		  flex: 1;
		  border: 1px solid #ccc;
		  border-radius: 8px;
		  overflow: hidden;
		  max-height: 85vh;
		  display: flex;
		  flex-direction: column;
		  box-shadow: 0 2px 4px rgba(0,0,0,0.1);
		}
		.diag-section.diag-orange { background: #fff4e5; border-color: orange; }
		.diag-section.diag-default { background: #f9f9f9; border-color: #999; }
		.diag-section.diag-blue { background: #e7f3ff; border-color: #99c2ff; }
		
		
		.diag-section-header {
		  padding: 12px 15px;
		  font-weight: bold;
		  cursor: pointer;
		  user-select: none;
		  background-color: #ddd;
		  border-bottom: 1px solid #bbb;
		  display: flex;
		  align-items: center;
		}
		.diag-section-header svg, .diag-section-header img { /* SVGs/images within header */
		  margin-right: 8px;
		  width: 20px;
		  height: 20px;
		  vertical-align: middle;
		}
		.diag-section-header.diag-orange { background-color: #f7d399; border-color: orange; }
		.diag-section-header.diag-default { background-color: #e0e0e0; border-color: #ccc; }
		.diag-section-header.diag-blue { background-color: #b3d7ff; border-color: #99c2ff;}
		
		
		.diag-content {
		  padding: 15px;
		  overflow-y: auto;
		  flex-grow: 1;
		  background-color: #fff;
		}
		
		.diag-dialogue {
		  margin-bottom: 20px;
		}
		
		.diag-prompt {
		  text-align: right;
		  background: #d0e6ff;
		  padding: 10px 14px;
		  border-radius: 15px 15px 0 15px;
		  margin-bottom: 8px;
		  max-width: 85%;
		  margin-left: auto;
		  font-weight: 600;
		  word-wrap: break-word;
		}
		
		.diag-response {
		  text-align: left;
		  background: #eaeaea;
		  padding: 10px 14px;
		  border-radius: 15px 15px 15px 0;
		  max-width: 85%;
		  position: relative;
		  white-space: normal;
		  font-size: 14px;
		  word-wrap: break-word;
		}
		.diag-response-text-area {
		   white-space: pre-wrap;
		}
		
		
		.diag-prompt img, .diag-response img {
		  max-width: 100%;
		  margin-top: 10px;
		  border-radius: 6px;
		  display: block;
		}
		
		ol {
		  padding-left: 25px;
		  margin-top: 5px;
		  margin-bottom: 5px;
		}
		ol li {
		  margin-bottom: 5px;
		}
		p {
		  margin-top: 5px;
		  margin-bottom: 10px;
		}
		strong {
		  font-weight: bold;
		}
		
		.diag-typing-caret::after {
		  content: '|';
		  animation: blink 1s step-end infinite;
		  margin-left: 2px;
		  font-weight: bold;
		}
		@keyframes blink {
		  50% { opacity: 0; }
		}
	</style>
	<style>
		.modal {
		    display: none;
		    position: fixed;
		    z-index: 1;
		    left: 0;
		    top: 0;
		    width: 100%;
		    height: 100%;
		    overflow: auto;
		    background-color: rgb(0,0,0);
		    background-color: rgba(0,0,0,0.9);
		}
		
		.modal-content {
		    margin: 15% auto;
		    padding: 20px;
		    width: 80%;
		    max-width: 700px;
		}
		
		.close {
		    color: #f1f1f1;
		    float: right;
		    font-size: 28px;
		    font-weight: bold;
		}
		
		.close:hover,
		.close:focus {
		    color: #000;
		    text-decoration: none;
		    cursor: pointer;
		}
	</style>
	<style>
		p, li, strong {
		text-align: left;
			}
	</style>
	<!-- ÂºïÂÖ•jsÊñá‰ª∂ -->
	<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
	<script type="text/javascript" src="./assets/sort-table.js" defer=""></script>
	<script src="./assets/jquery.min.js"></script>
	<script defer="" src="./assets/fontawesome.all.min.js"></script>
	<script src="./assets/bulma-carousel.min.js"></script>
	<script src="./assets/bulma-slider.min.js"></script>
	<script src="./assets/explorer-index.js"></script>
	<script src="./assets/question_card.js"></script>
	<script src="./assets/leaderboard_testmini.js"></script>
	<script src="./assets/output_folders.js" defer=""></script>
	<script src="./assets/model_scores.js" defer=""></script>
	<script src="./assets/data_public.js" defer=""></script>
</head>
<nav2 class="navbar" role="navigation" aria-label="main navigation">
	<div class="navbar-brand">
		<a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
			<span aria-hidden="true"></span>
			<span aria-hidden="true"></span>
			<span aria-hidden="true"></span>
		</a>
	</div>
	<div class="navbar-menu">
		<div class="navbar-start" style="flex-grow: 1; justify-content: center;">
			<!-- @PAN TODO: consider adding links? -->
			<div class="navbar-item has-dropdown is-hoverable">
				<a class="navbar-link"> More Research </a>
				<div class="navbar-dropdown">
					<a class="navbar-item" href="https://github.com/PKU-Alignment/safe-rlhf">
						<b>PKU-SafeRLHF (ACL2025 Main)</b>
					</a>
					<a class="navbar-item" href="https://github.com/PKU-Alignment/aligner">
						<b>Aligner (NeurIPS 2024 Oral)</b>
					</a>
					<a class="navbar-item" href="https://github.com/PKU-Alignment/safe-sora">
						<b>SafeSora (NeurIPS 2024 DB Track)</b>
					</a>
					<a class="navbar-item" href="https://github.com/PKU-Alignment/beavertails">
						<b>BeaverTails (NeurIPS 2024 DB Track)</b>
					</a>
					<a class="navbar-item" href="https://align-anything.readthedocs.io/en/latest/index.html">
						<b>Align-Anythingüî•üî•üî•</b>
					</a>
				</div>
			</div>
		</div>
	</div>
</nav2>
<section class="hero" style="background-color: #1f5ceb;">
	<div class="hero-body">
		<div class="container is-max-desktop">
			<div class="columns is-centered">
				<div class="column has-text-centered">
					<h1 class="title is-1 publication-title" style="color: white;">Multi-Turn Interleaved Preference Alignment with Human Feedback</h1>
					<div class="is-size-5 publication-authors">
						<div class="is-size-5 publication-authors">
							<span class="author-block" style="color: white;"> Boyuan Chen<sup>1*</sup>Ôºå </span>
							<span class="author-block" style="color: white;"> Donghai Hong<sup>1*</sup>Ôºå </span>
							<span class="author-block" style="color: white;"> Jiaming Ji<sup>1*</sup>Ôºå </span>
							<span class="author-block" style="color: white;"> Jiacheng Zheng<sup>2</sup>Ôºå </span>
							<span class="author-block" style="color: white;"> Bowen Dong<sup>1</sup>Ôºå </span>
							<span class="author-block" style="color: white;"> Jiayi Zhou<sup>1</sup>Ôºå </span>
							<span class="author-block" style="color: white;"> Kaile Wang<sup>1</sup>Ôºå </span>
							<span class="author-block" style="color: white;"> Josef Dai<sup>1</sup>Ôºå </span>
							<span class="author-block" style="color: white;"> Xuyao Wang<sup>1</sup>Ôºå </span>
							<span class="author-block" style="color: white;"> Wenqi Chen<sup>1</sup>Ôºå </span>
							<span class="author-block" style="color: white;"> Qirui Zheng<sup>1</sup>Ôºå </span>
							<span class="author-block" style="color: white;"> Wenxin Li<sup>1</sup>Ôºå </span>
							<span class="author-block" style="color: white;"> Sirui Han<sup>2</sup>Ôºå </span>
							<span class="author-block" style="color: white;"> Yike Guo<sup>2</sup>Ôºå </span>
							<span class="author-block" style="color: white;"> Yaodong Yang<sup>1‚Ä†</sup>Ôºå </span>
						</div>
						<div class="is-size-5 publication-authors">
							<span class="author-block" style="color: white;">
								<sup>1</sup>
								<em>Peking University</em>
							</span>
							<span class="author-block" style="color: white;">
								<sup>2</sup>
								<em>The Hong Kong University of Science and Technology</em>
							</span>
						</div>
						<p style="text-align: center; color: white;">
							<small> * Equal contribution, <sup>‚Ä†</sup> corresponding author. </small>
						</p>
						<br>
					</div>
					<div class="column has-text-centered">
						<div class="publication-links">
							<!-- PDF Link. -->
							<span class="link-block">
								<a href="#" class="external-link button is-normal is-rounded is-dark">
									<span class="icon">
										<svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg="">
											<path fill="currentColor"
												d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z">
											</path>
										</svg><!-- <i class="fas fa-file-pdf"></i> Font Awesome fontawesome.com -->
									</span>
									<span>Paper</span>
								</a>
							</span>
							<span class="link-block">
								<a href="https://huggingface.co/datasets/PKU-Alignment/INTERMT" class="external-link button is-normal is-rounded is-dark">
									<span class="icon">
										<img src="./assets/huggingface-color.svg" alt="HuggingFace" style="width: 1em; height: 1em;" />
									</span>
									<span>HuggingFace</span>
								</a>
							</span>
							<!-- Video Link. -->
							<span class="link-block">
								<a href="https://github.com/cby-pku/INTERMT" class="external-link button is-normal is-rounded is-dark">
									<span class="icon">
										<svg class="svg-inline--fa fa-github fa-w-16" aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" data-fa-i2svg="">
											<path fill="currentColor"
												d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z">
											</path>
										</svg><!-- <i class="fab fa-github"></i> Font Awesome fontawesome.com -->
									</span>
									<span>GitHub</span>
								</a>
							</span>
							<!-- Code Link. -->
							<span class="link-block">
								<a href="https://huggingface.co/PKU-Alignment/INTERMT-Judge" class="external-link button is-normal is-rounded is-dark">
									<span class="icon">
										<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512">
											<!--!Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2025 Fonticons, Inc.-->
											<path fill="#ffffff"
												d="M0 96C0 60.7 28.7 32 64 32l320 0c35.3 0 64 28.7 64 64l0 320c0 35.3-28.7 64-64 64L64 480c-35.3 0-64-28.7-64-64L0 96zm144 4c-24.3 0-44 19.7-44 44l0 48c0 24.3 19.7 44 44 44l32 0c24.3 0 44-19.7 44-44l0-48c0-24.3-19.7-44-44-44l-32 0zm-4 44c0-2.2 1.8-4 4-4l32 0c2.2 0 4 1.8 4 4l0 48c0 2.2-1.8 4-4 4l-32 0c-2.2 0-4-1.8-4-4l0-48zm140-44c-11 0-20 9-20 20c0 9.7 6.9 17.7 16 19.6l0 76.4c0 11 9 20 20 20s20-9 20-20l0-96c0-11-9-20-20-20l-16 0zM132 296c0 9.7 6.9 17.7 16 19.6l0 76.4c0 11 9 20 20 20s20-9 20-20l0-96c0-11-9-20-20-20l-16 0c-11 0-20 9-20 20zm96 24l0 48c0 24.3 19.7 44 44 44l32 0c24.3 0 44-19.7 44-44l0-48c0-24.3-19.7-44-44-44l-32 0c-24.3 0-44 19.7-44 44zm44-4l32 0c2.2 0 4 1.8 4 4l0 48c0 2.2-1.8 4-4 4l-32 0c-2.2 0-4-1.8-4-4l0-48c0-2.2 1.8-4 4-4z" />
										</svg>
									</span>
									<span>Model</span>
								</a>
							</span>
						</div>
					</div>
				</div>
			</div>
		</div>
	</div>
</section>
<br>
<section class="hero teaser">
	<div class="container is-max-desktop">
		<div class="content has-text-centered">
			<img src="./assets/figure1-0516_00.jpg" alt="geometric reasoning" width="100%" style="box-shadow: 0 4px 10px rgba(0, 0, 0, 0.3); border-radius: 8px;">
			<h2 style="text-align: center;">Abstract</h2>
			<p> As multimodal large models (MLLMs) continue to advance across challenging tasks, a key question emerges: <strong><em>What essential capabilities are still missing?</em></strong> A critical aspect of human learning is continuous interaction with the environment -- not limited to
				language, but also involving multimodal understanding and generation. To move closer to human-level intelligence, models must similarly support <strong>multi-turn</strong>, <strong>multimodal interaction</strong>. In particular, they should comprehend interleaved multimodal contexts
				and respond coherently in ongoing exchanges. In this work, we present <strong>an initial exploration</strong> through the <span class="icon">
					<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
				</span><strong>InterMT</strong> -- <strong>the first preference dataset for <em>multi-turn</em> multimodal interaction</strong>, grounded in real human feedback. In this exploration, we particularly emphasize the importance of human oversight, introducing expert annotations to guide
				the process, motivated by the fact that current MLLMs lack such complex interactive capabilities. <span class="icon">
					<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
				</span><strong>InterMT</strong> captures human preferences at both global and local levels into nine sub-dimensions, consists of 15.6k prompts, 52.6k multi-turn dialogue instances, and 32.4k human-labeled preference pairs. To compensate for the lack of capability for multi-modal
				understanding and generation, we introduce an agentic workflow that leverages tool-augmented MLLMs to construct multi-turn QA instances. To further this goal, we introduce <span class="icon">
					<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
				</span><strong>InterMT-Bench</strong> to assess the ability of MLLMs in assisting judges with multi-turn, multimodal tasks. We demonstrate the utility of <span class="icon">
					<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
				</span><strong>InterMT</strong> through applications such as judge moderation and further reveal the <em>multi-turn scaling law</em> of judge model. We hope the open-source of our data can help facilitate further research on aligning current MLLMs to the next step.</p>
		</div>
	</div>
</section>
<br>
<br>
<section class="hero is-light is-small">
	<div class="hero-body has-text-centered">
		<!-- <h1 class="title is-1 mathvista"><img src="static/images/mathvista.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>MathVista Dataset</h1> -->
		<h1 class="title is-1 mathvista">
			<!--<img src="static/images/mathvista.png" style="width:1em;vertical-align: middle" alt="Logo"/>-->
			<span class="mathvista" style="vertical-align: middle" id="introduction">Introduction</span>
		</h1>
	</div>
</section>
<section class="section">
	<div class="container" style="margin-bottom: 2vh;">
		<div class="columns is-centered has-text-centered">
			<div class="column is-four-fifths">
				<p>Humans perceive the world through dynamic, multimodal interactions involving text, images, audio, video, and more. Building on the success of multimodal large language models (MLLMs), recent efforts aim to develop general-purpose AI assistants that handle multiple mixed
					modalities. A key feature of such general-purpose assistants is to engage in natural multi-turn conversations, perceive and generate any modality, to enable more smooth interaction and grounded understanding.</p>
				<p>Recent years have seen community efforts in transplanting alignment techniques, such as Reinforcement Learning from Human Feedback (RLHF), from the text modality to multiple modalities settings. Within this line of research, most studies focus exclusively on either understanding
					or generation. The lack of alignment considerations for multimodal mixed input-output settings exacerbates the imbalance across modalities, i.e., modality disequilibrium. Furthermore, existing methods primarily focus on single-turn interactions, where an LLM generates a response
					from a prompt and receives immediate alignment feedback. However, real-world interactions typically occur in long-horizon conversations and often feature interleaved multimodal inputs and outputs.</p>
				<blockquote style="border:1px solid #ccc; padding:10px; background:#f9f9f9; margin:20px 0; text-align:center; font-style:italic; font-size:regular;"> How to improve <strong style="color:#8B0000;">multi-turn</strong> interleaved <strong
						style="color:#00008B;">understanding</strong>-<strong style="color:#006400;">generation</strong> alignment via human feedback? </blockquote>
				<p><strong><u style="color:#C6011F">Our reflections highlight several key issues in the alignment of MLLMs:<svg class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="currentColor" viewBox="0 0 24 24">
								<path
									d="M13.849 4.22c-.684-1.626-3.014-1.626-3.698 0L8.397 8.387l-4.552.361c-1.775.14-2.495 2.331-1.142 3.477l3.468 2.937-1.06 4.392c-.413 1.713 1.472 3.067 2.992 2.149L12 19.35l3.897 2.354c1.52.918 3.405-.436 2.992-2.15l-1.06-4.39 3.468-2.938c1.353-1.146.633-3.336-1.142-3.477l-4.552-.36-1.754-4.17Z" />
							</svg>
						</u></strong></p>
				<ul>
					<li><strong>Modality Fusion via Harmonizing Understanding and Generation.</strong> To build general-purpose AI assistants, high-fidelity perception and understanding alone are not sufficient. The system should also support the selective generation of multimodal outputs to
						effectively communicate, instruct, or interact with users in a natural and contextually appropriate manner.</li>
					<li><strong>Modeling Long‚ÄêHorizon, Interleaved Multimodal Interactions.</strong> Real‚Äêworld user‚ÄìAI exchanges typically span many turns and interleave text, vision, and other modalities. Such interactions demand not only precise instruction following but also sustained attention
						and reasoning over an evolving context, approaching near‚Äêhuman in‚Äêcontext reasoning capabilities.</li>
					<li><strong>Dynamic Human‚Äêin‚Äêthe‚ÄêLoop Alignment.</strong> In extended, multimodal interactions, user preferences continually evolve. Capturing and aligning with these emergent, dynamic preferences calls for genuine, iterative human feedback throughout the interaction.</li>
				</ul>
				<p>In response, we introduce <span class="icon">
						<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
					</span>InterMT, a human preference dataset designed to capture the complexity and diversity of human intent in multi-turn settings. Specifically, <span class="icon">
						<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
					</span>InterMT targets vision-language interaction scenarios involving interleaved understanding and generation. To model dynamic human preferences, <span class="icon">
						<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
					</span>InterMT comprises <strong>
						<font color="red">15604</font>
					</strong> seed questions that elicit multi-turn, multimodal conversations spanning <strong>
						<font color="red">15+</font>
					</strong> domains. Helpfulness is then decomposed into <strong>
						<font color="red">9</font>
					</strong> sub-dimensions, capturing both global (conversation-level) and local (turn-level) aspects of human feedback.</p>
				<p><strong><u style="color:#C6011F">Our key contributions are summarized as follows:<svg class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="currentColor" viewBox="0 0 24 24">
								<path
									d="M13.09 3.294c1.924.95 3.422 1.69 5.472.692a1 1 0 0 1 1.438.9v9.54a1 1 0 0 1-.562.9c-2.981 1.45-5.382.24-7.25-.701a38.739 38.739 0 0 0-.622-.31c-1.033-.497-1.887-.812-2.756-.77-.76.036-1.672.357-2.81 1.396V21a1 1 0 1 1-2 0V4.971a1 1 0 0 1 .297-.71c1.522-1.506 2.967-2.185 4.417-2.255 1.407-.068 2.653.453 3.72.967.225.108.443.216.655.32Z" />
							</svg>
						</u></strong></p>
				<ul>
					<li><strong>The First Multi-turn Interleaved Preference Dataset:</strong> To the best of our knowledge, <span class="icon">
							<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
						</span>InterMT is the first dataset that captures real human preferences for tasks involving multi-turn and interleaved multimodal understanding and generation. It contains <strong>
							<font color="red">15604</font>
						</strong> unique seed questions across diverse categories, <strong>
							<font color="red">52.6k</font>
						</strong> multi-turn interleaved vision-language QA instances, and <strong>
							<font color="red">32459</font>
						</strong> sets of multi-dimensional human preference annotations.</li>
					<li><strong>Agent-based Construction Workflow:</strong> <span class="icon">
							<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
						</span>InterMT employs a carefully designed agent-based multi-turn QA construction workflow that leverages strong MLLMs augmented with external tools to simulate high-quality real multi-turn interactions.</li>
					<li><strong>Decoupled Helpfulness in Multi-turn Multimodal Scenarios:</strong> <span class="icon">
							<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
						</span>InterMT decomposes the concept of helpfulness for multi-turn, multimodal interactions into two distinct levels: local (turn-level) and global (conversation-level). Furthermore, <span class="icon">
							<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
						</span>InterMT breaks down helpfulness into <strong>
							<font color="red">9</font>
						</strong> specific dimensions, allowing for a detailed and nuanced evaluation of multi-turn, multi-modal interactions.</li>
					<li><strong>Effective for Multi-turn Alignment:</strong> Building on <span class="icon">
							<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
						</span>InterMT, we investigate methods to model long-horizon values and align dynamic human values. Our findings reveal the phenomenon of preference transfer in multi-turn multimodal interactions, which facilitates preference modeling for predicting human judgments.
						Additionally, we identify a scaling phenomenon in multi-turn multimodal judge moderation.</li>
					<li><strong>One More Thing</strong> We introduce InterMT-Bench to evaluate the ability of MLLMs in assisting judges across multi-turn, multimodal tasks, encompassing three parts: Scoring Evaluation, Pair Comparison, and Crucial Step Recognition. Despite strong reasoning
						capabilities, advanced MLLMs fail to align with human values in judgment tasks. However, they show potential in identifying crucial steps in long-context scenarios.</li>
				</ul>
			</div>
		</div>
	</div>
</section>
<!-- DATASET SECTION -->
<section class="hero is-light is-small">
	<div class="hero-body has-text-centered">
		<!-- <h1 class="title is-1 mathvista"><img src="static/images/mathvista.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>MathVista Dataset</h1> -->
		<h1 class="title is-1 mathvista">
			<!--<img src="static/images/mathvista.png" style="width:1em;vertical-align: middle" alt="Logo"/>-->
			<span class="mathvista" style="vertical-align: middle" id="Dataset"><span class="icon">
					<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
				</span>InterMT Dataset</span>
		</h1>
	</div>
</section>
<section class="section">
	<div class="container">
		<center>
			<h2 class="title is-3">Data Construction</h2>
		</center>
		<div class="columns is-centered">
			<div class="column is-half">
				<div class="content has-text-justified">
					<section id="dataset">
						<h4>Dataset</h4>
						<p>Our core contribution is the introduction of a human preference dataset designed for <span style="color:#8B0000;">multi-turn</span>, multimodal <span style="color:#00008B;">understanding</span> and <span style="color:#006400;">generation</span> tasks. This section outlines
							the dataset's composition, the collection of prompts and multi-turn QA instances, and the human annotation process.</p>
						<section id="composition">
							<h4>Dataset Composition</h4>
							<p>The dataset comprises two primary data types: (1) <em>Seed Questions</em> for initiating <span style="color:#8B0000;">multi-turn</span> conversations, and (2) human preference annotations at both the local and global levels. Prompts are refined using principles from
								linguistics and cognitive psychology to simulate realistic multimodal <span style="color:#8B0000;">multi-turn</span> tasks effectively.</p>
						</section>
						<section id="qa-construction">
							<h4>Multi-turn QA Construction</h4>
							<p>A tool-augmented workflow is employed to simulate human-like dialogues across various <span style="color:#8B0000;">multi-turn</span> tasks, integrating large models (e.g., GPT-4o, Qwen2.5-VL) with multimodal tools for image generation, editing, and retrieval.</p>
						</section>
						<section id="annotation">
							<h4>Preference Annotation</h4>
							<p>Human annotation includes local and global evaluations of conversation quality, focusing on <span style="color:#00008B;">understanding</span>-<span style="color:#006400;">generation</span> alignment, coherence, and completeness.</p>
						</section>
					</section>
				</div>
			</div>
			<div class="column is-half">
				<div class="carousel results-carousel">
					<div class="carousel-item">
						<div class="box m-5">
							<div class="content has-text-centered">
								<img src="./assets/Figure2.png" style="width: 100%;" onclick="openFullscreenImage(this.src)">
								<p style="margin-bottom: 30px;">
									<span class="icon">
										<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
									</span>
									<b>InterMT</b> includes over 15 tasks in vision-language scenarios, capturing communication examples across diverse multi-turn settings.
								</p>
							</div>
						</div>
					</div>
					<div class="carousel-item">
						<div class="box m-5">
							<div class="content has-text-centered">
								<img src="./assets/Figure3-0515_00.png" style="width: 100%;" onclick="openFullscreenImage(this.src)">
								<p style="margin-bottom: 30px;">Overview of the four-stage pipeline for constructing <span class="icon">
										<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
									</span>
									<b>InterMT</b>. The pipeline covers data harvesting, model-generated expansion, human annotation, and final QA tree assembly.
								</p>
							</div>
						</div>
					</div>
				</div>
			</div>
			<!-- Fullscreen Image Modal -->
			<div id="fullscreen-image-modal" class="modal" style="display: none;">
				<div class="modal-content" style="position: relative;">
					<span class="close" onclick="closeFullscreenImage()" style="position: absolute; top: 10px; right: 10px; cursor: pointer; font-size: 24px;">&times;</span>
					<img id="fullscreen-image" src="" style="width: 100%; height: auto;">
				</div>
			</div>
			<script>
				function openFullscreenImage(imageSrc) {
				    const modal = document.getElementById('fullscreen-image-modal');
				    const fullscreenImage = document.getElementById('fullscreen-image');
				    fullscreenImage.src = imageSrc;
				    modal.style.display = 'block';
				}
				
				function closeFullscreenImage() {
				    const modal = document.getElementById('fullscreen-image-modal');
				    modal.style.display = 'none';
				}
				
				// Close the modal when clicking outside the modal content
				window.onclick = function(event) {
				    const modal = document.getElementById('fullscreen-image-modal');
				    if (event.target === modal) {
				        modal.style.display = 'none';
				    }
				}
			</script>
</section>
<section class="hero is-light is-small">
	<div class="hero-body has-text-centered">
		<!-- <h1 class="title is-1 mathvista"><img src="static/images/mathvista.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>MathVista Dataset</h1> -->
		<h1 class="title is-1 mathvista">
			<!--<img src="static/images/mathvista.png" style="width:1em;vertical-align: middle" alt="Logo"/>-->
			<span class="mathvista" style="vertical-align: middle" id="Experiment Analysis">Experiment Analysis</span>
		</h1>
	</div>
</section>
<section class="section">
	<div class="container">
		<div class="columns is-centered has-text-centered">
			<div class="column is-four-fifths" style="width: 100%;">
				<h2 class="title is-3">Main Analysis</h2>
				<div class="content has-text-justified">
					<div id="results-carousel" class="carousel results-carousel">
						<div class="content has-text-centered">
							<img src="./assets/Figure5-1.png" style="width: 65%;">
							<p style="margin-left: auto; margin-right: auto; width: 75%;"> Since the dataset captures <em>real</em> human preferences across multiple dimensions at both <em>global</em> and <em>local</em> levels, it is meaningful to analyze the correlations among these dimensions,
								examine the relationship between per-turn preferences and overall evaluation, and further compare human feedback with AI feedback. <br>
								<br>
								<strong>Correlation Analysis</strong>
								<br> Three key findings from the analysis: <br> 1. <strong>Modality perception precedes effective modality fusion:</strong> The evaluation of image-text consistency is strongly correlated with visual perceptual quality, indicating that clear perception of individual
								modalities is a prerequisite for reliable multimodal judgment. <br> 2. <strong>Long-horizon evaluations hinge on coherence and temporal consistency:</strong> Metrics such as helpfulness and completeness strongly align with context awareness and global visual
								consistency, underscoring the importance of maintaining coherent semantics across extended interactions. <br> 3. <strong>Intent grounding drives long-horizon crucial step recognition:</strong> In multi-turn scenarios, models may produce plausible outputs but deviate
								from core user intentions, leading to stylistic drift and omission of key steps. <br>
								<br>
								<strong>Human Feedback vs. AI Feedback</strong>
								<br> Our analysis reveals that while AI annotators achieve reasonable agreement with human annotators at the local level (60%), their agreement drops significantly at the global level, particularly in long-horizon, multi-turn tasks. This indicates a current gap in
								AI's ability to align with human judgments in extended interactions.
							</p>
						</div>
						<div class="content has-text-centered">
							<img src="./assets/Figure4-0516_00.jpg" style="width: 75%;">
							<p>This figure illustrates the relationship between global and local preference annotation dimensions. We identify three key findings: <br><strong>(1) Modality perception precedes effective modality fusion</strong>: for both the local-local and local-global correlation,
								the evaluation of image-text consistency is strongly correlated with visual perceptual quality (up to 0.81). This suggests that before assessing multimodal information, human evaluators tend to prioritize a clear understanding of each individual modality, indicating
								that a clear perception of individual modalities is a prerequisite for reliable multimodal judgment. <br><strong>(2) Long-horizon evaluations hinge on coherence and temporal consistency</strong>: for the global-global correlation, metrics such as helpfulness and
								completeness strongly align with context awareness and global visual consistency, underscoring the importance of maintaining coherent semantics, multimodal information, and consistency with prior conversational context over extended interactions. <br><strong>(3)
									Intent grounding drives long-horizon crucial step recognition</strong>: in multi-turn scenarios, models may deviate from the user‚Äôs core intentions, producing self-directed responses. Despite locally high-scoring and plausible outputs, this leads to stylistic
								drift and omission of key steps over extended interactions, as demonstrated in the local-global setting.</p>
						</div>
						<div class="content has-text-centered">
							<img src="./assets/Figure5.png" style="width: 60%;">
							<p style="margin-left: auto; margin-right: auto; width: 75%;"> This figure presents an analysis of the agreement among human annotators, AI annotators, and human verifiers. Our observations reveal that while AI annotators achieved an average score agreement of
								approximately 60% on <em>local</em> evaluation tasks, their agreement with humans is considerably lower for <em>global</em>, namely, longer-horizon, evaluation tasks. This finding partly indicates that current advanced models struggle to reach consensus with human
								judgments when performing score evaluations in multi-turn, multi-modal conversation tasks. We further conduct a broader investigation of the ability of MLLMs to assist human judges in three types of tasks. Consequently, until further validation of AI feedback's
								efficacy, we maintain a conservative position, considering the replacement of human annotation to be presently challenging. </p>
						</div>
						<div class="content has-text-centered">
							<img src="./assets/Figure6_00.jpg" style="width: 65%;">
							<p style="margin-left: auto; margin-right: auto; width: 75%;">Our findings, presented in this figure, suggest that modeling fine-grained <em>local (turn-level)</em> preferences is more effective in capturing human values and achieving better alignment. In contrast,
								directly modeling <em>global (conversation-level)</em> preferences often fails to reflect these nuanced preferences, especially in complex, long-horizon scenarios. We examine the bidirectional transfer between turn-level (<em>local</em>) and conversation-level
								(<em>global</em>) human preferences. Both <em>local-to-global</em> and <em>global-to-local</em> transfers are effective, since multi-turn questions typically hinge on the seed question‚Äôs intent. However, <em>global-to-local</em> transfer is consistently easier and
								better aligned with actual preferences. We attribute this to the greater stability of global preferences‚Äîreflecting users‚Äô overarching tendencies‚Äîwhereas local preferences are short-term and more context-dependent, making <em>local-to-global</em> transfer more
								challenging.</p>
						</div>
					</div>
				</div>
			</div>
		</div>
	</div>
	<section class="section">
		<div class="container">
			<div class="columns is-centered has-text-centered">
				<div class="column is-four-fifths" style="width: 100%;">
					<h2 class="title is-3">More Results</h2>
					<div class="content has-text-justified">
						<div id="results-carousel" class="carousel results-carousel">
							<div class="content has-text-centered">
								<img src="./assets/Figure7_00.jpg" style="width: 70%;">
								<p style="margin-left: auto; margin-right: auto; width: 75%;"> As shown in the Figure, for evaluation turn \( k \), as the number of preceding turns increases from 1 to \( k-1 \), the model's accuracy continues to improve, with average future performance rising,
									indicating that training on multi-turn data with a limited number of turns can generalize to longer horizons.</p>
								<p style="margin-left: auto; margin-right: auto; width: 75%;"> Regarding the number of turns in the training data, the generalization effect shows a diminishing trend. As demonstrated in the Figure, training with \( k \) turns does improve performance for \( k+1
									\rightarrow T \) turns, but this effect diminishes as the number of turns increases. This decline can be attributed to three factors:</p>
								<ul style="margin-left: auto; margin-right: auto; width: 100%;">
									<li style="margin-left: auto; margin-right: auto; width: 75%;"> <em>Diminishing returns:</em> As the number of turns increases, the model's ability to capture long-term preferences weakens, leading to a performance plateau beyond a certain threshold.</li>
									<li style="margin-left: auto; margin-right: auto; width: 75%;"> <em>Contextual drift:</em> As the conversation progresses, earlier turns lose relevance, causing the model to lose context and negatively affecting later predictions.</li>
									<li style="margin-left: auto; margin-right: auto; width: 75%;"> <em>Intention-preference interaction:</em> The evolving relationship between user intentions (e.g., task-oriented goals) and latent preferences (e.g., personal preferences) may complicate the model's
										ability to distinguish between the two, with the conversational structure influencing predictive accuracy.</li>
								</ul>
							</div>
							<div class="content has-text-centered">
								<img src="./assets/Figure8-v1_00.jpg" style="width: 75%;">
								<p style="margin-left: auto; margin-right: auto; width: 75%;"> The dataset includes multi-turn multimodal interleaved communication histories and human-annotated ground truth. Evaluated models must assess the conversation at both the turn and conversation levels
									across nine dimensions, following a set of guidelines.</p>
								<p style="margin-left: auto; margin-right: auto; width: 75%;"><em>Scoring Evaluation</em> requires the model to assign scores on a 0-3 scale, with evaluation based on agreement and Pearson similarity.</p>
								<p style="margin-left: auto; margin-right: auto; width: 75%;"><em>Pair Comparison</em> directly compares two individual turns or entire conversations, without considering ties, and is evaluated for accuracy against human judgments.</p>
								<p style="margin-left: auto; margin-right: auto; width: 75%;"><em>Crucial Step Recognition</em> addresses a key challenge in multi-turn conversations: accurately identifying the user‚Äôs intent and determining whether it has been fulfilled, evaluated by the score
									provided by the judge according to the human-annotated reference answers.</p>
							</div>
							<div class="content has-text-centered">
								<img src="./assets/model_scores_00.jpg" style="width: 65%;">
								<p style="margin-left: auto; margin-right: auto; width: 75%;">We evaluated 6 advanced MLLMs for their ability to assist in judgment for multi-turn multimodal interactions, considering the nine dimensions proposed above. The results reveal key observations:
									<strong>Existing models still face challenges in aligning with long-horizon human values, but they perform more accurately in evaluating local, fine-grained preferences</strong>. As shown in Table below(<span class="icon">
										<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
									</span><strong>InterMT Performance Evaluation</strong>), all models exhibit significant gaps in performance compared to humans in both Score Evaluation and Pair Comparison tasks. However, the models demonstrate better accuracy when assessing local dimensions
									rather than global dimensions, suggesting that capturing fine-grained (<em>e.g.</em>, turn-level) human preferences is crucial for both evaluation and alignment with human dynamic and long-horizon values. However, there is cause for optimism: current MLLMs exhibit
									near-human-level performance (4.38/5) in recognizing task completion and aligning with human intent (<em>i.e.</em>, <em>Crucial Step Recognition</em>), providing potential solutions for long-term value alignment.</p>
							</div>
						</div>
					</div>
				</div>
			</div>
		</div>
		<section class="section">
			<div class="container">
				<div class="columns is-centered">
					<div class="column is-full has-text-centered content">
						<h2 class="title is-3" id="Evaluation">
							<span class="icon">
								<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
							</span>
							<b>InterMT</b> Performance Evaluation
						</h2>
						<div class="content">
							<table class="data-table center-table">
								<thead>
									<tr>
										<th rowspan="2" style="color: #ffffff;">Settings</th>
										<th rowspan="2" style="color: #ffffff;">MLLMs</th>
										<th colspan="5" style="color: #ffffff;">Local Setting</th>
										<th colspan="6" style="color: #ffffff;">Global Setting</th>
									</tr>
									<tr>
										<th style="color: #ffffff;">L1</th>
										<th style="color: #ffffff;">L2</th>
										<th style="color: #ffffff;">L3</th>
										<th style="color: #ffffff;">L4</th>
										<th style="color: #ffffff;">Avg.</th>
										<th style="color: #ffffff;">G1</th>
										<th style="color: #ffffff;">G2</th>
										<th style="color: #ffffff;">G3</th>
										<th style="color: #ffffff;">G4</th>
										<th style="color: #ffffff;">G5</th>
										<th style="color: #ffffff;">Avg.</th>
									</tr>
								</thead>
								<tbody>
									<tr>
										<td rowspan="12" class="bold">Scoring Evaluation</td>
										<td>Gemini-Flash* ‚Ä†</td>
										<td>0.346</td>
										<td>0.107</td>
										<td>0.119</td>
										<td>0.173</td>
										<td>0.186</td>
										<td>0.163</td>
										<td>0.042</td>
										<td>0.051</td>
										<td>
											<strong style="color:#C6011F">0.246</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
												<path fill-rule="evenodd"
													d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
													clip-rule="evenodd" />
											</svg>
										</td>
										<td>0.005</td>
										<td>0.101</td>
									</tr>
									<tr>
										<td>Gemini-Flash* (+reason)</td>
										<td>
											<strong style="color:#C6011F">0.361</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
												<path fill-rule="evenodd"
													d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
													clip-rule="evenodd" />
											</svg>
										</td>
										<td>0.072</td>
										<td>0.122</td>
										<td>0.168</td>
										<td>0.181</td>
										<td>-0.038</td>
										<td>0.083</td>
										<td>0.139</td>
										<td>0.199</td>
										<td>0.048</td>
										<td>0.086</td>
									</tr>
									<tr>
										<td>GPT-4.1</td>
										<td>0.264</td>
										<td>0.095</td>
										<td>0.242</td>
										<td>0.269</td>
										<td>0.218</td>
										<td>0.215</td>
										<td>0.216</td>
										<td>0.084</td>
										<td>0.044</td>
										<td>0.049</td>
										<td>0.122</td>
									</tr>
									<tr>
										<td>GPT-4.1 (+reason)</td>
										<td>0.281</td>
										<td>0.094</td>
										<td>0.272</td>
										<td>0.271</td>
										<td>0.229</td>
										<td>0.215</td>
										<td>0.255</td>
										<td>0.217</td>
										<td>0.216</td>
										<td>0.050</td>
										<td>
											<strong style="color:#C6011F">0.191</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
												<path fill-rule="evenodd"
													d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
													clip-rule="evenodd" />
											</svg>
										</td>
									</tr>
									<tr>
										<td>GPT-4o</td>
										<td>0.291</td>
										<td>
											<strong style="color:#C6011F">0.131</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
												<path fill-rule="evenodd"
													d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
													clip-rule="evenodd" />
											</svg>
										</td>
										<td>0.277</td>
										<td>0.268</td>
										<td>
											<strong style="color:#C6011F">0.242</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
												<path fill-rule="evenodd"
													d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
													clip-rule="evenodd" />
											</svg>
										</td>
										<td>0.254</td>
										<td>0.167</td>
										<td>0.137</td>
										<td>0.139</td>
										<td>0.069</td>
										<td>0.153</td>
									</tr>
									<tr>
										<td>GPT-4o (+reason)</td>
										<td>0.290</td>
										<td>0.091</td>
										<td>0.252</td>
										<td>
											<strong style="color:#C6011F">0.280</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
												<path fill-rule="evenodd"
													d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
													clip-rule="evenodd" />
											</svg>
										</td>
										<td>0.228</td>
										<td>0.183</td>
										<td>0.243</td>
										<td>0.194</td>
										<td>0.086</td>
										<td>0.072</td>
										<td>0.156</td>
									</tr>
									<tr>
										<td>Gemini-Pro*</td>
										<td>0.273</td>
										<td>0.079</td>
										<td>0.258</td>
										<td>0.168</td>
										<td>0.194</td>
										<td>
											<strong style="color:#C6011F">0.285</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
												<path fill-rule="evenodd"
													d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
													clip-rule="evenodd" />
											</svg>
										</td>
										<td>0.240</td>
										<td>-0.024</td>
										<td>0.235</td>
										<td>
											<strong style="color:#C6011F">0.145</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
												<path fill-rule="evenodd"
													d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
													clip-rule="evenodd" />
											</svg>
										</td>
										<td>0.176</td>
									</tr>
									<tr>
										<td>Gemini-Pro* (+reason)</td>
										<td>0.274</td>
										<td>0.070</td>
										<td>0.304</td>
										<td>0.211</td>
										<td>0.215</td>
										<td>0.239</td>
										<td>
											<strong style="color:#C6011F">0.267</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
												<path fill-rule="evenodd"
													d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
													clip-rule="evenodd" />
											</svg>
										</td>
										<td>0.195</td>
										<td>0.129</td>
										<td>0.060</td>
										<td>0.178</td>
									</tr>
									<tr>
										<td>Claude-thinking*</td>
										<td>0.299</td>
										<td>0.044</td>
										<td>0.262</td>
										<td>0.229</td>
										<td>0.209</td>
										<td>0.172</td>
										<td>0.140</td>
										<td>0.175</td>
										<td>0.150</td>
										<td>0.069</td>
										<td>0.141</td>
									</tr>
									<tr>
										<td>Claude-thinking* (+reason)</td>
										<td>0.291</td>
										<td>0.023</td>
										<td>0.254</td>
										<td>0.214</td>
										<td>0.196</td>
										<td>0.207</td>
										<td>0.260</td>
										<td>0.183</td>
										<td>0.155</td>
										<td>-0.001</td>
										<td>0.161</td>
									</tr>
									<tr>
										<td>o4-mini</td>
										<td>0.334</td>
										<td>0.062</td>
										<td>0.306</td>
										<td>0.134</td>
										<td>0.209</td>
										<td>0.169</td>
										<td>0.161</td>
										<td>0.120</td>
										<td>0.096</td>
										<td>0.028</td>
										<td>0.115</td>
									</tr>
									<tr>
										<td>o4-mini (+reason)</td>
										<td>0.326</td>
										<td>0.056</td>
										<td>
											<strong style="color:#C6011F">0.322</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
												<path fill-rule="evenodd"
													d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
													clip-rule="evenodd" />
											</svg>
										</td>
										<td>0.151</td>
										<td>0.214</td>
										<td>0.215</td>
										<td>0.229</td>
										<td>
											<strong style="color:#C6011F">0.347</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
												<path fill-rule="evenodd"
													d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
													clip-rule="evenodd" />
											</svg>
										</td>
										<td>0.137</td>
										<td>0.016</td>
										<td>0.189</td>
									</tr>
									<tr>
										<td rowspan="10" class="bold">Pair Comparison</td>
										<td>GPT-4.1</td>
										<td>0.541</td>
										<td>
											<strong style="color:#C6011F">0.589</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
												<path fill-rule="evenodd"
													d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
													clip-rule="evenodd" />
											</svg>
										</td>
										<td>0.508</td>
										<td>0.484</td>
										<td>0.531</td>
										<td>0.540</td>
										<td>0.520</td>
										<td>0.530</td>
										<td>
											<strong style="color:#C6011F">0.590</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
												<path fill-rule="evenodd"
													d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
													clip-rule="evenodd" />
											</svg>
										</td>
										<td>
											<strong style="color:#C6011F">0.563</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
												<path fill-rule="evenodd"
													d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
													clip-rule="evenodd" />
											</svg>
										</td>
										<td>
											<strong style="color:#C6011F">0.549</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
												<path fill-rule="evenodd"
													d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
													clip-rule="evenodd" />
											</svg>
										</td>
									</tr>
									<tr>
										<td>GPT-4.1 (+reason)</td>
										<td>0.550</td>
										<td>0.584</td>
										<td>0.501</td>
										<td>0.521</td>
										<td>
											<strong style="color:#C6011F">0.539</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
												<path fill-rule="evenodd"
													d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
													clip-rule="evenodd" />
											</svg>
										</td>
										<td>0.520</td>
										<td>0.520</td>
										<td>0.477</td>
										<td>0.513</td>
										<td>0.540</td>
										<td>0.514</td>
									</tr>
									<tr>
										<td>GPT-4o</td>
										<td>0.513</td>
										<td>0.488</td>
										<td>0.499</td>
										<td>0.510</td>
										<td>0.503</td>
										<td>0.560</td>
										<td>0.517</td>
										<td>
											<strong style="color:#C6011F">0.550</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
												<path fill-rule="evenodd"
													d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
													clip-rule="evenodd" />
											</svg>
										</td>
										<td>0.543</td>
										<td>0.470</td>
										<td>0.528</td>
									</tr>
									<tr>
										<td>GPT-4o (+reason)</td>
										<td>0.500</td>
										<td>0.537</td>
										<td>
											<strong style="color:#C6011F">0.511</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
												<path fill-rule="evenodd"
													d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
													clip-rule="evenodd" />
											</svg>
										</td>
										<td>0.509</td>
										<td>0.514</td>
										<td>0.542</td>
										<td>0.490</td>
										<td>0.545</td>
										<td>0.522</td>
										<td>0.528</td>
										<td>0.525</td>
									</tr>
									<tr>
										<td>Gemini-Pro*</td>
										<td>0.533</td>
										<td>0.521</td>
										<td>0.496</td>
										<td>0.533</td>
										<td>0.521</td>
										<td>
											<strong style="color:#C6011F">0.562</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
												<path fill-rule="evenodd"
													d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
													clip-rule="evenodd" />
											</svg>
										</td>
										<td>
											<strong style="color:#C6011F">0.566</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
												<path fill-rule="evenodd"
													d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
													clip-rule="evenodd" />
											</svg>
										</td>
										<td>0.523</td>
										<td>0.505</td>
										<td>0.505</td>
										<td>0.532</td>
									</tr>
									<tr>
										<td>Gemini-Pro* (+reason)</td>
										<td>0.526</td>
										<td>0.528</td>
										<td>0.513</td>
										<td>0.514</td>
										<td>0.520</td>
										<td>0.548</td>
										<td>0.562</td>
										<td>0.495</td>
										<td>0.522</td>
										<td>0.538</td>
										<td>0.533</td>
									</tr>
									<tr>
										<td>Claude-thinking*</td>
										<td>0.561</td>
										<td>0.568</td>
										<td>0.508</td>
										<td>0.502</td>
										<td>0.535</td>
										<td>0.539</td>
										<td>0.523</td>
										<td>0.518</td>
										<td>0.521</td>
										<td>0.528</td>
										<td>0.526</td>
									</tr>
									<tr>
										<td>Claude-thinking* (+reason)</td>
										<td>
											<strong style="color:#C6011F">0.567</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
												<path fill-rule="evenodd"
													d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
													clip-rule="evenodd" />
											</svg>
										</td>
										<td>0.550</td>
										<td>0.506</td>
										<td>0.519</td>
										<td>0.536</td>
										<td>0.512</td>
										<td>0.522</td>
										<td>0.512</td>
										<td>0.547</td>
										<td>0.512</td>
										<td>0.521</td>
									</tr>
									<tr>
										<td>o4-mini</td>
										<td>0.556</td>
										<td>0.549</td>
										<td>0.508</td>
										<td>
											<strong style="color:#C6011F">0.536</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
												<path fill-rule="evenodd"
													d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
													clip-rule="evenodd" />
											</svg>
										</td>
										<td>0.537</td>
										<td>0.552</td>
										<td>0.498</td>
										<td>0.522</td>
										<td>0.518</td>
										<td>0.495</td>
										<td>0.517</td>
									</tr>
									<tr>
										<td>o4-mini (+reason)</td>
										<td>0.521</td>
										<td>0.564</td>
										<td>0.522</td>
										<td>0.513</td>
										<td>0.530</td>
										<td>0.534</td>
										<td>0.510</td>
										<td>0.507</td>
										<td>0.512</td>
										<td>0.483</td>
										<td>0.509</td>
									</tr>
								</tbody>
							</table>
						</div>
					</div>
				</div>
			</div>
			</div>
		</section>
		<section>
			<h2 class="title is-3" id="Evaluation" style="text-align: center;"> Case Study </h2>
			<p style="text-align: center;">In this case study, we evaluated two models and identified deficiencies in image-text consistency, instruction following fidelity, and output quality. <strong>In contrast, our dataset provides high-quality multi-turn multimodal dialogue samples that
					demonstrate robust image-text alignment and strong adherence to instruction prompts throughout the interaction.</strong></p>
			<div id="dialog-sample-container"></div>
			<svg style="display:none;">
				<symbol id="icon-gemini" viewBox="0 0 24 24">
					<?xml version="1.0" ?><svg id="Apple" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg">
						<defs>
							<style>
								.cls-1{fill:#f0432c;}.cls-2{fill:#ffbf00;}.cls-3{fill:#24aa49;}.cls-4{fill:#3384fc;}
							</style>
						</defs>
						<title />
						<path class="cls-1" d="M90,93.25l65.06,65.06a140.31,140.31,0,0,1,201-1v0l65-65.09a232.33,232.33,0,0,0-331,1Z" />
						<path class="cls-2" d="M90.85,419.76l65-65a140.34,140.34,0,0,1,1-198.71L91.81,91a232.35,232.35,0,0,0-1,328.8Z" />
						<polygon class="cls-3" points="301.98 301.84 301.98 302.8 302.94 302.8 301.98 301.84" />
						<path class="cls-3" d="M255.5,488.21a231.65,231.65,0,0,0,164.55-68.3L355,354.84a140.3,140.3,0,0,1-199,0l-65,65A231.65,231.65,0,0,0,255.5,488.21Z" />
						<path class="cls-4" d="M484.36,210.33l-181.42.25V302.8h85.84a140.7,140.7,0,0,1-33.69,52.91l65.05,65.06a232.65,232.65,0,0,0,64.22-210.44Z" />
					</svg>
				</symbol>
				<symbol id="icon-janus" viewBox="0 0 24 24">
					<svg xmlns="http://www.w3.org/2000/svg" shape-rendering="geometricPrecision" text-rendering="geometricPrecision" image-rendering="optimizeQuality" fill-rule="evenodd" clip-rule="evenodd" viewBox="0 0 512 509.64">
						<path fill="#fff" d="M115.612 0h280.775C459.974 0 512 52.026 512 115.612v278.415c0 63.587-52.026 115.613-115.613 115.613H115.612C52.026 509.64 0 457.614 0 394.027V115.612C0 52.026 52.026 0 115.612 0z" />
						<path fill="#4D6BFE" fill-rule="nonzero"
							d="M440.898 139.167c-4.001-1.961-5.723 1.776-8.062 3.673-.801.612-1.479 1.407-2.154 2.141-5.848 6.246-12.681 10.349-21.607 9.859-13.048-.734-24.192 3.368-34.04 13.348-2.093-12.307-9.048-19.658-19.635-24.37-5.54-2.449-11.141-4.9-15.02-10.227-2.708-3.795-3.447-8.021-4.801-12.185-.861-2.509-1.725-5.082-4.618-5.512-3.139-.49-4.372 2.142-5.601 4.349-4.925 9.002-6.833 18.921-6.647 28.962.432 22.597 9.972 40.597 28.932 53.397 2.154 1.47 2.707 2.939 2.032 5.082-1.293 4.41-2.832 8.695-4.186 13.105-.862 2.817-2.157 3.429-5.172 2.205-10.402-4.346-19.391-10.778-27.332-18.553-13.481-13.044-25.668-27.434-40.873-38.702a177.614 177.614 0 00-10.834-7.409c-15.512-15.063 2.032-27.434 6.094-28.902 4.247-1.532 1.478-6.797-12.251-6.736-13.727.061-26.285 4.653-42.288 10.777-2.34.92-4.801 1.593-7.326 2.142-14.527-2.756-29.608-3.368-45.367-1.593-29.671 3.305-53.368 17.329-70.788 41.272-20.928 28.785-25.854 61.482-19.821 95.59 6.34 35.943 24.683 65.704 52.876 88.974 29.239 24.123 62.911 35.943 101.32 33.677 23.329-1.346 49.307-4.468 78.607-29.27 7.387 3.673 15.142 5.144 28.008 6.246 9.911.92 19.452-.49 26.839-2.019 11.573-2.449 10.773-13.166 6.586-15.124-33.915-15.797-26.47-9.368-33.24-14.573 17.235-20.39 43.213-41.577 53.369-110.222.8-5.448.121-8.877 0-13.287-.061-2.692.553-3.734 3.632-4.041 8.494-.981 16.742-3.305 24.314-7.471 21.975-12.002 30.84-31.719 32.933-55.355.307-3.612-.061-7.348-3.879-9.245v-.003zM249.4 351.89c-32.872-25.838-48.814-34.352-55.4-33.984-6.155.368-5.048 7.41-3.694 12.002 1.415 4.532 3.264 7.654 5.848 11.634 1.785 2.634 3.017 6.551-1.784 9.493-10.587 6.55-28.993-2.205-29.856-2.635-21.421-12.614-39.334-29.269-51.954-52.047-12.187-21.924-19.267-45.435-20.435-70.542-.308-6.061 1.478-8.207 7.509-9.307 7.94-1.471 16.127-1.778 24.068-.615 33.547 4.9 62.108 19.902 86.054 43.66 13.666 13.531 24.007 29.699 34.658 45.496 11.326 16.778 23.514 32.761 39.026 45.865 5.479 4.592 9.848 8.083 14.035 10.656-12.62 1.407-33.673 1.714-48.075-9.676zm15.899-102.519c.521-2.111 2.421-3.658 4.722-3.658a4.74 4.74 0 011.661.305c.678.246 1.293.614 1.786 1.163.861.859 1.354 2.083 1.354 3.368 0 2.695-2.154 4.837-4.862 4.837a4.748 4.748 0 01-4.738-4.034 5.01 5.01 0 01.077-1.981zm47.208 26.915c-2.606.996-5.2 1.778-7.707 1.88-4.679.244-9.787-1.654-12.556-3.981-4.308-3.612-7.386-5.631-8.679-11.941-.554-2.695-.247-6.858.246-9.246 1.108-5.144-.124-8.451-3.754-11.451-2.954-2.449-6.711-3.122-10.834-3.122-1.539 0-2.954-.673-4.001-1.224-1.724-.856-3.139-3-1.785-5.634.432-.856 2.525-2.939 3.018-3.305 5.6-3.185 12.065-2.144 18.034.244 5.54 2.266 9.727 6.429 15.759 12.307 6.155 7.102 7.263 9.063 10.773 14.39 2.771 4.163 5.294 8.451 7.018 13.348.877 2.561.071 4.74-2.341 6.277-.981.625-2.109 1.044-3.191 1.458z" />
					</svg>
				</symbol>
				<symbol id="icon-dataset" viewBox="0 0 24 24" fill="currentColor">
					<svg id="Layer_1" data-name="Layer 1" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 122.88 116.41">
						<defs>
							<style>
								.cls-1{fill-rule:evenodd;}
							</style>
						</defs>
						<title>database-table</title>
						<path class="cls-1"
							d="M16.49,24.88C24.05,27.41,34.57,29,46.26,29S68.48,27.41,76,24.88c6.63-2.22,10.73-4.9,10.73-7.52S82.67,12.06,76,9.84C68.48,7.33,58,5.75,46.27,5.75S24.06,7.33,16.49,9.84c-14.06,4.7-14.46,10.21,0,15ZM64.91,55.34h48.73a9.27,9.27,0,0,1,9.24,9.24v42.58a9.27,9.27,0,0,1-9.24,9.25H64.91a9.27,9.27,0,0,1-9.24-9.25V64.58a9.27,9.27,0,0,1,9.24-9.24ZM91.09,99.18H118v12H91.09v-12Zm-30.89,0H87.13v12H60.2v-12Zm0-31.89H87.13v12H60.2v-12Zm0,15.94H87.13v12H60.2v-12ZM91.09,67.29H118v12H91.09v-12Zm0,15.94H118v12H91.09v-12ZM5.82,45.77c.52,2.45,4.5,4.91,10.68,7,7.22,2.42,17.16,3.95,28.24,4.08v5.77c-11.67-.13-22.25-1.78-30.05-4.39A35.86,35.86,0,0,1,5.84,54V71.27c.52,2.45,4.5,4.91,10.68,7,7.22,2.4,17.15,3.94,28.22,4.07v5.75c-11.67-.14-22.25-1.78-30.05-4.4A36.08,36.08,0,0,1,5.83,79.5V96.75c.52,2.45,4.51,4.91,10.68,7,7.22,2.41,17.16,4,28.23,4.08v5.75c-11.67-.13-22.24-1.78-30-4.4C10.4,107.72,0,103,0,97.38V95.55C0,69.86,0,43.06,0,17.41c0-5.43,5.61-10,14.66-13C22.82,1.68,34,0,46.27,0S69.7,1.68,77.87,4.41s13.64,6.78,14.55,11.53a3,3,0,0,1,.16,1v28.6H86.8V26.09a36.69,36.69,0,0,1-8.93,4.22c-8.15,2.75-19.31,4.41-31.58,4.41S22.83,33,14.66,30.31A36.26,36.26,0,0,1,5.8,26.14V45.77Z" />
					</svg>
				</symbol>
			</svg>
			<div class="diag-dropdown-container">
				<label for="exampleSelect">Select Cases: </label>
				<select id="exampleSelect"></select>
			</div>
			<div class="diag-container">
				<div id="Gemini" class="diag-section diag-default">
					<div class="diag-section-header diag-default">
						<svg>
							<use xlink:href="#icon-gemini"></use>
						</svg>
						<span>Gemini ‚ñº</span>
					</div>
					<div class="diag-content"></div>
				</div>
				<div id="Janus" class="diag-section diag-default">
					<div class="diag-section-header diag-default">
						<svg>
							<use xlink:href="#icon-janus"></use>
						</svg>
						<span>Janus ‚ñº</span>
					</div>
					<div class="diag-content"></div>
				</div>
				<div id="Ours" class="diag-section diag-orange">
					<div class="diag-section-header diag-orange">
						<svg>
							<use xlink:href="#icon-dataset"></use>
						</svg>
						<span>Ours ‚ñº</span>
					</div>
					<div class="diag-content"></div>
				</div>
			</div>
			<script>
				const data = [
							{
				"model": "Janus",
				"title": "Case 1",
				"figures": 1,
				"prompt": "What immediate actions would you recommend for the local authorities to take in response to the situation depicted in the image, and why?",
				"response": "The image depicts a flooded area, with water covering the street and reflecting surrounding trees and buildings. The floodwater appears to be at a significant depth, potentially affecting traffic and access to homes. Immediate actions for local authorities to take in response to this situation would include: 1. *Evacuation and Safety Measures*: Ensure the safety of residents by providing alternative shelters and evacuation routes. This includes setting up emergency shelters and coordinating with local emergency services. 2. *Flood Control and Drainage*: Activate flood control measures such as opening storm drains and releasing water from nearby reservoirs or flood management systems to reduce water accumulation. 3. *Communication and Coordination*: Establish communication channels with affected residents to provide updates on the flood situation, safety instructions, and resources available. 4. *Damage Assessment and Repair*: Send teams to assess the extent of the damage and initiate repair efforts to restore infrastructure, such as roads, bridges, and utilities. 5. *Environmental Monitoring*: Monitor water quality and potential contamination risks to ensure public health and safety.",
				"imageR": ".\\assets\\diag_sample\\image1.jpg",
				"imageP": "#",
				"color": "default"
							},
							{
				"model": "Janus",
				"title": "Case 1",
				"figures": 1,
				"prompt": "Please tell me how to build temporary roads or bridges, and generate one in the diagram.",
				"response": "To build temporary roads or bridges, follow these steps: 1. *Assessment and Planning*: Evaluate the flood-affected area to determine the most suitable locations for temporary roads or bridges. Consider factors such as water depth, flow velocity, and structural stability. 2. *Materials and Equipment*: Gather materials like steel beams, concrete blocks, and construction tools. Ensure that the equipment is suitable for the water conditions, such as using waterproof materials and equipment. 3. *Foundation Preparation*: Prepare the ground by clearing debris and leveling the surface. Ensure that the foundation is stable and can support the weight of the temporary structure. 4. *Construction*: Construct the temporary road or bridge using the prepared materials. Use strong materials to ensure durability and safety. 5. *Testing and Maintenance*: Test the temporary structure for stability and safety. Regularly maintain it to ensure it remains functional and safe for use.",
				"imageR": ".\\assets\\diag_sample\\image2.jpg",
				"imageP": "#",
				"color": "default"
							},
							{
				"model": "Janus",
				"title": "Case 1",
				"figures": 3,
				"prompt": "How long does it take to build such a bridge?",
				"response": "Building a temporary bridge or road in flood-affected areas requires careful planning and execution. The time taken depends on several factors, including the complexity of the structure, the depth of the water, and the availability of materials and equipment. 1. *Planning and Assessment*: This phase typically takes a few days to a week. It involves assessing the flood conditions, determining the best location for the bridge or road, and planning the construction process. 2. *Material Procurement*: Gathering materials such as steel beams, concrete blocks, and construction tools can take a few days. 3. *Construction*: Building the bridge or road can take several weeks to a few months, depending on the complexity and the number of workers involved. 4. *Testing and Maintenance*: Once completed, the bridge or road needs to be tested for safety and durability. Regular maintenance is required to ensure its longevity. Overall, the construction of a temporary bridge or road in flood-affected areas can take anywhere from a few weeks to several months.",
				"imageR": "#",
				"imageP": "#",
				"color": "default"
							},
							{
				"model": "Gemini",
				"title": "Case 1",
				"figures": 1,
				"prompt": "What immediate actions would you recommend for the local authorities to take in response to the situation depicted in the image, and why?",
				"response": "The image depicts a street scene with significant floodwater inundating buildings and vehicles. Some vehicles appear partially submerged. *Immediate Actions for Local Authorities:* 1. *Issue Emergency Alerts and Warnings:* Utilize all available channels (sirens, SMS, social media, radio, television) to inform residents about the dangerous flooding and advise them to seek higher ground immediately. Emphasize the risks of driving or walking through floodwaters. 2. *Activate Emergency Response Teams:* Deploy police, fire department, and emergency medical services to the affected areas. Their immediate priorities should be: * *Life Safety:* Conducting search and rescue operations for individuals trapped in homes or vehicles. * *Medical Assistance:* Providing aid to anyone injured. * *Security:* Securing the flooded areas to prevent looting and ensure public safety. 3. *Establish Shelters:* Open designated emergency shelters (schools, community centers, etc.) for displaced residents, providing them with temporary housing, food, water, and essential supplies. 4. *Assess the Situation and Infrastructure:* Begin assessing the extent of the damage to roads, bridges, power lines, and other critical infrastructure. This will help in understanding the scope of the disaster and planning the subsequent response. 5. *Block Off Affected Roads:* Close roads leading into and within the flooded areas to prevent further traffic and potential accidents in the dangerous conditions. *Why These Actions Are Crucial:* These immediate actions prioritize the safety and well-being of the affected population. Swift warnings and the deployment of emergency services are essential to minimize casualties and injuries. Providing shelter addresses the immediate needs of those displaced from their homes. Assessing the damage and securing the area lays the groundwork for a coordinated and effective longer-term recovery effort. Preventing access to flooded areas reduces the risk of further harm and allows emergency responders to work more efficiently.",
				"imageR": ".\\assets\\diag_sample\\image3.png",
				"imageP": "#",
				"color": "default"
							},
							{
				"model": "Gemini",
				"title": "Case 1",
				"figures": 1,
				"prompt": "In addition to these immediate actions, what long-term strategies could the local authorities implement to prevent or mitigate future flooding incidents in this area?",
				"response": "I will generate an image depicting a flooded street scene to help illustrate potential immediate actions for local authorities.",
				"imageR": ".\\assets\\diag_sample\\image4.png",
				"imageP": "#",
				"color": "default"
							},
							{
				"model": "Gemini",
				"title": "Case 1",
				"figures": 1,
				"prompt": "It seems that your image does not show any long-term strategies, please supplement with textual description.",
				"response": "Immediate Actions:\n* Emergency Response Activation: Deploy emergency services, including police, fire department, and ambulance, to the affected areas.\n* Evacuation and Shelter: Initiate evacuation procedures for residents in high-risk zones, providing safe shelter and essential supplies.\n* Road Closure and Traffic Management: Close flooded roads and manage traffic flow to ensure safety and allow access for emergency vehicles.\n* Damage Assessment: Begin preliminary assessments of the extent of the flooding and damage to infrastructure and property.\n* Communication and Information: Keep the public informed about the situation, safety measures, and available assistance through various channels.\n* Resource Mobilization: Activate and coordinate resources, including personnel, equipment, and supplies, for rescue and relief efforts.",
				"imageR": ".\\assets\\diag_sample\\image5.png",
				"imageP": "#",
				"color": "default"
							},
							{
				"model": "Ours",
				"title": "Case 1",
				"figures": 3,
				"prompt": "What immediate actions would you recommend for the local authorities to take in response to the situation depicted in the image, and why?",
				"response": "The image shows a flooded street with a traffic light and power lines submerged in water. Based on the current state, I would recommend that the local authorities take immediate actions to ensure the safety of residents and maintain traffic flow.\n\nFirst and foremost, they should deploy emergency personnel to the affected areas to rescue people that might be in danger and ensure their safety. Given the possible risk of electrocution from the submerged power lines, it is vital to shut off and secure the power supply to the area.\n\nNext, the local authorities should assess the traffic light's functionality and ensure it remains operational to manage the flow of vehicles and pedestrians. If necessary, they may need to replace or repair the damaged traffic light and lines to restore proper functionality.\n\nFurthermore, they should work on clearing and draining the water accumulated in the streets to minimize the risk of infections and to restore normality to daily life. They might consider using specialized machinery and personnel for this task, if the situation is severe or widespread.\n\nLastly, the local authorities should communicate their actions and plans effectively to the public via social media, emergency alerts, and notifications to keep residents informed about the situation and any assistance or resources available.\n\nConclusion: Look for opportunities to improve the traffic flow, safety, and well-being of the community during the recovery process.",
				"imageR": "#",
				"imageP": "#",
				"color": "yellow"
							},
							{
				"model": "Ours",
				"title": "Case 1",
				"figures": 3,
				"prompt": "In addition to these immediate actions, what long-term strategies could the local authorities implement to prevent or mitigate future flooding incidents in this area?",
				"response": "Based on the previous response, I would recommend that the local authorities in the area take immediate actions to ensure the safety of residents and maintain traffic flow in the flooded street. To achieve this, they should deploy emergency personnel to rescue people that may be in danger and ensure their safety. Given the possible risk of electrocution from the submerged power lines, it is crucial to shut off and secure the power supply to the area. \n\nIn addition to these immediate actions, long-term strategies could include implementing flood-prevention measures, such as improving the city's drainage system, raising roads and infrastructure to minimize the impact of flooding, creating emergency response plans, and conducting regular maintenance checks on traffic lights and power lines to reduce the risk of damage during extreme weather events.\n\nThe implementation of these measures can help prevent or mitigate future flooding incidents in the area, ensuring the safety and well-being of residents and minimizing the disruption to daily life.",
				"imageR": "#",
				"imageP": "#",
				"color": "yellow"
							},
							{
				"model": "Ours",
				"title": "Case 1",
				"figures": 1,
				"prompt": "That's a great overview. Could you elaborate on how the local authorities might go about 'improving the city's drainage system'? What specific improvements or technologies might be considered?",
				"response": "To improve the city's drainage system and mitigate future flooding effectively, local authorities can consider several specific improvements and technologies:\n\n1. *Upgrading and Expanding Drainage Infrastructure*: By increasing the capacity of existing drainage systems and constructing additional channels, the city can better accommodate heavy rainfall and reduce flood risk. This might involve widening culverts, enlarging stormwater pipes, and constructing detention basins to manage water flow more effectively.\n\n2. *Implementing Green Infrastructure*: Integrating green technology such as rain gardens, permeable pavements, and green roofs can help absorb and divert rainwater naturally. These solutions not only reduce immediate drainage loads but also improve urban aesthetics and biodiversity.\n\n3. *Advanced Hydrological Modeling*: Utilizing sophisticated software to simulate rainfall scenarios can help identify potential weaknesses in drainage and plan infrastructure improvements more strategically. This modeling aids in efficient planning and resource allocation.\n\n4. *Sensor-Based Monitoring Systems*: Implementing sensor technology in critical drainage locations can provide real-time data on water levels and system performance. This allows for proactive management and quick responses during heavy rain events.\n\n5. *Public Awareness and Engagement*: Educating citizens about flood risks and encouraging practices that reduce blockage in drainage systems, like proper disposal of waste, can significantly improve overall efficiency.\n\n6. *Regular Maintenance and Cleaning*: Conducting frequent checks and cleaning of drainage systems ensures they remain clear of debris, reducing the likelihood of blockages during storms.\n\n7. *Infrastructure Integration*: Consideration of multi-use designs that integrate drainage systems with urban features can both save space and enhance functionality, such as park spaces that double as flood basins.\n\nThese strategies, when combined, can create a robust drainage system tailored to the city's unique needs, reducing the impact of future flooding events. \n\nWould you like more information on the benefits of green infrastructure?",
				"imageR": ".\\assets\\diag_sample\\Figure6.jpg",
				"imageP": "#",
				"color": "yellow"
							},
							{
				"model": "Gemini",
				"title": "Case 2",
				"figures": 2,
				"prompt": "Considering the cleanliness and arrangement, what can you infer about the maintenance of this bathroom?",
				"response": "The bathroom appears to be meticulously maintained. The surfaces are clear of clutter and look clean, suggesting regular tidying. The arrangement of items seems deliberate and organized, further indicating a commitment to upkeep.",
				"imageR": ".\\assets\\diag_sample\\image8.png",
				"imageP": ".\\assets\\diag_sample\\image7.png",
				"color": "default"
							},
							{
				"model": "Gemini",
				"title": "Case 2",
				"figures": 0,
				"prompt": "The bathroom image you generated looks a bit cluttered. What kind of updates or additions could enhance the bathroom's functionality or aesthetics?",
				"response": "The current image shows a bathroom with some items on the counter and a few things in the shower area. Updates to enhance functionality and aesthetics could include adding closed storage like drawers or cabinets to reduce visible clutter, incorporating a consistent color scheme for towels and accessories, and perhaps a larger mirror to visually expand the space.",
				"imageR": "#",
				"imageP": ".\\assets\\diag_sample\\image9.png",
				"color": "default"
							},
							{
				"model": "Gemini",
				"title": "Case 2",
				"figures": 1,
				"prompt": "That's helpful! Considering the potential for adding storage or decorative elements, how would you prioritize these additions based on cost-effectiveness and impact on the bathroom's overall appeal?",
				"response": "The image displays a clean and relatively well-maintained bathroom. The sink area is tidy, and the surfaces appear to be clear of excessive clutter. This suggests a regular cleaning routine. The presence of what looks like a toothbrush holder and a neatly placed hand towel indicates attention to organization.",
				"imageR": ".\\assets\\diag_sample\\image10.png",
				"imageP": "#",
				"color": "default"
							},
							{
				"model": "Janus",
				"title": "Case 2",
				"figures": 2,
				"prompt": "Considering the cleanliness and arrangement, what can you infer about the maintenance of this bathroom?",
				"response": "The bathroom appears to be well-maintained, as evidenced by the cleanliness of the tiles, the organized placement of items, and the absence of visible dirt or clutter. The mirror is clean, and the sink and surrounding area are tidy, suggesting regular upkeep. The presence of a trash bin and a toilet paper holder further indicates attention to cleanliness and organization.",
				"imageR": ".\\assets\\diag_sample\\image12.png",
				"imageP": ".\\assets\\diag_sample\\image11.png",
				"color": "default"
							},
							{
				"model": "Janus",
				"title": "Case 2",
				"figures": 1,
				"prompt": "The toilet picture you generated looks dark and plain. What kind of updates or additions could enhance the bathroom's functionality or aesthetics?",
				"response": "To enhance the functionality and aesthetics of the bathroom, several updates or additions could be considered:\n\n1. Lighting: Adding a brighter or more modern light fixture above the mirror could improve visibility and create a more inviting atmosphere.\n2. Storage Solutions: Installing additional shelves or cabinets could provide more storage space for toiletries and other bathroom essentials.\n3. Decorative Elements: Introducing decorative elements such as a small plant, artwork, or a stylish soap dispenser could add personality and warmth to the space.\n4. Color Scheme: Updating the tiles or adding colorful accents could refresh the overall look and feel of the bathroom.\n5. Mirror Frame: A decorative frame for the mirror could add a touch of elegance and make the space feel more cohesive.",
				"imageR": ".\\assets\\diag_sample\\image13.png",
				"imageP": "#",
				"color": "default"
							},
							{
				"model": "Janus",
				"title": "Case 2",
				"figures": 3,
				"prompt": "Please present the complete scene of this toilet to me, not just the sink.",
				"response": "The complete scene of this toilet includes a white ceramic toilet with a standard flush mechanism, positioned against tiled walls. Above the toilet, there is a rectangular mirror with a simple frame, reflecting part of the room. To the right of the toilet, there is a wall-mounted metal rack with a wire basket, likely intended for holding toilet paper rolls. The floor is tiled in a light color, matching the walls, and there is a small trash bin with a lid located near the toilet. The overall design is functional and minimalistic, with a focus on cleanliness and practicality.",
				"imageR": "#",
				"imageP": "#",
				"color": "default"
							},
							{
				"model": "Ours",
				"title": "Case 2",
				"figures": 1,
				"prompt": "Considering the cleanliness and arrangement, what can you infer about the maintenance of this bathroom?",
				"response": "Based on the cleanliness of the sink area, bright white room, and mirror, it can be inferred that the bathroom is well-maintained. The white sink, white tiles, and white curtains contribute to a clean and organized appearance, which suggests that the space is cared for. Another notable aspect of the bathroom is the empty wall space, also shown in the image, which could indicate that the bathroom might need to be updated or finished. Overall, the cleanliness and arrangement of the bathroom seem to show good maintenance practices.",
				"imageR": "#",
				"imageP": ".\\assets\\diag_sample\\image14.png",
				"color": "yellow"
							},
							{
				"model": "Ours",
				"title": "Case 2",
				"figures": 1,
				"prompt": "You mentioned that the bathroom might need to be updated or finished due to the empty wall space. What kind of updates or additions could enhance the bathroom's functionality or aesthetics?",
				"response": "Updating or finishing the empty wall space in the bathroom could enhance its functionality and aesthetics. Some possible additions or updates could include installing additional storage, such as shelves or cabinets, to organize toiletries and accessories; installing a mirror above the sink to create the illusion of a larger space; or even incorporating decorative elements like artwork or plants to add visual interest and make the bathroom feel more inviting.",
				"imageR": ".\\assets\\diag_sample\\image15.jpg",
				"imageP": "#",
				"color": "yellow"
							},
							{
				"model": "Ours",
				"title": "Case 2",
				"figures": 3,
				"prompt": "That's helpful! Considering the potential for adding storage or decorative elements, how would you prioritize these additions based on cost-effectiveness and impact on the bathroom's overall appeal?",
				"response": "To prioritize additions based on cost-effectiveness and impact on the bathroom's overall appeal, I recommend starting with functional improvements that also enhance aesthetics. Here are some suggestions:\n\n1. Additional Storage: Installing floating shelves or a vanity with built-in cabinets can provide much-needed storage while maintaining a sleek look. This is both practical and visually appealing.\n\n2. Mirror Above Sink: Adding a mirror above the sink not only serves a practical purpose but also makes the space feel larger and brighter.\n\n3. Decorative Elements: Incorporating small, affordable items like a plant or a piece of artwork can add personality and warmth to the bathroom without breaking the bank.\n\n4. Lighting: Upgrading the lighting fixture can significantly improve the ambiance of the bathroom. Opt for energy-efficient LED lights that offer both cost savings and better illumination.\n\n5. Tile Accents: Adding a tile backsplash or border can give the bathroom a fresh look without requiring a full renovation.\n\nEach of these additions can be tailored to fit your budget and style preferences, ensuring a balanced approach to enhancing both functionality and aesthetics. Would you like to see some visual examples of these ideas?",
				"imageR": "#",
				"imageP": "#",
				"color": "yellow"
							}
							];
				
				const exampleSelect = document.getElementById('exampleSelect');
				const uniqueTitles = [...new Set(data.map(item => item.title))];
				uniqueTitles.forEach(title => {
				  const option = document.createElement('option');
				  option.value = title;
				  option.textContent = title;
				  exampleSelect.appendChild(option);
				});
				
				function parseBold(text) {
				  return text.replace(/\*\*(.*?)\*\*/g, '<strong>$1</strong>');
				}
				
				function parseOrderedList(text) {
				    const lines = text.split('\n');
				    let htmlOutput = '';
				    let currentListItemContent = [];
				    let inListScope = false;
				
				    function flushListItem() {
				        if (currentListItemContent.length > 0) {
				            htmlOutput += `<li>${parseBold(currentListItemContent.join('<br>'))}</li>`;
				            currentListItemContent = [];
				        }
				    }
				
				    function startList() {
				        if (!inListScope) {
				            htmlOutput += '<ol>';
				            inListScope = true;
				        }
				    }
				
				    function endList() {
				        if (inListScope) {
				            flushListItem();
				            htmlOutput += '</ol>';
				            inListScope = false;
				        }
				    }
				
				    for (const line of lines) {
				        if (/^\d+\.\s/.test(line)) {
				            startList();
				            flushListItem();
				            currentListItemContent.push(line.replace(/^\d+\.\s/, ''));
				        } else if (inListScope && line.trim() === '' && currentListItemContent.length > 0) {
				            currentListItemContent.push('');
				        } else if (inListScope && currentListItemContent.length > 0) {
				             currentListItemContent.push(line);
				        } else {
				            endList();
				            if (line.trim() !== '') {
				                htmlOutput += `<p>${parseBold(line)}</p>`;
				            }
				        }
				    }
				    endList();
				    if (htmlOutput.trim() === '' && text.trim() !== '') {
				        return '<p>' + parseBold(text.replace(/\n/g, '<br>')) + '</p>';
				    }
				    return htmlOutput;
				}
				
				function typeWriter(html, el, callback) {
				  el.innerHTML = '';
				  const tempDiv = document.createElement('div');
				  tempDiv.innerHTML = html;
				  let charDelay = 5;
				
				  let currentNode = tempDiv.firstChild;
				  function processNextRootNode() {
				    if (currentNode) {
				      const next = currentNode.nextSibling;
				      typeNodeRecursive(currentNode, el, () => {
				        currentNode = next;
				        processNextRootNode();
				      });
				    } else {
				      callback && callback();
				    }
				  }
				
				  function typeNodeRecursive(sourceNode, targetParent, doneCallback) {
				    if (sourceNode.nodeType === Node.TEXT_NODE) {
				      let text = sourceNode.textContent;
				      let i = 0;
				      function typeChar() {
				        if (i < text.length) {
				          targetParent.appendChild(document.createTextNode(text[i]));
				          i++;
				          setTimeout(typeChar, charDelay);
				        } else {
				          doneCallback();
				        }
				      }
				      typeChar();
				    } else if (sourceNode.nodeType === Node.ELEMENT_NODE) {
				      const newElement = document.createElement(sourceNode.nodeName);
				      for (let attr of sourceNode.attributes) {
				        newElement.setAttribute(attr.name, attr.value);
				      }
				      targetParent.appendChild(newElement);
				
				      let childNode = sourceNode.firstChild;
				      function processChildren() {
				        if (childNode) {
				          const nextChild = childNode.nextSibling;
				          typeNodeRecursive(childNode, newElement, () => {
				            childNode = nextChild;
				            processChildren();
				          });
				        } else {
				          doneCallback();
				        }
				      }
				      processChildren();
				    } else {
				      doneCallback();
				    }
				  }
				  processNextRootNode();
				}
				
				function createDialogueElement(item) {
				  const dialogueContainer = document.createElement('div');
				  dialogueContainer.className = 'diag-dialogue';
				
				  const promptDiv = document.createElement('div');
				  promptDiv.className = 'diag-prompt';
				  promptDiv.textContent = item.prompt;
				  dialogueContainer.appendChild(promptDiv);
				
				  if (item.imageP && item.imageP !== '#') {
				    const promptImg = document.createElement('img');
				    promptImg.src = item.imageP;
				    promptImg.alt = "Prompt image";
				    promptDiv.appendChild(promptImg);
				  }
				
				  const responseContainer = document.createElement('div');
				  responseContainer.className = 'diag-response';
				
				  const responseTextDiv = document.createElement('div');
				  responseTextDiv.className = 'diag-response-text-area diag-typing-caret';
				  
				  let responseImgElement = null;
				  if (item.imageR && item.imageR !== '#') {
				    responseImgElement = document.createElement('img');
				    responseImgElement.src = item.imageR;
				    responseImgElement.alt = "Response image";
				    responseImgElement.style.display = 'none';
				  }
				
				  responseContainer.appendChild(responseTextDiv);
				  if (responseImgElement) {
				      responseContainer.appendChild(responseImgElement);
				  }
				  
				  dialogueContainer.appendChild(responseContainer);
				
				  const processedResponseText = parseOrderedList(item.response);
				
				  typeWriter(processedResponseText, responseTextDiv, () => {
				    responseTextDiv.classList.remove('diag-typing-caret');
				    if (responseImgElement) {
				      responseImgElement.style.display = 'block';
				    }
				  });
				
				  return dialogueContainer;
				}
				
				function renderExample(title) {
				  const sections = {
				    Gemini: document.querySelector('#Gemini .diag-content'), // Prefixed
				    Janus: document.querySelector('#Janus .diag-content'),  // Prefixed
				    Ours: document.querySelector('#Ours .diag-content')     // Prefixed
				  };
				
				  document.getElementById('Gemini').className = 'diag-section diag-default'; // Prefixed
				  document.querySelector('#Gemini .diag-section-header').className = 'diag-section-header diag-default'; // Prefixed
				  document.querySelector('#Gemini .diag-section-header span').textContent = 'Gemini ‚ñº';
				
				  document.getElementById('Janus').className = 'diag-section diag-default'; // Prefixed
				  document.querySelector('#Janus .diag-section-header').className = 'diag-section-header diag-default'; // Prefixed
				  document.querySelector('#Janus .diag-section-header span').textContent = 'Janus ‚ñº';
				
				  document.getElementById('Ours').className = 'diag-section diag-orange'; // Prefixed
				  document.querySelector('#Ours .diag-section-header').className = 'diag-section-header diag-orange'; // Prefixed
				  document.querySelector('#Ours .diag-section-header span').textContent = 'Ours ‚ñº';
				
				  Object.values(sections).forEach(section => section.innerHTML = '');
				
				  const filtered = data.filter(d => d.title === title);
				
				  ['Gemini', 'Janus', 'Ours'].forEach(model => {
				    const items = filtered.filter(item => item.model === model);
				    const modelSectionContent = sections[model];
				    if (modelSectionContent) {
				        if (items.length === 0) {
				            modelSectionContent.innerHTML = '<p><i>No data for this example.</i></p>';
				        } else {
				            items.forEach(item => {
				                const el = createDialogueElement(item);
				                modelSectionContent.appendChild(el);
				            });
				        }
				    }
				  });
				}
				
				document.querySelectorAll('.diag-section-header').forEach(header => { // Prefixed
				  header.addEventListener('click', () => {
				    const content = header.nextElementSibling;
				    const currentSpan = header.querySelector('span');
				    if (content.style.display === 'none' || content.style.display === '') {
				      content.style.display = 'block';
				      if(currentSpan) currentSpan.textContent = currentSpan.textContent.replace('‚ñ∂', '‚ñº');
				    } else {
				      content.style.display = 'none';
				      if(currentSpan) currentSpan.textContent = currentSpan.textContent.replace('‚ñº', '‚ñ∂');
				    }
				  });
				});
				
				exampleSelect.addEventListener('change', e => {
				  renderExample(e.target.value);
				});
				
				if (uniqueTitles.length > 0) {
				    exampleSelect.value = uniqueTitles[0];
				    renderExample(uniqueTitles[0]);
				}
				
			</script>
		</section>
		<!-- @PAN TODO: bibtex -->
		<section class="section" id="BibTeX">
			<div class="container is-max-desktop content">
				<h2 class="title is-3 has-text-centered">BibTeX</h2>
				<pre><code>@article{chen2025intermt,
  title={InterMT: Multi-Turn Interleaved Preference Alignment with Human Feedback},
  author={Boyuan Chen and Donghai Hong and Jiaming Ji and Jiacheng Zheng and Bowen Dong and Jiayi Zhou and Kaile Wang and Josef Dai and Xuyao Wang and Wenqi Chen and Qirui Zheng and Wenxin Li and Sirui Han and Yike Guo and Yaodong Yang},
  year={2025},
  institution={Peking University and Hong Kong University of Science and Technology},
  url={https://pku-intermt.github.io},
  keywords={Multimodal Learning, Multi-Turn Interaction, Human Feedback, Preference Alignment}
}</code></pre>
			</div>
		</section>
		<section>
			<div class="section" id="org-banners" style="display: flex; align-items: center;">
				<a href="https://www.pku.edu.cn/" target="_blank" rel="external" style="margin-right: 20px;">
					<img class="center-block org-banner" src="./assets/PKU_logo.png" style="width: 150px; height: 150px;">
				</a>
				<a href="https://hkust.edu.hk/" target="_blank" class="ext-link" style="margin-left: 20px;">
					<img class="center-block org-banner" src="./assets/HKUST-STD.svg" style="width: 400px; height: 200px;">
				</a>
			</div>
		</section>
		<footer class="footer">
			<!-- <div class="container"> -->
			<div class="content has-text-centered">
			</div>
			<div class="columns is-centered">
				<div class="column is-8">
					<div class="content">
						<p> This website is website adapted from <a href="https://nerfies.github.io/">Nerfies</a>, licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>. </p>
					</div>
				</div>
			</div>
			<!-- </div> -->
		</footer>
		</div>
	</section>
	</body>
</html>
