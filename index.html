<!DOCTYPE html>
<html class="fontawesome-i2svg-active fontawesome-i2svg-complete">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<style data-merge-styles="true"></style>
	<style data-merge-styles="true"></style>
	<style data-merge-styles="true"></style>
	<meta name="description" content="Multi-Turn Interleaved Preference Alignment with Human Feedback">
	<meta name="keywords" content="Multi-Turn, Multi-Modality">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>InterMT: Multi-Turn Interleaved Preference Alignment with Human Feedback</title>
	<link rel="icon" href="./assets/logo_transp.png" type="image/png">
	<!-- ÂºïÂÖ•cssÊñá‰ª∂ -->
	<link href="./assets/css" rel="stylesheet">
	<link rel="stylesheet" href="./assets/bulma.min.css">
	<link rel="stylesheet" href="./assets/bulma-carousel.min.css">
	<link rel="stylesheet" href="./assets/font-face.css">
	<link rel="stylesheet" href="./assets/bulma-slider.min.css">
	<link rel="stylesheet" href="./assets/fontawesome.all.min.css">
	<link rel="stylesheet" href="./assets/academicons.min.css">
	<link rel="stylesheet" href="./assets/index.css">
	<link rel="stylesheet" href="./assets/leaderboard.css">
	<style>
		p, ul {
		text-align: justify;
		margin-left: auto;
		margin-right: auto;
		width: 75%;
		list-style:disc;
		margin-bottom: 10px; /* ËÆæÁΩÆÊÆµËêΩ‰πãÈó¥ÁöÑÈó¥Ë∑ù */
		}
		li {
		margin-bottom: 10px;
		}
	</style>
	<style>
		.data-card {
		background-color: #f8f9fa;
		border: 1px solid #e1e5e9;
		border-radius: 12px;
		padding: 25px;
		margin: 25px 0;
		box-shadow: 0 4px 8px rgba(0,0,0,0.1);
		transition: transform 0.3s ease, box-shadow 0.3s ease;
		}
		.data-card:hover {
		transform: translateY(-5px);
		box-shadow: 0 8px 16px rgba(0,0,0,0.15);
		}
		.data-card h4 {
		margin-top: 0;
		color: #2c3e50;
		border-left: none;
		padding-left: 0;
		display: flex;
		align-items: center;
		}
		.data-card h4::before {
		content: 'üìä';
		margin-right: 10px;
		font-size: 1.2em;
		}
		.data-card ul {
		margin: 15px 0;
		padding-left: 20px;
		}
		.data-card li {
		margin-bottom: 12px;
		text-align: justify;
		position: relative;
		}
		.data-card li::marker {
		color: #1b2f3d;
		font-weight: bold;
		}
		.dimensions-table {
		margin: 25px 0;
		background-color: #f8f9fa;
		padding: 20px;
		border-radius: 12px;
		box-shadow: 0 4px 8px rgba(0,0,0,0.1);
		overflow: hidden;
		}
		.dimensions-table h4 {
		margin-top: 0;
		text-align: center;
		color: #2c3e50;
		border-left: none;
		padding-left: 0;
		}
		.dimensions-table table {
		width: 100%;
		border-collapse: collapse;
		background-color: white;
		border-radius: 8px;
		overflow: hidden;
		box-shadow: 0 2px 4px rgba(0,0,0,0.1);
		}
		.dimensions-table th {
		background: linear-gradient(135deg, #3498db, #2980b9);
		color: white;
		padding: 15px;
		text-align: left;
		font-weight: bold;
		border-bottom: 3px solid #2980b9;
		position: relative;
		}
		.dimensions-table th::after {
		content: '';
		position: absolute;
		top: 0;
		left: 0;
		right: 0;
		bottom: 0;
		background: rgba(255,255,255,0.1);
		opacity: 0;
		transition: opacity 0.3s ease;
		}
		.dimensions-table th:hover::after {
		opacity: 1;
		}
		.dimensions-table td {
		padding: 12px 15px;
		border-bottom: 1px solid #ecf0f1;
		color: #2c3e50;
		font-weight: 500;
		transition: background-color 0.3s ease;
		}
		.dimensions-table tr:nth-child(even) {
		background-color: #f8f9fa;
		}
		.dimensions-table tr:hover {
		background-color: #e8f4f8;
		transform: scale(1.02);
		box-shadow: 0 2px 8px rgba(52, 152, 219, 0.2);
		}
		.dimensions-table tr:last-child td {
		border-bottom: none;
		}
		.section-intro {
		background: linear-gradient(135deg, #e8f4f8, #dbeafe);
		padding: 25px;
		border-radius: 12px;
		margin-bottom: 30px;
		border-left: 6px solid #3498db;
		position: relative;
		overflow: hidden;
		}
		.section-intro::before {
		content: '';
		position: absolute;
		top: 0;
		left: 0;
		right: 0;
		bottom: 0;
		background: url('data:image/svg+xml,
		<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 100 100">
			<defs>
				<pattern id="grid" width="10" height="10" patternUnits="userSpaceOnUse">
					<path d="M 10 0 L 0 0 0 10" fill="none" stroke="%23ffffff" stroke-width="0.5" opacity="0.1"/>
				</pattern>
			</defs>
			<rect width="100" height="100" fill="url(%23grid)"/>
		</svg>
		');
		pointer-events: none;
		}
		.math-formula {
		background-color: #f8f9fa;
		padding: 15px;
		border-radius: 8px;
		border-left: 4px solid #3498db;
		margin: 20px 0;
		font-family: 'Courier New', monospace;
		}
		.warning {
		background-color: #fff3cd;
		border: 1px solid #ffeaa7;
		color: #856404;
		padding: 15px;
		border-radius: 8px;
		margin: 20px 0;
		border-left: 4px solid #f39c12;
		}
		.info {
		background-color: #d1ecf1;
		border: 1px solid #bee5eb;
		color: #0c5460;
		padding: 15px;
		border-radius: 8px;
		margin: 20px 0;
		border-left: 4px solid #17a2b8;
		}
		.success {
		background-color: #d4edda;
		border: 1px solid #c3e6cb;
		color: #155724;
		padding: 15px;
		border-radius: 8px;
		margin: 20px 0;
		border-left: 4px solid #28a745;
		}
		/* Âä†ËΩΩÂä®Áîª */
		.loading {
		display: inline-block;
		width: 20px;
		height: 20px;
		border: 3px solid rgba(52, 152, 219, 0.3);
		border-radius: 50%;
		border-top-color: #3498db;
		animation: spin 1s ease-in-out infinite;
		}
		@keyframes spin {
		to { transform: rotate(360deg); }
		}
		/* ÂìçÂ∫îÂºèËÆæËÆ° */
		@media (max-width: 768px) {
		body {
		padding: 15px;
		}
		.container {
		padding: 20px;
		}
		.dimensions-table table {
		font-size: 14px;
		}
		.dimensions-table th,
		.dimensions-table td {
		padding: 10px 8px;
		}
		.data-card {
		padding: 15px;
		}
		}
		@media (max-width: 480px) {
		.dimensions-table {
		overflow-x: auto;
		}
		.dimensions-table table {
		min-width: 500px;
		}
		}
		/* ÊâìÂç∞Ê†∑Âºè */
		@media print {
		body {
		background-color: white;
		padding: 0;
		}
		.container {
		box-shadow: none;
		padding: 20px;
		}
		.data-card,
		.dimensions-table {
		box-shadow: none;
		border: 1px solid #ccc;
		}
		.data-card:hover,
		.dimensions-table tr:hover {
		transform: none;
		box-shadow: none;
		}
		}
	</style>
	<style>
		/* Custom styles for the modal */
		.modal {
		display: none;
		position: fixed;
		z-index: 50;
		left: 0;
		top: 0;
		width: 100%;
		height: 100%;
		overflow: auto;
		background-color: rgba(0, 0, 0, 0.5);
		}
		.modal-content {
		background-color: white;
		margin: 5% auto;
		padding: 20px;
		border: 1px solid #888;
		width: 90%;
		max-width: 800px;
		border-radius: 8px;
		max-height: 80vh;
		overflow-y: auto;
		}
		.close {
		color: #aaa;
		float: right;
		font-size: 28px;
		font-weight: bold;
		cursor: pointer;
		}
		.close:hover {
		color: #000;
		}
		td {
		cursor: pointer;
		}
		td:hover {
		background-color: #e5e7eb;
		}
	</style>
	<!-- Ë°®Ê†ºÊ†∑ÂºècssÊñá‰ª∂ -->
	<style>
		/* Ë°®Ê†ºÂü∫Á°ÄÊ†∑Âºè */
		.data-table {
		width: 100%;
		border-collapse: collapse;
		margin: 20px 0;
		background-color: white;
		border: none;
		}
		/* Ë°®Ê†ºÂ∏ÉÂ±Ä */
		.data-table th,
		.data-table td {
		padding: 12px 15px;
		text-align: center;
		vertical-align: middle;
		border: none;
		}
		/* Ë°®Â§¥Ê†∑Âºè */
		.data-table thead {
		background-color: #2c3e50;
		}
		.data-table th {
		font-weight: bold;
		font-size: 14px;
		color: #ffffff;
		}
		/* Ë°®Ê†ºË°åÊ†∑Âºè */
		.data-table tbody tr:nth-child(even) {
		background-color: #f9f9f9;
		}
		.data-table tbody tr:hover {
		background-color: #f1f1f1;
		}
		/* ÂçïÂÖÉÊ†ºÂ≠ó‰ΩìÂ§ßÂ∞è */
		.data-table td {
		font-size: 13px;
		}
		/* ÁâπÊÆäÊ†∑ÂºèÁ±ª */
		.bold {
		font-weight: bold;
		}
		.highlight {
		color: #d02907;
		}
		/* Ë°®Ê†ºÂØπÈΩê */
		.center-table {
		margin-left: auto;
		margin-right: auto;
		}
	</style>
	<!-- ÂØπËØùcssÊñá‰ª∂ -->
	<style>
		.diag-dropdown-container {
		margin-bottom: 15px;
		padding: 10px;
		background-color: #fff;
		border-radius: 8px;
		box-shadow: 0 2px 4px rgba(0,0,0,0.1);
		}
		select {
		padding: 8px 12px;
		font-size: 16px;
		border-radius: 4px;
		border: 1px solid #ccc;
		}
		.diag-container {
		display: flex;
		gap: 15px;
		}
		.diag-section {
		flex: 1;
		border: 1px solid #ccc;
		border-radius: 8px;
		overflow: hidden;
		max-height: 85vh;
		display: flex;
		flex-direction: column;
		box-shadow: 0 2px 4px rgba(0,0,0,0.1);
		}
		.diag-section.diag-orange { background: #fff4e5; border-color: orange; }
		.diag-section.diag-default { background: #f9f9f9; border-color: #999; }
		.diag-section.diag-blue { background: #e7f3ff; border-color: #99c2ff; }
		.diag-section-header {
		padding: 12px 15px;
		font-weight: bold;
		cursor: pointer;
		user-select: none;
		background-color: #ddd;
		border-bottom: 1px solid #bbb;
		display: flex;
		align-items: center;
		}
		.diag-section-header svg, .diag-section-header img { /* SVGs/images within header */
		margin-right: 8px;
		width: 20px;
		height: 20px;
		vertical-align: middle;
		}
		.diag-section-header.diag-orange { background-color: #f7d399; border-color: orange; }
		.diag-section-header.diag-default { background-color: #e0e0e0; border-color: #ccc; }
		.diag-section-header.diag-blue { background-color: #b3d7ff; border-color: #99c2ff;}
		.diag-content {
		padding: 15px;
		overflow-y: auto;
		flex-grow: 1;
		background-color: #fff;
		}
		.diag-dialogue {
		margin-bottom: 20px;
		}
		.diag-prompt {
		text-align: right;
		background: #d0e6ff;
		padding: 10px 14px;
		border-radius: 15px 15px 0 15px;
		margin-bottom: 8px;
		max-width: 85%;
		margin-left: auto;
		font-weight: 600;
		word-wrap: break-word;
		}
		.diag-response {
		text-align: left;
		background: #eaeaea;
		padding: 10px 14px;
		border-radius: 15px 15px 15px 0;
		max-width: 85%;
		position: relative;
		white-space: normal;
		font-size: 14px;
		word-wrap: break-word;
		}
		.diag-response-text-area {
		white-space: pre-wrap;
		}
		.diag-prompt img, .diag-response img {
		max-width: 100%;
		margin-top: 10px;
		border-radius: 6px;
		display: block;
		}
		ol {
		padding-left: 25px;
		margin-top: 5px;
		margin-bottom: 5px;
		}
		ol li {
		margin-bottom: 5px;
		}
		p {
		margin-top: 5px;
		margin-bottom: 10px;
		}
		strong {
		font-weight: bold;
		}
		.diag-typing-caret::after {
		content: '|';
		animation: blink 1s step-end infinite;
		margin-left: 2px;
		font-weight: bold;
		}
		@keyframes blink {
		50% { opacity: 0; }
		}
	</style>
	<style>
		.modal {
		display: none;
		position: fixed;
		z-index: 1;
		left: 0;
		top: 0;
		width: 100%;
		height: 100%;
		overflow: auto;
		background-color: rgb(0,0,0);
		background-color: rgba(0,0,0,0.9);
		}
		.modal-content {
		margin: 15% auto;
		padding: 20px;
		width: 80%;
		max-width: 700px;
		}
		.close {
		color: #f1f1f1;
		float: right;
		font-size: 28px;
		font-weight: bold;
		}
		.close:hover,
		.close:focus {
		color: #000;
		text-decoration: none;
		cursor: pointer;
		}
	</style>
	<style>
		p, li, strong {
		text-align: left;
		}
	</style>
	<!-- ÂºïÂÖ•jsÊñá‰ª∂ -->
	<script>
		window.MathJax = {
		    tex: {
		        inlineMath: [['$', '$'], ['\\(', '\\)']],
		        displayMath: [['$$', '$$'], ['\\[', '\\]']],
		        packages: {'[+]': ['ams', 'newcommand', 'configmacros']}
		    },
		    options: {
		        processHtmlClass: 'tex2jax_process',
		        processScriptType: 'text/tex',
		        ignoreHtmlClass: 'tex2jax_ignore'
		    },
		    startup: {
		        ready: function () {
		            MathJax.startup.defaultReady();
		            console.log('MathJax is loaded and ready to go!');
		        }
		    }
		};
	</script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js"></script>
	<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
	<script type="text/javascript" src="./assets/sort-table.js" defer=""></script>
	<script src="./assets/jquery.min.js"></script>
	<script defer="" src="./assets/fontawesome.all.min.js"></script>
	<script src="./assets/bulma-carousel.min.js"></script>
	<script src="./assets/bulma-slider.min.js"></script>
	<script src="./assets/explorer-index.js"></script>
	<script src="./assets/question_card.js"></script>
	<script src="./assets/leaderboard_testmini.js"></script>
	<script src="./assets/output_folders.js" defer=""></script>
	<script src="./assets/model_scores.js" defer=""></script>
	<script src="./assets/data_public.js" defer=""></script>
</head>
<nav2 class="navbar" role="navigation" aria-label="main navigation">
	<div class="navbar-brand">
		<a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
			<span aria-hidden="true"></span>
			<span aria-hidden="true"></span>
			<span aria-hidden="true"></span>
		</a>
	</div>
	<div class="navbar-menu">
		<div class="navbar-start" style="flex-grow: 1; justify-content: center;">
			<!-- @PAN TODO: consider adding links? -->
			<div class="navbar-item has-dropdown is-hoverable">
				<a class="navbar-link">
					<span class="icon">
						<svg xmlns="http://www.w3.org/2000/svg" xml:space="preserve" width="512" height="501" shape-rendering="geometricPrecision" text-rendering="geometricPrecision" image-rendering="optimizeQuality" fill-rule="evenodd" clip-rule="evenodd" viewBox="0 0 512 500.59">
							<path fill-rule="nonzero"
								d="m107.22 313.44-23.58 55.2a7.338 7.338 0 0 1-6.57 4.47l-59.32 5.33 45.22 39.48c2.1 1.84 2.9 4.62 2.32 7.17l-13.36 58.52 51.51-30.8c2.4-1.44 5.3-1.34 7.54 0l51.53 30.8-5.87-25.66c4.88.55 9.91.19 14.8-1.15l6.68 29.31c.19 1.27.19 2.58-.02 3.89a12.8 12.8 0 0 1-1.62 4.47 12.559 12.559 0 0 1-7.71 5.73c-3.1.78-6.5.37-9.49-1.41l-52.07-31.13-51.62 30.88c-1.12.74-2.38 1.29-3.73 1.63l-.94.23c-1.45.26-2.99.26-4.55-.04l-.5-.09c-3.34-.78-6.08-2.85-7.78-5.56a12.573 12.573 0 0 1-1.67-9.24l13.57-59.39-45.68-39.89a12.566 12.566 0 0 1-2.94-3.78 12.6 12.6 0 0 1-1.29-4.2l-.05-.65c-.22-3.31.91-6.45 2.94-8.87 2.01-2.39 4.9-4.05 8.2-4.42l60.67-5.41 23.78-55.71c.62-1.47 1.54-2.85 2.7-4.02.94-.95 2.04-1.76 3.3-2.39.44-.24.91-.44 1.4-.6 2.96-1.06 6.12-.91 8.87.19 2.75 1.11 5.12 3.19 6.54 6.01l18.82 44.01-5.47 23.93c-.4-.5-.72-1.05-.97-1.64l-23.59-55.2zm137.19-302.2c0-6.2 5.03-11.24 11.24-11.24 6.2 0 11.24 5.04 11.24 11.24V62c0 6.2-5.04 11.24-11.24 11.24-6.21 0-11.24-5.04-11.24-11.24V11.24zM40.94 151.78c-5.7-2.4-8.38-8.96-5.98-14.66 2.4-5.7 8.97-8.38 14.67-5.98l46.73 19.8c5.7 2.4 8.38 8.97 5.98 14.66-2.4 5.7-8.97 8.38-14.67 5.98l-46.73-19.8zm47.52-72.84c-4.36-4.39-4.34-11.49.05-15.85 4.39-4.36 11.49-4.34 15.85.05l35.7 36.08c4.36 4.39 4.34 11.49-.05 15.85-4.39 4.36-11.49 4.34-15.85-.05l-35.7-36.08zm71.65-49.34c-2.35-5.72.38-12.27 6.1-14.62 5.73-2.35 12.27.38 14.63 6.1l19.3 46.95c2.35 5.72-.39 12.27-6.11 14.62-5.72 2.35-12.27-.38-14.62-6.1l-19.3-46.95zm302.26 101.54c5.7-2.4 12.27.28 14.67 5.98 2.4 5.7-.28 12.26-5.98 14.66l-46.73 19.8c-5.7 2.4-12.27-.28-14.67-5.98-2.4-5.69.28-12.26 5.98-14.66l46.73-19.8zm-54.73-68c4.36-4.39 11.46-4.41 15.85-.05 4.39 4.36 4.41 11.46.05 15.85l-35.7 36.08c-4.36 4.39-11.46 4.41-15.85.05-4.39-4.36-4.41-11.46-.05-15.85l35.7-36.08zm-76.48-42.06c2.36-5.72 8.9-8.45 14.63-6.1 5.72 2.35 8.45 8.9 6.1 14.62l-19.3 46.95c-2.35 5.72-8.9 8.45-14.62 6.1-5.72-2.35-8.46-8.9-6.11-14.62l19.3-46.95zm-53.09 110.8 33.5 78.45 83.93 7.53c.77 0 1.53.06 2.27.16 6.17.76 11.54 3.87 15.3 8.34l1.22 1.67c2.97 4.22 4.6 9.39 4.32 14.82l-.2 2.03c-.37 2.69-1.19 5.23-2.36 7.54l-.93 1.6c-.84 1.36-1.81 2.62-2.89 3.79-.82.97-1.76 1.85-2.79 2.6l-63.56 55.5 19.33 84.72c1.09 6.02-.21 11.99-3.28 16.87a23.685 23.685 0 0 1-14.82 10.51c-3.12.74-6.32.78-9.41.23l-.01.06c-3.19-.57-6.23-1.79-8.91-3.54L256 381.22l-74.01 44.25a23.716 23.716 0 0 1-17.57 2.45c-5.92-1.49-11.25-5.23-14.61-10.81l-.9-1.71a23.7 23.7 0 0 1-2.18-6.9c-.48-3.03-.37-6.08.29-8.96l19.1-83.63-64.6-56.4a23.796 23.796 0 0 1-8.09-16.36c-.39-6 1.52-12.21 5.85-17.16a23.87 23.87 0 0 1 7.47-5.73c2.81-1.4 5.93-2.19 9.12-2.37l84.56-7.56 33.69-78.88a23.734 23.734 0 0 1 13.02-12.75c12.18-4.91 25.86 1.03 30.83 12.93l.1.25zM416.44 303.3l23.72 55.56 60.67 5.41c3.3.37 6.19 2.03 8.2 4.42 2.03 2.42 3.16 5.56 2.94 8.87l-.05.65a12.6 12.6 0 0 1-1.29 4.2c-.72 1.43-1.73 2.72-2.94 3.78l-45.68 39.89 13.57 59.39c.68 3.29 0 6.58-1.67 9.24-1.7 2.71-4.44 4.78-7.78 5.56l-.5.09c-1.56.3-3.1.3-4.55.04l-.94-.23c-1.35-.34-2.61-.89-3.73-1.63l-51.63-30.88-52.06 31.13a12.513 12.513 0 0 1-9.49 1.41c-3.1-.78-5.93-2.76-7.71-5.73-.84-1.41-1.38-2.93-1.63-4.45l-.01-.08c-.21-1.25-.2-2.54 0-3.81l6.69-29.36c1.09.3 2.21.55 3.33.75 3.75.68 7.62.84 11.46.41l-5.87 25.68 51.52-30.8c2.25-1.34 5.14-1.44 7.54 0l51.52 30.8-13.37-58.52c-.57-2.55.23-5.33 2.33-7.17l45.22-39.48-59.32-5.33a7.338 7.338 0 0 1-6.57-4.47l-23.58-55.2-23.57 55.2c-.26.61-.61 1.18-1.02 1.69l-5.46-23.91 18.84-44.08c1.42-2.82 3.79-4.9 6.54-6.01 2.75-1.1 5.91-1.25 8.87-.19.49.16.96.36 1.4.6 1.26.63 2.36 1.44 3.3 2.39 1.16 1.17 2.08 2.55 2.7 4.02l.06.15z" />
							<path fill="#F7D345" fill-rule="nonzero"
								d="m263.04 137.82 37.45 87.7 95.01 8.52c4.19.37 7.29 4.06 6.93 8.25a7.59 7.59 0 0 1-2.57 5.07v.01l-71.87 62.75 21.26 93.02c.94 4.11-1.63 8.2-5.74 9.15-2.1.47-4.2.04-5.88-1.06L256 362.43l-81.89 48.96a7.627 7.627 0 0 1-10.46-2.64 7.602 7.602 0 0 1-.89-5.61h-.01l21.26-93.02-71.87-62.75c-3.17-2.77-3.49-7.59-.72-10.76a7.621 7.621 0 0 1 5.33-2.59l94.75-8.5 37.48-87.75c1.65-3.88 6.13-5.68 10.01-4.03 1.91.81 3.31 2.31 4.05 4.08z" />
						</svg>
					</span> &nbsp;&nbsp;More Research </a>
				<div class="navbar-dropdown">
					<a class="navbar-item" href="https://github.com/PKU-Alignment/safe-rlhf">
						<b>SafeRLHF (ICLR 2024 Spotlight)</b>
					</a>
					<a class="navbar-item" href="https://huggingface.co/datasets/PKU-Alignment/PKU-SafeRLHF">
						<b>PKU-SafeRLHF (ACL2025 Main)</b>
					</a>
					<a class="navbar-item" href="https://github.com/PKU-Alignment/aligner">
						<b>Aligner (NeurIPS 2024 Oral)</b>
					</a>
					<a class="navbar-item" href="https://github.com/PKU-Alignment/safe-sora">
						<b>SafeSora (NeurIPS 2024 DB Track)</b>
					</a>
					<a class="navbar-item" href="https://github.com/PKU-Alignment/beavertails">
						<b>BeaverTails (NeurIPS 2024 DB Track)</b>
					</a>
					<a class="navbar-item" href="https://align-anything.readthedocs.io/en/latest/index.html">
						<b>Align-Anythingüî•üî•üî•</b>
					</a>
				</div>
			</div>
		</div>
	</div>
</nav2>
<section class="hero" style="background-color: #1f5ceb;">
	<div class="hero-body">
		<div class="container is-max-desktop">
			<div class="columns is-centered">
				<div class="column has-text-centered">
					<h1 class="title is-1 publication-title" style="color: white;">
						<span class="img">
							<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
						</span>InterMT: Multi-Turn Interleaved Preference Alignment with Human Feedback
					</h1>
					<div class="is-size-5 publication-authors">
						<div class="is-size-5 publication-authors">
							<span class="author-block" style="color: white;"> Boyuan Chen<sup>1*</sup>Ôºå </span>
							<span class="author-block" style="color: white;"> Donghai Hong<sup>1*</sup>Ôºå </span>
							<span class="author-block" style="color: white;"> Jiaming Ji<sup>1*</sup>Ôºå </span>
							<span class="author-block" style="color: white;"> Jiacheng Zheng<sup>2</sup>Ôºå </span>
							<span class="author-block" style="color: white;"> Bowen Dong<sup>1</sup>Ôºå </span>
							<span class="author-block" style="color: white;"> Jiayi Zhou<sup>1</sup>Ôºå </span>
							<span class="author-block" style="color: white;"> Kaile Wang<sup>1</sup>Ôºå </span>
							<span class="author-block" style="color: white;"> Josef Dai<sup>1</sup>Ôºå </span>
							<span class="author-block" style="color: white;"> Xuyao Wang<sup>1</sup>Ôºå </span>
							<span class="author-block" style="color: white;"> Wenqi Chen<sup>1</sup>Ôºå </span>
							<span class="author-block" style="color: white;"> Qirui Zheng<sup>1</sup>Ôºå </span>
							<span class="author-block" style="color: white;"> Wenxin Li<sup>1</sup>Ôºå </span>
							<span class="author-block" style="color: white;"> Sirui Han<sup>2</sup>Ôºå </span>
							<span class="author-block" style="color: white;"> Yike Guo<sup>2</sup>Ôºå </span>
							<span class="author-block" style="color: white;"> Yaodong Yang<sup>1‚Ä†</sup>Ôºå </span>
						</div>
						<div class="is-size-5 publication-authors">
							<span class="author-block" style="color: white;">
								<sup>1</sup>
								<em>Peking University</em>
							</span>
							<span class="author-block" style="color: white;">
								<sup>2</sup>
								<em>The Hong Kong University of Science and Technology</em>
							</span>
						</div>
						<p style="text-align: center; color: white;">
							<small> * Equal contribution, <sup>‚Ä†</sup> corresponding author. </small>
						</p>
						<br>
					</div>
					<div class="column has-text-centered">
						<div class="publication-links">
							<!-- PDF Link. -->
							<span class="link-block">
								<a href="#" class="external-link button is-normal is-rounded is-dark">
									<span class="icon">
										<svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg="">
											<path fill="currentColor"
												d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z">
											</path>
										</svg>
										<!-- <i class="fas fa-file-pdf"></i> Font Awesome fontawesome.com -->
									</span>
									<span>Paper</span>
								</a>
							</span>
							<span class="link-block">
								<a href="https://huggingface.co/datasets/PKU-Alignment/INTERMT" class="external-link button is-normal is-rounded is-dark">
									<span class="icon">
										<img src="./assets/huggingface-color.svg" alt="HuggingFace" style="width: 1em; height: 1em;" />
									</span>
									<span>InterMT</span>
								</a>
							</span>
							<!-- Video Link. -->
							<span class="link-block">
								<a href="https://github.com/cby-pku/INTERMT" class="external-link button is-normal is-rounded is-dark">
									<span class="icon">
										<svg class="svg-inline--fa fa-github fa-w-16" aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" data-fa-i2svg="">
											<path fill="currentColor"
												d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z">
											</path>
										</svg>
										<!-- <i class="fab fa-github"></i> Font Awesome fontawesome.com -->
									</span>
									<span>InterMT-Bench</span>
								</a>
							</span>
							<!-- Code Link. -->
							<span class="link-block">
								<a href="https://huggingface.co/PKU-Alignment/INTERMT-Judge" class="external-link button is-normal is-rounded is-dark">
									<span class="icon">
										<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512">
											<!--!Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2025 Fonticons, Inc.-->
											<path fill="#ffffff"
												d="M0 96C0 60.7 28.7 32 64 32l320 0c35.3 0 64 28.7 64 64l0 320c0 35.3-28.7 64-64 64L64 480c-35.3 0-64-28.7-64-64L0 96zm144 4c-24.3 0-44 19.7-44 44l0 48c0 24.3 19.7 44 44 44l32 0c24.3 0 44-19.7 44-44l0-48c0-24.3-19.7-44-44-44l-32 0zm-4 44c0-2.2 1.8-4 4-4l32 0c2.2 0 4 1.8 4 4l0 48c0 2.2-1.8 4-4 4l-32 0c-2.2 0-4-1.8-4-4l0-48zm140-44c-11 0-20 9-20 20c0 9.7 6.9 17.7 16 19.6l0 76.4c0 11 9 20 20 20s20-9 20-20l0-96c0-11-9-20-20-20l-16 0zM132 296c0 9.7 6.9 17.7 16 19.6l0 76.4c0 11 9 20 20 20s20-9 20-20l0-96c0-11-9-20-20-20l-16 0c-11 0-20 9-20 20zm96 24l0 48c0 24.3 19.7 44 44 44l32 0c24.3 0 44-19.7 44-44l0-48c0-24.3-19.7-44-44-44l-32 0c-24.3 0-44 19.7-44 44zm44-4l32 0c2.2 0 4 1.8 4 4l0 48c0 2.2-1.8 4-4 4l-32 0c-2.2 0-4-1.8-4-4l0-48c0-2.2 1.8-4 4-4z" />
										</svg>
									</span>
									<span>Model</span>
								</a>
							</span>
						</div>
					</div>
				</div>
			</div>
		</div>
	</div>
</section>
<br>
<section class="hero teaser">
	<div class="container is-max-desktop">
		<div class="content has-text-centered">
			<img src="./assets/figure1-0516_00.jpg" alt="geometric reasoning" width="100%" style="box-shadow: 0 4px 10px rgba(0, 0, 0, 0.3); border-radius: 8px;">
			<h2 style="text-align: center;">Abstract</h2>
			<p> As multimodal large models (MLLMs) continue to advance across challenging tasks, a key question emerges: <strong><em>What essential capabilities are still missing?</em></strong> A critical aspect of human learning is continuous interaction with the environment -- not limited to
				language, but also involving multimodal understanding and generation. To move closer to human-level intelligence, models must similarly support <strong>multi-turn</strong>, <strong>multimodal interaction</strong>. In particular, they should comprehend interleaved multimodal contexts
				and respond coherently in ongoing exchanges. In this work, we present <strong>an initial exploration</strong> through the <span class="icon">
					<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
				</span><strong>InterMT</strong> -- <strong>the first preference dataset for <em>multi-turn</em> multimodal interaction</strong>, grounded in real human feedback. In this exploration, we particularly emphasize the importance of human oversight, introducing expert annotations to guide
				the process, motivated by the fact that current MLLMs lack such complex interactive capabilities. <span class="icon">
					<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
				</span><strong>InterMT</strong> captures human preferences at both global and local levels into nine sub-dimensions, consists of 15.6k prompts, 52.6k multi-turn dialogue instances, and 32.4k human-labeled preference pairs. To compensate for the lack of capability for multi-modal
				understanding and generation, we introduce an agentic workflow that leverages tool-augmented MLLMs to construct multi-turn QA instances. To further this goal, we introduce <span class="icon">
					<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
				</span><strong>InterMT-Bench</strong> to assess the ability of MLLMs in assisting judges with multi-turn, multimodal tasks. We demonstrate the utility of <span class="icon">
					<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
				</span><strong>InterMT</strong> through applications such as judge moderation and further reveal the <em>multi-turn scaling law</em> of judge model. We hope the open-source of our data can help facilitate further research on aligning current MLLMs to the next step. </p>
		</div>
	</div>
</section>
<br>
<br>
<section class="hero is-light is-small">
	<div class="hero-body has-text-centered">
		<!-- <h1 class="title is-1 mathvista"><img src="static/images/mathvista.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>MathVista Dataset</h1> -->
		<h1 class="title is-1 mathvista">
			<!--<img src="static/images/mathvista.png" style="width:1em;vertical-align: middle" alt="Logo"/>-->
			<span class="mathvista" style="vertical-align: middle" id="introduction">Introduction</span>
		</h1>
	</div>
</section>
<section class="section">
	<div class="container" style="margin-bottom: 2vh;">
		<div class="columns is-centered has-text-centered">
			<div class="column is-four-fifths">
				<p>Humans perceive the world through dynamic, multimodal interactions involving text, images, audio, video, and more. Building on the success of multimodal large language models (MLLMs), recent efforts aim to develop general-purpose AI assistants that handle multiple mixed
					modalities. A key feature of such general-purpose assistants is to engage in natural multi-turn conversations, perceive and generate any modality, to enable more smooth interaction and grounded understanding. </p>
				<p>Recent years have seen community efforts in transplanting alignment techniques, such as Reinforcement Learning from Human Feedback (RLHF), from the text modality to multiple modalities settings. Within this line of research, most studies focus exclusively on either understanding
					or generation. The lack of alignment considerations for multimodal mixed input-output settings exacerbates the imbalance across modalities, i.e., modality disequilibrium. Furthermore, existing methods primarily focus on single-turn interactions, where an LLM generates a response
					from a prompt and receives immediate alignment feedback. However, real-world interactions typically occur in long-horizon conversations and often feature interleaved multimodal inputs and outputs. </p>
				<blockquote style="border:1px solid #ccc; padding:10px; background:#f9f9f9; margin:20px 0; text-align:center; font-style:italic; font-size:regular; "> How to improve <strong style="color:#8B0000;">multi-turn</strong> interleaved <strong
						style="color:#00008B;">understanding</strong>-<strong style="color:#006400;">generation</strong> alignment via human feedback? </blockquote>
				<p>
					<strong>
						<u style="color:#C6011F"> Our reflections highlight several key issues in the alignment of MLLMs: <svg class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="currentColor" viewBox="0 0 24 24">
								<path
									d="M13.849 4.22c-.684-1.626-3.014-1.626-3.698 0L8.397 8.387l-4.552.361c-1.775.14-2.495 2.331-1.142 3.477l3.468 2.937-1.06 4.392c-.413 1.713 1.472 3.067 2.992 2.149L12 19.35l3.897 2.354c1.52.918 3.405-.436 2.992-2.15l-1.06-4.39 3.468-2.938c1.353-1.146.633-3.336-1.142-3.477l-4.552-.36-1.754-4.17Z" />
							</svg>
						</u>
					</strong>
				</p>
				<ul>
					<li><strong>Modality Fusion via Harmonizing Understanding and Generation.</strong> To build general-purpose AI assistants, high-fidelity perception and understanding alone are not sufficient. The system should also support the selective generation of multimodal outputs to
						effectively communicate, instruct, or interact with users in a natural and contextually appropriate manner. </li>
					<li><strong>Modeling Long‚ÄêHorizon, Interleaved Multimodal Interactions.</strong> Real‚Äêworld user‚ÄìAI exchanges typically span many turns and interleave text, vision, and other modalities. Such interactions demand not only precise instruction following but also sustained attention
						and reasoning over an evolving context, approaching near‚Äêhuman in‚Äêcontext reasoning capabilities. </li>
					<li><strong>Dynamic Human‚Äêin‚Äêthe‚ÄêLoop Alignment.</strong> In extended, multimodal interactions, user preferences continually evolve. Capturing and aligning with these emergent, dynamic preferences calls for genuine, iterative human feedback throughout the interaction.</li>
				</ul>
				<p>In response, we introduce <span class="icon">
						<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
					</span>InterMT, a human preference dataset designed to capture the complexity and diversity of human intent in multi-turn settings. Specifically, <span class="icon">
						<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
					</span>InterMT targets vision-language interaction scenarios involving interleaved understanding and generation. To model dynamic human preferences, <span class="icon">
						<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
					</span>InterMT comprises <strong>
						<font color="red">15604</font>
					</strong> seed questions that elicit multi-turn, multimodal conversations spanning <strong>
						<font color="red">15+</font>
					</strong> domains. Helpfulness is then decomposed into <strong>
						<font color="red">9</font>
					</strong> sub-dimensions, capturing both global (conversation-level) and local (turn-level) aspects of human feedback. </p>
				<p>
					<strong>
						<u style="color:#C6011F"> Our key contributions are summarized as follows: <svg class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="currentColor" viewBox="0 0 24 24">
								<path
									d="M13.09 3.294c1.924.95 3.422 1.69 5.472.692a1 1 0 0 1 1.438.9v9.54a1 1 0 0 1-.562.9c-2.981 1.45-5.382.24-7.25-.701a38.739 38.739 0 0 0-.622-.31c-1.033-.497-1.887-.812-2.756-.77-.76.036-1.672.357-2.81 1.396V21a1 1 0 1 1-2 0V4.971a1 1 0 0 1 .297-.71c1.522-1.506 2.967-2.185 4.417-2.255 1.407-.068 2.653.453 3.72.967.225.108.443.216.655.32Z" />
							</svg>
						</u>
					</strong>
				</p>
				<ul>
					<li><strong>The First Multi-turn Interleaved Preference Dataset:</strong> To the best of our knowledge, <span class="icon">
							<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
						</span>InterMT is the first dataset that captures real human preferences for tasks involving multi-turn and interleaved multimodal understanding and generation. It contains <strong>
							<font color="red">15604</font>
						</strong> unique seed questions across diverse categories, <strong>
							<font color="red">52.6k</font>
						</strong> multi-turn interleaved vision-language QA instances, and <strong>
							<font color="red">32459</font>
						</strong> sets of multi-dimensional human preference annotations. </li>
					<li><strong>Agent-based Construction Workflow:</strong> <span class="icon">
							<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
						</span>InterMT employs a carefully designed agent-based multi-turn QA construction workflow that leverages strong MLLMs augmented with external tools to simulate high-quality real multi-turn interactions. </li>
					<li><strong>Decoupled Helpfulness in Multi-turn Multimodal Scenarios:</strong> <span class="icon">
							<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
						</span>InterMT decomposes the concept of helpfulness for multi-turn, multimodal interactions into two distinct levels: local (turn-level) and global (conversation-level). Furthermore, <span class="icon">
							<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
						</span>InterMT breaks down helpfulness into <strong>
							<font color="red">9</font>
						</strong> specific dimensions, allowing for a detailed and nuanced evaluation of multi-turn, multi-modal interactions. </li>
					<li><strong>Effective for Multi-turn Alignment:</strong> Building on <span class="icon">
							<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
						</span>InterMT, we investigate methods to model long-horizon values and align dynamic human values. Our findings reveal the phenomenon of preference transfer in multi-turn multimodal interactions, which facilitates preference modeling for predicting human judgments.
						Additionally, we identify a scaling phenomenon in multi-turn multimodal judge moderation. </li>
					<li><strong>One More Thing</strong> We introduce InterMT-Bench to evaluate the ability of MLLMs in assisting judges across multi-turn, multimodal tasks, encompassing three parts: Scoring Evaluation, Pair Comparison, and Crucial Step Recognition. Despite strong reasoning
						capabilities, advanced MLLMs fail to align with human values in judgment tasks. However, they show potential in identifying crucial steps in long-context scenarios. </li>
				</ul>
			</div>
		</div>
	</div>
</section>
<!-- DATASET SECTION -->
<section class="hero is-light is-small">
	<div class="hero-body has-text-centered">
		<!-- <h1 class="title is-1 mathvista"><img src="static/images/mathvista.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>MathVista Dataset</h1> -->
		<h1 class="title is-1 mathvista">
			<!--<img src="static/images/mathvista.png" style="width:1em;vertical-align: middle" alt="Logo"/>-->
			<span class="mathvista" style="vertical-align: middle" id="Dataset"><span class="img">
					<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
				</span>InterMT Dataset</span>
		</h1>
	</div>
</section>
<section class="section">
	<center>
		<h2 class="title is-3">Data Overview</h2>
	</center>
	<div class="columns is-centered has-text-centered">
		<div class="column is-four-fifths">
			<div class="content has-text-justified">
				<div class="carousel results-carousel">
					<div class="carousel-item">
						<div class="box m-5">
							<div class="content has-text-centered">
								<img src="./assets/Figure2.png" style="width: 60%;" onclick="openFullscreenImage(this.src)">
								<p style="margin-bottom: 30px;">
									<span class="icon">
										<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
									</span><b>InterMT</b> includes over 15 tasks in vision-language scenarios, capturing communication examples across diverse settings. These examples demonstrate <span style="color: #8B0000;">multi-turn</span>, interleaved <span
										style="color: #00008B;">understanding</span> and <span style="color: #006400;">generation</span> in six representative domains.
								</p>
							</div>
						</div>
					</div>
					<div class="carousel-item">
						<div class="box m-5">
							<div class="content has-text-centered">
								<img src="./assets/Figure3-0515_00.png" style="width: 60%;" onclick="openFullscreenImage(this.src)">
								<p style="margin-bottom: 30px;">
								<p> Overview of the four‚Äêstage pipeline for constructing<span class="icon">
										<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
									</span><b>InterMT</b>.<br>
									<strong style="font-weight: bold;">Stage I:</strong> seed questions are harvested from open‚Äêsource corpora, websites, and human writing, then filtered for perceptual quality, diversity, and multi‚Äêturn potential. <strong style="font-weight: bold;">Stage
										II:</strong> iterative calls to large models and external tools (<em>e.g.</em> retrieve, generate) produce answer expansions and follow-up questions, forming a candidate QA tree. <strong style="font-weight: bold;">Stage III:</strong> human annotators perform
									per‚Äêturn (local) and conversation-level (global) evaluations‚Äîcovering quality, coherence, context awareness, and completeness‚Äîto prune and select preferred branches. <strong style="font-weight: bold;">Stage IV:</strong> the retained branches are reassembled into
									deep, coherent QA trees (depth &ge; 5) yielding the final multi‚Äêturn QA pairs for model training.
								</p>
							</div>
						</div>
					</div>
				</div>
			</div>
			<div class="container">
				<div class="container" style="margin-bottom: 2vh;">
					<div class="columns is-centered has-text-centered">
						<div class="column is-four-fifths">
							<p><strong>Dataset Composition</strong></p>
							<P>The dataset includes: <strong>(1)</strong> carefully crafted seed questions for multi-turn, multimodal conversations, and <strong>(2)</strong> fine-grained human preference annotations at both local and global conversation levels. The seed questions are rigorously
								selected and refined to enable more faithful simulation of real-world tasks. </p>
							<P> We collect preference data through score evaluations and pairwise comparisons of multi-modal responses at each conversation turn, based on four sub-dimensions. Global conversation helpfulness is then evaluated via five sub-dimensions. Incorporating natural language
								feedback further improves annotation quality and alignment with human intent.</p1>
							<p><strong>Data Card</strong></p>
							<ul>
								<li>The dataset is built from a corpus of <span class="highlight">100k</span> image-text examples, comprising <span class="highlight">72.1%</span> from open-source vision-language datasets, <span class="highlight">22.8%</span> from web data, and <span
										class="highlight">5.1%</span> from human-written content. All prompts are refined following constitutional guidelines to improve multi-turn compatibility, resulting in <span class="highlight">15604</span> unique seed questions.</li>
								<li>Each seed question is expanded via an agent-based multi-turn QA construction workflow, producing at least <span class="highlight">8</span> multi-turn QA instances per prompt. After pruning and filtering, we obtain <span class="highlight">52.6k</span> high-quality
									multi-turn QA instances, with <span class="highlight">41.92%</span> containing five or more turns. </li>
								<li>The resulting <span class="highlight">52.6k</span> QA instances cover <span class="highlight">15+</span> vision-language <span class="highlight">understanding</span> and <span class="highlight">generation</span> tasks, such as image editing and visual tutorials.
									Each instance features interleaved textual and visual content in both inputs and outputs, with an average of <span class="highlight">5.33</span> images per conversation. </li>
								<li>The dataset features <span class="highlight">32,459</span> human preference annotations, organized as score evaluation pairwise comparisons at both the local and global levels. Preferences are decomposed into <span class="highlight">9</span> dimensions of
									helpfulness, accompanied by human-written critiques, refinement suggestions, and rationales. </li>
							</ul>
							<p style="font-size: 1.5em; text-align: center;"><strong>Human Annotation</strong></p>
							<P> Defining high-quality multi-turn multimodal dialogues is inherently challenging, as it requires assessing response correctness, the coherence of image-text interleaving, and the dynamic nature of human preferences throughout the conversation. We conduct multiple
								rounds of in-depth discussions with our annotation team regarding existing open-source datasets and prior work on MLLMs.</p>
							<P> We then identify the following <span class="highlight">9</span> annotation dimensions:<em><u>(click table elements for detailed definition.)</u></em></p>
							<div class="dimensions-table">
								<table class="w-full border-collapse bg-white shadow-md rounded-lg">
									<thead>
										<tr class="bg-gray-800 text-white">
											<th class="p-4 text-left">Global Dimensions</th>
											<th class="p-4 text-left">Local Dimensions</th>
										</tr>
									</thead>
									<tbody>
										<tr>
											<td class="p-4 border-t" data-dimension="G1">G1: Context Awareness</td>
											<td class="p-4 border-t" data-dimension="L1">L1: Local Image-Text Consistency</td>
										</tr>
										<tr>
											<td class="p-4 border-t" data-dimension="G2">G2: Helpfulness and Completeness</td>
											<td class="p-4 border-t" data-dimension="L2">L2: Visual Perceptual Quality</td>
										</tr>
										<tr>
											<td class="p-4 border-t" data-dimension="G3">G3: Crucial Step Recognition</td>
											<td class="p-4 border-t" data-dimension="L3">L3: Contextual Coherence</td>
										</tr>
										<tr>
											<td class="p-4 border-t" data-dimension="G4">G4: Global Image-Text Consistency</td>
											<td class="p-4 border-t" data-dimension="L4">L4: Text Quality</td>
										</tr>
										<tr>
											<td class="p-4 border-t" data-dimension="G5">G5: Style Coherence</td>
											<td class="p-4 border-t"></td>
										</tr>
									</tbody>
								</table>
							</div>
							<!-- Modal for displaying definitions -->
							<div id="definitionModal" class="modal">
								<div class="modal-content">
									<span class="close">&times;</span>
									<div id="definitionContent" class="prose"></div>
								</div>
							</div>
							<script>
								// Definitions object
								const definitions = {
								    G1: {
								        title: "Context Awareness",
								        definition: "The model should retain and understand the dialogue history to ensure contextual coherence, rather than treating each turn as an isolated interaction.",
								        examples: [
								            "In <i>visual storytelling</i> tasks, the model should maintain consistent characters, settings, and plot lines.",
								            "In <i>design revision</i> tasks, the model should remember the user's previous requests and avoid repeating suggestions that were previously rejected."
								        ],
								        scoring: [
								            "<b>0 points</b>: The model completely ignores the context; responses are irrelevant or contradict the dialogue history.",
								            "<b>1 point</b>: The model partially recalls context but exhibits noticeable information loss or inconsistency in roles.",
								            "<b>2 points</b>: Context is mostly preserved, with occasional minor inconsistencies.",
								            "<b>3 points</b>: Full understanding of context; responses are logically coherent, with no information loss or contradictions."
								        ]
								    },
								    G2: {
								        title: "Helpfulness and Completeness",
								        definition: "Measures how well the model‚Äôs textual and visual outputs follow task instructions and provide complete information to fulfill the user‚Äôs request. This also includes the logical structure of the response. In multi-turn image-text interactions, the model should accurately follow all instructions and ultimately deliver a complete solution.",
								        examples: [
								            "<b>Task</b>: Cake design improvement",
								            "&nbsp;&nbsp;- <i>User</i>: Please help me improve this design (uploads image)",
								            "&nbsp;&nbsp;- <i>AI</i>: Suggests adding frosting (but no new image generated) ‚Üí Deduct points",
								            "&nbsp;&nbsp;- <i>User</i>: Please show me the modified 3D rendering",
								            "&nbsp;&nbsp;- <i>AI</i>: [Generates an image of the cake with frosting] ‚Üí Full score"
								        ],
								        scoring: [
								            "<b>0 points</b>: The model fails to meet the user‚Äôs needs; responses are irrelevant or severely incorrect, making the task unachievable.",
								            "<b>1 point</b>: Partially satisfies the user‚Äôs request but lacks critical content or contains major errors that hinder task completion.",
								            "<b>2 points</b>: Largely completes the task but has minor omissions or inaccuracies that affect the final outcome.",
								            "<b>3 points</b>: Fully and accurately fulfills all user requirements; information is comprehensive, logically structured, and free from errors or omissions."
								        ]
								    },
								    G3: {
								        title: "Crucial Step Recognition",
								        definition: "In multi-turn interactions, the model must accurately identify and complete crucial steps, avoiding irrelevant or incorrect information.",
								        examples: [
								            "<b>Task</b>: Step-by-step guidance for drawing a cat",
								            "&nbsp;&nbsp;- <b>Crucial steps</b>: Sketch outline ‚Üí Refine facial features ‚Üí Adjust proportions ‚Üí Apply color",
								            "&nbsp;&nbsp;- <i>Incorrect</i>: Model asks the user to color before the outline is drawn",
								            "&nbsp;&nbsp;- <i>Correct</i>: Model guides the user through steps in a logical order"
								        ],
								        scoring: [
								            "<b>0 points</b>: Key steps are entirely incorrect or omitted, preventing task completion.",
								            "<b>1 point</b>: Some steps are inaccurate, though the task may still proceed with effort.",
								            "<b>2 points</b>: Overall step sequence is reasonable, with minor deviations or logical flaws.",
								            "<b>3 points</b>: All crucial steps are correctly identified and ordered, with no redundancy or omissions."
								        ]
								    },
								    G4: {
								        title: "Global Image-Text Consistency",
								        definition: "In multi-turn image-text interactions, textual descriptions should align closely with the generated images. Inconsistencies between text and images, or failing to generate images when required, result in lower scores.",
								        examples: [
								            "<b>Task</b>: AI-generated interior design plan",
								            "&nbsp;&nbsp;- <i>User</i>: Please provide a modern-style living room design",
								            "&nbsp;&nbsp;- <i>AI</i>: [Generates image, but the style does not match] ‚Üí Deduct points",
								            "&nbsp;&nbsp;- <i>User</i>: Please change the sofa color to dark grey",
								            "&nbsp;&nbsp;- <i>AI</i>: [Generates image with dark grey sofa] ‚Üí Full score"
								        ],
								        scoring: [
								            "<b>0 points</b>: Images are completely unrelated to the text, or necessary images are missing.",
								            "<b>1 point</b>: Partial relevance, but with significant mismatches (e.g., incorrect color or structure).",
								            "<b>2 points</b>: Largely consistent, with minor deviations.",
								            "<b>3 points</b>: Perfect alignment between text and images, with no inconsistencies."
								        ]
								    },
								    G5: {
								        title: "Style Coherence",
								        definition: "Assesses the consistency of style and subject representation across generated images, including texture, color harmony, lighting, rendering style, physical properties, clothing, and behavior. It penalizes visual repetition, such as overly similar outputs or duplicated elements within a single image. In multi-turn interactions, generated images should exhibit stylistic coherence across turns, with smooth transitions and no abrupt changes.",
								        examples: [
								            "<b>Special Case</b>: If only one turn includes an image while the other does not, visual style coherence is <b>not</b> affected. In such cases, assign a <b>default score of 3 points</b>."
								        ],
								        scoring: [
								            "<b>‚Äì1 point</b>: The task required image generation, but none was provided.",
								            "<b>0 points</b>: Images exhibit entirely different styles, tones, rendering, or subject traits, resulting in visual dissonance.",
								            "<b>1 point</b>: Some stylistic or subject consistency, but with clear discrepancies (e.g., sudden tone changes, mismatched rendering, or inconsistent subject traits).",
								            "<b>2 points</b>: Style, tone, and subject representation are generally consistent, with minor variations that do not affect overall coherence.",
								            "<b>3 points</b>: All images are highly consistent in style, tone, quality, and subject representation; visual transitions are smooth and contextually appropriate."
								        ]
								    },
								    L1: {
								        title: "Local Image-Text Consistency",
								        definition: "In a single dialogue turn, the textual description should closely match the generated image(s), ensuring the text accurately reflects the visual content without ambiguity or misleading information.",
								        examples: [
								            "If the turn includes multiple images, evaluate the overall consistency of the text with all images. Individual image feedback can be added as needed (e.g., [3,1] Image 1: accurate; Image 2: inconsistent).",
								            "If no image is generated, evaluate based on task requirements:",
								            "&nbsp;&nbsp;- If image generation was expected but omitted, assess the inconsistency between the text and the missing visual content.",
								            "&nbsp;&nbsp;- If the task (e.g., Visual Analysis) does not require image generation, assess consistency between the input image and the text.",
								            "Otherwise, default evaluation compares the answer text with the image(s) generated in that turn."
								        ],
								        scoring: [
								            "<b>0 points</b>: Text is irrelevant to the image(s) or contains major factual errors; Key descriptions are missing or completely incorrect (e.g., referencing nonexistent objects or scenes); Text may cause significant misunderstanding.",
								            "<b>1 point</b>: Text is partially related to the image(s), but includes clear errors or misleading descriptions; Covers part of the image content but omits or misrepresents key details or relationships; Reader must infer or adjust understanding to align with the image(s).",
								            "<b>2 points</b>: Text generally matches the image(s), with minor local inaccuracies (e.g., imprecise attribute descriptions or slight omissions); Does not hinder overall comprehension, but lacks precision upon close inspection.",
								            "<b>3 points</b>: Text is highly aligned with the image(s), covering all key elements and details; Free from factual errors or ambiguity; the description is natural and coherent."
								        ]
								    },
								    L2: {
								        title: "Visual Perceptual Quality",
								        definition: "Evaluates the visual realism, naturalness, and absence of distortion or artifacts in the generated image(s). Focuses on whether the image structure, colors, and composition realistically simulate the physical world, avoiding unnatural artifacts.",
								        examples: [
								            "In multi-image outputs, assign a unified score for overall quality. If image quality varies significantly, provide per-image feedback as needed (e.g., [3,1] Image 1: good; Image 2: distorted).",
								            "If no image is generated, assess any image provided in the user prompt. If the image has issues, point them out in the textual answer."
								        ],
								        scoring: [
								            "<b>0 points</b>: Obvious artifacts (e.g., disconnections, misalignments), severe distortions (e.g., highly unrealistic shapes), or structural errors (e.g., unbalanced proportions, illogical composition); Unnatural color rendering (e.g., harsh color blocks, abnormal tones); Lighting does not follow physical laws, severely affecting image recognizability.",
								            "<b>1 point</b>: Image is mostly recognizable but contains localized severe flaws; Examples: anatomical errors (e.g., limb dislocation), inconsistent local color (e.g., banding, strong noise), or small rendering failures; Overall naturalness is compromised, affecting visual coherence.",
								            "<b>2 points</b>: Image is generally natural and coherent; structure, color, and lighting are mostly reasonable; Minor local imperfections such as rough edges, small artifacts, or slight blurring that do not affect overall perceptual quality.",
								            "<b>3 points</b>: Image is visually realistic and natural; Well-structured, smooth color transitions, physically consistent lighting; No visible artifacts, distortions, or flaws; overall aesthetics and details are excellent."
								        ]
								    },
								    L3: {
								        title: "Contextual Coherence",
								        definition: "Assesses whether the response in this turn logically continues the dialogue history and remains consistent with prior content, avoiding contradictions.",
								        examples: [],
								        scoring: [
								            "<b>0 points</b>: Completely irrelevant or logically inconsistent with previous context.",
								            "<b>1 point</b>: Partially relevant but includes clear inconsistencies.",
								            "<b>2 points</b>: Mostly coherent, with minor deviations.",
								            "<b>3 points</b>: Fully consistent with prior dialogue; no contradictions."
								        ]
								    },
								    L4: {
								        title: "Text Quality",
								        definition: "Measures the clarity, coherence, and correctness of the output text. Includes grammar, spelling, readability, consistency with instructions and context, and absence of redundancy. Responses should be logically sound, well-structured, and clearly expressed, avoiding abrupt transitions or repetition.",
								        examples: [],
								        scoring: [
								            "<b>0 points</b>: Text is disorganized, lacks logic, and is hard to understand; may contain numerous grammar or spelling errors or repetitive content.",
								            "<b>1 point</b>: Some parts are logically clear, but the text includes noticeable jumps, omissions, or contradictions that hurt overall readability; may include frequent language errors or redundant expressions.",
								            "<b>2 points</b>: The overall logic is reasonable and the flow mostly smooth, but there are minor incoherences; some sentences may require optimization to improve readability.",
								            "<b>3 points</b>: Text is logically rigorous, clearly expressed, well-organized, and naturally structured; no obvious jumps or repetition; grammar and spelling are correct, providing a good reading experience."
								        ]
								    }
								};
								
								// Get modal elements
								const modal = document.getElementById('definitionModal');
								const modalContent = document.getElementById('definitionContent');
								const closeBtn = document.getElementsByClassName('close')[0];
								
								// Add click event listeners to table cells
								document.querySelectorAll('td[data-dimension]').forEach(cell => {
								    cell.addEventListener('click', () => {
								        const dimension = cell.getAttribute('data-dimension');
								        const def = definitions[dimension];
								        if (def) {
								            // Build HTML for the definition
								            let html = `<h1 class="text-xl font-bold mb-2" style="font-size: 36px;"><b><u>${def.title}</u></b></h1>`;
								            html += `<h3 class="text-lg font-semibold"><b>Definition</b></h3>`;
								            html += `<p class="mb-4">${def.definition}</p>`;
								            if (def.examples && def.examples.length > 0) {
								                html += `<h3 class="text-lg font-semibold"><b>Examples</b></h3>`;
								                html += `<ul class="list-disc pl-5 mb-4">`;
								                def.examples.forEach(example => {
								                    html += `<li>${example}</li>`;
								                });
								                html += `</ul>`;
								            }
								            html += `<h3 class="text-lg font-semibold"><b>Scoring Criteria</b></h3>`;
								            html += `<ul class="list-disc pl-5">`;
								            def.scoring.forEach(criterion => {
								                html += `<li>${criterion}</li>`;
								            });
								            html += `</ul>`;
								            modalContent.innerHTML = html;
								            modal.style.display = 'block';
								        }
								    });
								});
								
								// Close modal when clicking the close button
								closeBtn.addEventListener('click', () => {
								    modal.style.display = 'none';
								});
								
								// Close modal when clicking outside the modal content
								window.addEventListener('click', (event) => {
								    if (event.target === modal) {
								        modal.style.display = 'none';
								    }
								});
							</script>
							<br>
							<p> Crowdworkers first rate individual turns and then evaluate entire conversations from both local and global perspectives. A <span class="bold">Dual Verification</span> stage combines dedicated annotator efforts with professional quality control reviews to ensure
								guideline adherence. Structured <span class="bold">Language Feedback</span>, which offers concise explanations of scoring rationale, focused critiques, and refinement suggestions, further guides response improvement and substantially enhances annotation reliability.
							</p>
						</div>
					</div>
				</div>
			</div>
			<!-- Fullscreen Image Modal -->
			<div id="fullscreen-image-modal" class="modal" style="display: none;">
				<div class="modal-content" style="position: relative;">
					<span class="close" onclick="closeFullscreenImage()" style="position: absolute; top: 10px; right: 10px; cursor: pointer; font-size: 24px;">&times;</span>
					<img id="fullscreen-image" src="" style="width: 100%; height: auto;">
				</div>
			</div>
			<script>
				function openFullscreenImage(imageSrc) {
				    const modal = document.getElementById('fullscreen-image-modal');
				    const fullscreenImage = document.getElementById('fullscreen-image');
				    fullscreenImage.src = imageSrc;
				    modal.style.display = 'block';
				}
				
				function closeFullscreenImage() {
				    const modal = document.getElementById('fullscreen-image-modal');
				    modal.style.display = 'none';
				}
				
				// Close the modal when clicking outside the modal content
				window.onclick = function(event) {
				    const modal = document.getElementById('fullscreen-image-modal');
				    if (event.target === modal) {
				        modal.style.display = 'none';
				    }
				}
			</script>
</section>
<section class="hero is-light is-small">
	<div class="hero-body has-text-centered">
		<!-- <h1 class="title is-1 mathvista"><img src="static/images/mathvista.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>MathVista Dataset</h1> -->
		<h1 class="title is-1 mathvista">
			<!--<img src="static/images/mathvista.png" style="width:1em;vertical-align: middle" alt="Logo"/>-->
			<span class="mathvista" style="vertical-align: middle" id="Data Experiment Analysis">Dataset Main Analysis</span>
		</h1>
	</div>
</section>
<section class="section">
	<div class="container">
		<div class="columns is-centered has-text-centered">
			<div class="column is-four-fifths" style="width: 100%;">
				<!-- <h2 class="title is-3">Dataset Main Analysis</h2> -->
				<div class="content has-text-justified">
					<div id="results-carousel" class="carousel results-carousel">
						<div class="content has-text-centered">
							<img src="./assets/Figure5-1.png" style="width: 55%;">
							<ol style="margin-left: auto; margin-right: auto; width: 75%;">
								<li> Each seed question is expanded via an agent-based multi-turn QA construction workflow, producing at least 8 multi-turn QA instances per prompt. After pruning and filtering, we obtain 52.6k high-quality multi-turn QA instances, with 41.92% containing five or more
									turns. </li>
								<li> The resulting 52.6k QA instances cover 15+ vision-language understanding and generation tasks, such as image editing and visual tutorials. Each instance features interleaved textual and visual content in both inputs and outputs, with an average of 5.33 images per
									conversation. </li>
							</ol>
						</div>
						<div class="content has-text-centered">
							<img src="./assets/Figure4-0516_00.jpg" style="width: 55%;">
							<p>This figure illustrates the relationship between global and local preference annotation dimensions. We identify three key findings:
							<ol style="margin-left: auto; margin-right: auto; width: 75%;">
								<li>
									<strong>Modality perception precedes effective modality fusion</strong>: for both the local-local and local-global correlation, the evaluation of image-text consistency is strongly correlated with visual perceptual quality (up to 0.81). This suggests that before
									assessing multimodal information, human evaluators tend to prioritize a clear understanding of each individual modality, indicating that a clear perception of individual modalities is a prerequisite for reliable multimodal judgment.
								</li>
								<li>
									<strong>Long-horizon evaluations hinge on coherence and temporal consistency</strong>: for the global-global correlation, metrics such as helpfulness and completeness strongly align with context awareness and global visual consistency, underscoring the importance
									of maintaining coherent semantics, multimodal information, and consistency with prior conversational context over extended interactions.
								</li>
								<li>
									<strong>Intent grounding drives long-horizon crucial step recognition</strong>: in multi-turn scenarios, models may deviate from the user‚Äôs core intentions, producing self-directed responses. Despite locally high-scoring and plausible outputs, this leads to
									stylistic drift and omission of key steps over extended interactions, as demonstrated in the local-global setting.
								</li>
							</ol>
							</p>
						</div>
						<div class="content has-text-centered">
							<img src="./assets/Figure5.png" style="width: 55%;">
							<p style="margin-left: auto; margin-right: auto; width: 75%;"> This figure presents an analysis of the agreement among human annotators, AI annotators, and human verifiers. Our observations reveal that while AI annotators achieved an average score agreement of
								approximately 60% on <em>local</em> evaluation tasks, their agreement with humans is considerably lower for <em>global</em>, namely, longer-horizon, evaluation tasks. This finding partly indicates that current advanced models struggle to reach consensus with human
								judgments when performing score evaluations in multi-turn, multi-modal conversation tasks. We further conduct a broader investigation of the ability of MLLMs to assist human judges in three types of tasks. Consequently, until further validation of AI feedback's
								efficacy, we maintain a conservative position, considering the replacement of human annotation to be presently challenging. </p>
						</div>
						<!-- <div class="content has-text-centered">
		<img src="./assets/Figure6_00.jpg" style="width: 65%;">
		<p style="margin-left: auto; margin-right: auto; width: 75%;">Our findings, presented in this figure, suggest that modeling fine-grained <em>local (turn-level)</em> preferences is more effective in capturing human values and achieving better alignment. In contrast,
			directly modeling <em>global (conversation-level)</em> preferences often fails to reflect these nuanced preferences, especially in complex, long-horizon scenarios. We examine the bidirectional transfer between turn-level (<em>local</em>) and conversation-level
			(<em>global</em>) human preferences. Both <em>local-to-global</em> and <em>global-to-local</em> transfers are effective, since multi-turn questions typically hinge on the seed question‚Äôs intent. However, <em>global-to-local</em> transfer is consistently easier and
			better aligned with actual preferences. We attribute this to the greater stability of global preferences‚Äîreflecting users‚Äô overarching tendencies‚Äîwhereas local preferences are short-term and more context-dependent, making <em>local-to-global</em> transfer more
			challenging.</p>
		</div> -->
					</div>
				</div>
			</div>
		</div>
	</div>
</section>
<div>
	<section class="hero is-light is-small">
		<div class="hero-body has-text-centered">
			<h1 class="title is-1 mathvista">
				<span class="mathvista" style="vertical-align: middle" id="Inspiring Future Research">Inspiring Future Research</span>
			</h1>
		</div>
	</section>
	<div class="container" style="margin-bottom: 2vh;">
		<div class="columns is-centered has-text-centered">
			<div class="column is-four-fifths">
				<p>
					<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" /> InterMT lays the groundwork for advancing research on aligning fine-grained human values in <em>multi-turn multimodal understanding and generation</em> tasks, potentially inspiring new
					research directions. Building on real human data provided by our dataset, we identify several promising directions for future research:
				</p>
				<ul>
					<li>
						<strong style="color:#8B0000;">Modeling long-horizon values.</strong> <em>How can we model long-horizon, interleaved multimodal preferences by leveraging the <em>local</em> and <em>global</em> human annotations in <img src="./assets/logo_transp.png" alt="HuggingFace"
								style="width: 1em; height: 1em;" />InterMT?</em>
					</li>
					<li>
						<strong style="color:#8B0000;">Aligning dynamic human values:</strong> <em>How can we design algorithms that effectively incorporate real human feedback from <img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />InterMT to assess and enhance
							the performance of MLLMs?</em>
					</li>
				</ul>
			</div>
		</div>
	</div>
</div>
<br>
<div class="container" style="margin-bottom: 2vh;">
	<div class="columns is-centered has-text-centered">
		<div class="column is-four-fifths">
			<!-- Á¨¨‰∏ÄÈ°µÂÜÖÂÆπÔºöPreference Modeling -->
			<div class="content has-text-centered">
				<h4>Preference Modeling for Multi-turn Interleaved Multimodal Scenarios</h4>
				<p> We investigate two strategies for modeling long-horizon preferences in multi-turn multimodal scenarios: <em>prefix preference</em> and <em>chain-based preference</em>. Our findings, presented in this Figure, suggest that modeling fine-grained <em>local (turn-level)</em>
					preferences is more effective in capturing human values and achieving better alignment. In contrast, directly modeling <em>global (conversation-level)</em> preferences often fails to reflect these nuanced preferences, especially in complex, long-horizon scenarios. </p>
				<figure style="width: 70%; margin: 0 auto;">
					<img src="./assets/Figure6_00.jpg" style="width: 100%;">
				</figure>
				<p> We examine the bidirectional transfer between turn-level (<em>local</em>) and conversation-level (<em>global</em>) human preferences. As shown in the figure, both <em>local-to-global</em> and <em>global-to-local</em> transfers are effective, since multi-turn questions typically
					hinge on the seed question‚Äôs intent. However, <em>global-to-local</em> transfer is consistently easier and better aligned with actual preferences. We attribute this to the greater stability of global preferences‚Äîreflecting users‚Äô overarching tendencies‚Äîwhereas local preferences
					are short-term and more context-dependent, making <em>local-to-global</em> transfer more challenging. </p>
			</div>
			<!-- Á¨¨‰∫åÈ°µÂÜÖÂÆπÔºöScaling Laws -->
			<div class="content has-text-centered">
				<h4>Multi-turn Scaling Law of Turn-based Judge Moderation</h4>
				<p>
					<em>Can we accurately capture users' intentions and latent preferences with a limited number of conversational turns, thereby improving the modeling of long-term values?</em> Such capabilities are crucial for building general-purpose AI assistants, which need to understand and
					predict users' needs across diverse contexts, adapting to changing preferences over time. We investigate whether the discriminative power of judge models, trained on the first <code style="color:#000000;">k</code> turns, improves in subsequent turns (from <code
						style="color:#000000;">k+1</code> to <code style="color:#000000;">N</code>) and exhibits <em>scaling laws</em>.
				</p>
				<figure style="width: 70%; margin: 0 auto;">
					<img src="./assets/Figure7_00.jpg" style="width: 100%;">
				</figure>
				<p style="margin-left: auto; margin-right: auto; width: 75%;"> The results reveal two key insights: <strong>(1)</strong> Multi-turn judge moderation exhibits a generalization effect linked to the number of turns. As shown in this figure (a), for evaluation turn <code
						style="color:#000000;">k</code>, as the number of preceding turns increases from 1 to <code style="color:#000000;">k-1</code>, the model's accuracy continues to improve, with average future performance rising, indicating that training on multi-turn data with a limited number
					of turns can generalize to longer horizons. <strong>(2)</strong> Regarding the number of turns in the training data, the generalization effect shows a diminishing trend. As demonstrated in this figure (b), training with <code style="color:#000000;">k</code> turns does improve
					performance for <em>k+1 ‚Üí T</em> turns, but this effect diminishes as the number of turns increases. The decline is due to three factors: diminishing returns as the model struggles with long-term preferences, contextual drift as earlier turns lose relevance, and the evolving
					interaction between user intentions and latent preferences. </p>
			</div>
		</div>
	</div>
</div>
</div>
</div>
<section class="section">
	<div class="container" style="margin-bottom: 2vh;">
		<div class="columns is-centered has-text-centered">
			<div class="column is-four-fifths">
				<h2 class="title is-3" id="Evaluation">
					<span class="icon">
						<img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />
					</span>
					<b>InterMT</b> Performance Evaluation
				</h2>
				<div class="content has-text-justified">
					<p>
						<em>Do MLLMs truly understand what is desirable in multi-turn, multimodal interactions and how to align with human values?</em> This task is particularly challenging due to the absence of multimodal benchmarks that capture human preferences in multi-turn settings. Leveraging
						genuine feedback from <strong><img src="./assets/logo_transp.png" alt="HuggingFace" style="width: 1em; height: 1em;" />InterMT</strong>, we introduce <strong>InterMT-Bench</strong> to assess MLLMs' alignment with human values in multi-turn, multimodal tasks.
					</p>
					<p>
						<strong>InterMT-Bench</strong> comprises three distinct tasks: <em>Scoring Evaluation</em>, <em>Pair Comparison</em>, and <em>Crucial Step Recognition</em>.
					</p>
				</div>
				<div class="content">
					<table class="data-table center-table">
						<thead>
							<tr>
								<th rowspan="2" style="color: #ffffff;">Settings</th>
								<th rowspan="2" style="color: #ffffff;">MLLMs</th>
								<th colspan="5" style="color: #ffffff;">Local Setting</th>
								<th colspan="6" style="color: #ffffff;">Global Setting</th>
							</tr>
							<tr>
								<th style="color: #ffffff;">L1</th>
								<th style="color: #ffffff;">L2</th>
								<th style="color: #ffffff;">L3</th>
								<th style="color: #ffffff;">L4</th>
								<th style="color: #ffffff;">Avg.</th>
								<th style="color: #ffffff;">G1</th>
								<th style="color: #ffffff;">G2</th>
								<th style="color: #ffffff;">G3</th>
								<th style="color: #ffffff;">G4</th>
								<th style="color: #ffffff;">G5</th>
								<th style="color: #ffffff;">Avg.</th>
							</tr>
						</thead>
						<tbody>
							<tr>
								<td rowspan="12" class="bold">Scoring Evaluation</td>
								<td>Gemini-Flash* ‚Ä†</td>
								<td>0.346</td>
								<td>0.107</td>
								<td>0.119</td>
								<td>0.173</td>
								<td>0.186</td>
								<td>0.163</td>
								<td>0.042</td>
								<td>0.051</td>
								<td>
									<strong style="color:#C6011F">0.246</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
										<path fill-rule="evenodd"
											d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
											clip-rule="evenodd" />
									</svg>
								</td>
								<td>0.005</td>
								<td>0.101</td>
							</tr>
							<tr>
								<td>Gemini-Flash* (+reason)</td>
								<td>
									<strong style="color:#C6011F">0.361</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
										<path fill-rule="evenodd"
											d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
											clip-rule="evenodd" />
									</svg>
								</td>
								<td>0.072</td>
								<td>0.122</td>
								<td>0.168</td>
								<td>0.181</td>
								<td>-0.038</td>
								<td>0.083</td>
								<td>0.139</td>
								<td>0.199</td>
								<td>0.048</td>
								<td>0.086</td>
							</tr>
							<tr>
								<td>GPT-4.1</td>
								<td>0.264</td>
								<td>0.095</td>
								<td>0.242</td>
								<td>0.269</td>
								<td>0.218</td>
								<td>0.215</td>
								<td>0.216</td>
								<td>0.084</td>
								<td>0.044</td>
								<td>0.049</td>
								<td>0.122</td>
							</tr>
							<tr>
								<td>GPT-4.1 (+reason)</td>
								<td>0.281</td>
								<td>0.094</td>
								<td>0.272</td>
								<td>0.271</td>
								<td>0.229</td>
								<td>0.215</td>
								<td>0.255</td>
								<td>0.217</td>
								<td>0.216</td>
								<td>0.050</td>
								<td>
									<strong style="color:#C6011F">0.191</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
										<path fill-rule="evenodd"
											d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
											clip-rule="evenodd" />
									</svg>
								</td>
							</tr>
							<tr>
								<td>GPT-4o</td>
								<td>0.291</td>
								<td>
									<strong style="color:#C6011F">0.131</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
										<path fill-rule="evenodd"
											d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
											clip-rule="evenodd" />
									</svg>
								</td>
								<td>0.277</td>
								<td>0.268</td>
								<td>
									<strong style="color:#C6011F">0.242</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
										<path fill-rule="evenodd"
											d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
											clip-rule="evenodd" />
									</svg>
								</td>
								<td>0.254</td>
								<td>0.167</td>
								<td>0.137</td>
								<td>0.139</td>
								<td>0.069</td>
								<td>0.153</td>
							</tr>
							<tr>
								<td>GPT-4o (+reason)</td>
								<td>0.290</td>
								<td>0.091</td>
								<td>0.252</td>
								<td>
									<strong style="color:#C6011F">0.280</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
										<path fill-rule="evenodd"
											d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
											clip-rule="evenodd" />
									</svg>
								</td>
								<td>0.228</td>
								<td>0.183</td>
								<td>0.243</td>
								<td>0.194</td>
								<td>0.086</td>
								<td>0.072</td>
								<td>0.156</td>
							</tr>
							<tr>
								<td>Gemini-Pro*</td>
								<td>0.273</td>
								<td>0.079</td>
								<td>0.258</td>
								<td>0.168</td>
								<td>0.194</td>
								<td>
									<strong style="color:#C6011F">0.285</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
										<path fill-rule="evenodd"
											d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
											clip-rule="evenodd" />
									</svg>
								</td>
								<td>0.240</td>
								<td>-0.024</td>
								<td>0.235</td>
								<td>
									<strong style="color:#C6011F">0.145</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
										<path fill-rule="evenodd"
											d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
											clip-rule="evenodd" />
									</svg>
								</td>
								<td>0.176</td>
							</tr>
							<tr>
								<td>Gemini-Pro* (+reason)</td>
								<td>0.274</td>
								<td>0.070</td>
								<td>0.304</td>
								<td>0.211</td>
								<td>0.215</td>
								<td>0.239</td>
								<td>
									<strong style="color:#C6011F">0.267</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
										<path fill-rule="evenodd"
											d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
											clip-rule="evenodd" />
									</svg>
								</td>
								<td>0.195</td>
								<td>0.129</td>
								<td>0.060</td>
								<td>0.178</td>
							</tr>
							<tr>
								<td>Claude-thinking*</td>
								<td>0.299</td>
								<td>0.044</td>
								<td>0.262</td>
								<td>0.229</td>
								<td>0.209</td>
								<td>0.172</td>
								<td>0.140</td>
								<td>0.175</td>
								<td>0.150</td>
								<td>0.069</td>
								<td>0.141</td>
							</tr>
							<tr>
								<td>Claude-thinking* (+reason)</td>
								<td>0.291</td>
								<td>0.023</td>
								<td>0.254</td>
								<td>0.214</td>
								<td>0.196</td>
								<td>0.207</td>
								<td>0.260</td>
								<td>0.183</td>
								<td>0.155</td>
								<td>-0.001</td>
								<td>0.161</td>
							</tr>
							<tr>
								<td>o4-mini</td>
								<td>0.334</td>
								<td>0.062</td>
								<td>0.306</td>
								<td>0.134</td>
								<td>0.209</td>
								<td>0.169</td>
								<td>0.161</td>
								<td>0.120</td>
								<td>0.096</td>
								<td>0.028</td>
								<td>0.115</td>
							</tr>
							<tr>
								<td>o4-mini (+reason)</td>
								<td>0.326</td>
								<td>0.056</td>
								<td>
									<strong style="color:#C6011F">0.322</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
										<path fill-rule="evenodd"
											d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
											clip-rule="evenodd" />
									</svg>
								</td>
								<td>0.151</td>
								<td>0.214</td>
								<td>0.215</td>
								<td>0.229</td>
								<td>
									<strong style="color:#C6011F">0.347</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
										<path fill-rule="evenodd"
											d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
											clip-rule="evenodd" />
									</svg>
								</td>
								<td>0.137</td>
								<td>0.016</td>
								<td>0.189</td>
							</tr>
							<tr>
								<td rowspan="10" class="bold">Pair Comparison</td>
								<td>GPT-4.1</td>
								<td>0.541</td>
								<td>
									<strong style="color:#C6011F">0.589</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
										<path fill-rule="evenodd"
											d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
											clip-rule="evenodd" />
									</svg>
								</td>
								<td>0.508</td>
								<td>0.484</td>
								<td>0.531</td>
								<td>0.540</td>
								<td>0.520</td>
								<td>0.530</td>
								<td>
									<strong style="color:#C6011F">0.590</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
										<path fill-rule="evenodd"
											d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
											clip-rule="evenodd" />
									</svg>
								</td>
								<td>
									<strong style="color:#C6011F">0.563</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
										<path fill-rule="evenodd"
											d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
											clip-rule="evenodd" />
									</svg>
								</td>
								<td>
									<strong style="color:#C6011F">0.549</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
										<path fill-rule="evenodd"
											d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
											clip-rule="evenodd" />
									</svg>
								</td>
							</tr>
							<tr>
								<td>GPT-4.1 (+reason)</td>
								<td>0.550</td>
								<td>0.584</td>
								<td>0.501</td>
								<td>0.521</td>
								<td>
									<strong style="color:#C6011F">0.539</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
										<path fill-rule="evenodd"
											d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
											clip-rule="evenodd" />
									</svg>
								</td>
								<td>0.520</td>
								<td>0.520</td>
								<td>0.477</td>
								<td>0.513</td>
								<td>0.540</td>
								<td>0.514</td>
							</tr>
							<tr>
								<td>GPT-4o</td>
								<td>0.513</td>
								<td>0.488</td>
								<td>0.499</td>
								<td>0.510</td>
								<td>0.503</td>
								<td>0.560</td>
								<td>0.517</td>
								<td>
									<strong style="color:#C6011F">0.550</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
										<path fill-rule="evenodd"
											d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
											clip-rule="evenodd" />
									</svg>
								</td>
								<td>0.543</td>
								<td>0.470</td>
								<td>0.528</td>
							</tr>
							<tr>
								<td>GPT-4o (+reason)</td>
								<td>0.500</td>
								<td>0.537</td>
								<td>
									<strong style="color:#C6011F">0.511</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
										<path fill-rule="evenodd"
											d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
											clip-rule="evenodd" />
									</svg>
								</td>
								<td>0.509</td>
								<td>0.514</td>
								<td>0.542</td>
								<td>0.490</td>
								<td>0.545</td>
								<td>0.522</td>
								<td>0.528</td>
								<td>0.525</td>
							</tr>
							<tr>
								<td>Gemini-Pro*</td>
								<td>0.533</td>
								<td>0.521</td>
								<td>0.496</td>
								<td>0.533</td>
								<td>0.521</td>
								<td>
									<strong style="color:#C6011F">0.562</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
										<path fill-rule="evenodd"
											d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
											clip-rule="evenodd" />
									</svg>
								</td>
								<td>
									<strong style="color:#C6011F">0.566</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
										<path fill-rule="evenodd"
											d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
											clip-rule="evenodd" />
									</svg>
								</td>
								<td>0.523</td>
								<td>0.505</td>
								<td>0.505</td>
								<td>0.532</td>
							</tr>
							<tr>
								<td>Gemini-Pro* (+reason)</td>
								<td>0.526</td>
								<td>0.528</td>
								<td>0.513</td>
								<td>0.514</td>
								<td>0.520</td>
								<td>0.548</td>
								<td>0.562</td>
								<td>0.495</td>
								<td>0.522</td>
								<td>0.538</td>
								<td>0.533</td>
							</tr>
							<tr>
								<td>Claude-thinking*</td>
								<td>0.561</td>
								<td>0.568</td>
								<td>0.508</td>
								<td>0.502</td>
								<td>0.535</td>
								<td>0.539</td>
								<td>0.523</td>
								<td>0.518</td>
								<td>0.521</td>
								<td>0.528</td>
								<td>0.526</td>
							</tr>
							<tr>
								<td>Claude-thinking* (+reason)</td>
								<td>
									<strong style="color:#C6011F">0.567</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
										<path fill-rule="evenodd"
											d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
											clip-rule="evenodd" />
									</svg>
								</td>
								<td>0.550</td>
								<td>0.506</td>
								<td>0.519</td>
								<td>0.536</td>
								<td>0.512</td>
								<td>0.522</td>
								<td>0.512</td>
								<td>0.547</td>
								<td>0.512</td>
								<td>0.521</td>
							</tr>
							<tr>
								<td>o4-mini</td>
								<td>0.556</td>
								<td>0.549</td>
								<td>0.508</td>
								<td>
									<strong style="color:#C6011F">0.536</strong><svg style="color:#C6011F" class="w-6 h-6 text-gray-800 dark:text-white" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" width="12" height="12" fill="currentColor" viewBox="0 0 24 24">
										<path fill-rule="evenodd"
											d="M12 2c-.791 0-1.55.314-2.11.874l-.893.893a.985.985 0 0 1-.696.288H7.04A2.984 2.984 0 0 0 4.055 7.04v1.262a.986.986 0 0 1-.288.696l-.893.893a2.984 2.984 0 0 0 0 4.22l.893.893a.985.985 0 0 1 .288.696v1.262a2.984 2.984 0 0 0 2.984 2.984h1.262c.261 0 .512.104.696.288l.893.893a2.984 2.984 0 0 0 4.22 0l.893-.893a.985.985 0 0 1 .696-.288h1.262a2.984 2.984 0 0 0 2.984-2.984V15.7c0-.261.104-.512.288-.696l.893-.893a2.984 2.984 0 0 0 0-4.22l-.893-.893a.985.985 0 0 1-.288-.696V7.04a2.984 2.984 0 0 0-2.984-2.984h-1.262a.985.985 0 0 1-.696-.288l-.893-.893A2.984 2.984 0 0 0 12 2Zm3.683 7.73a1 1 0 1 0-1.414-1.413l-4.253 4.253-1.277-1.277a1 1 0 0 0-1.415 1.414l1.985 1.984a1 1 0 0 0 1.414 0l4.96-4.96Z"
											clip-rule="evenodd" />
									</svg>
								</td>
								<td>0.537</td>
								<td>0.552</td>
								<td>0.498</td>
								<td>0.522</td>
								<td>0.518</td>
								<td>0.495</td>
								<td>0.517</td>
							</tr>
							<tr>
								<td>o4-mini (+reason)</td>
								<td>0.521</td>
								<td>0.564</td>
								<td>0.522</td>
								<td>0.513</td>
								<td>0.530</td>
								<td>0.534</td>
								<td>0.510</td>
								<td>0.507</td>
								<td>0.512</td>
								<td>0.483</td>
								<td>0.509</td>
							</tr>
						</tbody>
					</table>
				</div>
			</div>
		</div>
	</div>
	</div>
	<div class="container" style="margin-bottom: 2vh;">
		<div class="columns is-centered has-text-centered">
			<div class="column is-four-fifths">
				<div class="content has-text-centered">
					<div style="display: flex; align-items: center; justify-content: center; gap: 0em; flex-wrap: wrap;">
						<!-- Image on the left -->
						<div style="flex: 1; min-width: 400px; text-align: center;">
							<figure>
								<img src="./assets/crucial_step_recognition_score_00.jpg" alt="Crucial Step Recognition" style="width: 100%; max-width: 400px; border-radius: 8px;">
							</figure>
						</div>
						<!-- Text on the right -->
						<div style="flex: 2; min-width: 300px;">
							<h4 style="text-align: justify;">Results and Takeaways</h4>
							<p> We evaluated <span style="color:#C6011F">6</span> advanced MLLMs for their ability to assist in judgment for multi-turn multimodal interactions, considering the nine dimensions proposed above. The results reveal key observations: <strong>Existing models still face
									challenges in aligning with long-horizon human values, but they perform more accurately in evaluating local, fine-grained preferences</strong>. As shown in the table, all models exhibit significant gaps in performance compared to humans in both <em>Score
									Evaluation</em> and <em>Pair Comparison</em> tasks. However, the models demonstrate better accuracy when assessing local dimensions rather than global dimensions, suggesting that capturing fine-grained (<em>e.g.</em>, turn-level) human preferences is crucial for
								both evaluation and alignment with human dynamic and long-horizon values. </p>
							<p> However, there is cause for optimism: current MLLMs exhibit near-human-level performance (<span style="color:#C6011F">4.38/5</span>) in recognizing task completion and aligning with human intent (<em>i.e.</em>, <em>Crucial Step Recognition</em>), providing potential
								solutions for long-term value alignment. </p>
							<p>
						</div>
						<!-- Á¨¨‰∏âÈ°µÂÜÖÂÆπÔºöEvaluation Setup -->
						<div class="content has-text-centered">
							<h4>Induced Bias and Hallucination</h4>
							<p> We identified issues related to bias and hallucination: <strong>Position Bias</strong>, where models consistently favor responses in specific positions (<em>e.g.</em>, the first answer), often influenced by training data that places correct answers at the beginning or
								end of prompts; and <strong>High-Score Bias</strong>, where models tend to assign higher scores to entire multi-turn communications. </p>
							<figure style="width: 75%; margin: 0 auto;">
								<img src="./assets/Figure8-v1_00.jpg" style="width: 100%;">
							</figure>
							<p style="margin-left: auto; margin-right: auto; width: 75%;"> These issues, particularly in long-horizon tasks, may hinder the model‚Äôs ability to capture differences between extended conversations, thereby posing challenges in modeling long-horizon human values and
								potentially leading to safety concerns. </p>
						</div>
					</div>
				</div>
			</div>
		</div>
	</div>
</section>
<section>
	<div class="container" style="margin-bottom: 2vh;">
		<div class="columns is-centered has-text-centered">
			<div class="column is-four-fifths">
				<h2 class="title is-3" id="Evaluation" style="text-align: center;">Examples</h2>
				<p style="text-align: justify;"> We present representative examples of multi-turn QA in InterMT here, which spanning over different multi-turn multimodal understanding and generation scenarios, demonstrating the strength and versatility of our dataset. </p>
				<div class="diag-dropdown-container"></div>
				<div id="Dataset Examples" class="diag-section diag-orange">
					<div class="diag-section-header diag-orange">
						<svg>
							<use xlink:href="#icon-dataset"></use>
						</svg>
						<span class="highlight">Dataset Examples</span>
					</div>
					<div class="diag-content" id="diag-content">
						<div class="columns is-multiline" style="margin-top: 2em;">
							<!-- Group 1 -->
							<div class="column is-one-third" style="padding: 1em;">
								<div class="card" style="border-radius: 8px; padding: 1em; box-shadow: 0 2px 10px rgba(0,0,0,0.1); background-color: white;">
									<h3 style="color: #ff6600; text-align: center; margin-bottom: 1em;">Group 1</h3>
									<div id="group1Content"></div>
								</div>
							</div>
							<!-- Group 2 -->
							<div class="column is-one-third" style="padding: 1em;">
								<div class="card" style="border-radius: 8px; padding: 1em; box-shadow: 0 2px 10px rgba(0,0,0,0.1); background-color: white;">
									<h3 style="color: #007acc; text-align: center; margin-bottom: 1em;">Group 2</h3>
									<div id="group2Content"></div>
								</div>
							</div>
							<!-- Group 3 -->
							<div class="column is-one-third" style="padding: 1em;">
								<div class="card" style="border-radius: 8px; padding: 1em; box-shadow: 0 2px 10px rgba(0,0,0,0.1); background-color: white;">
									<h3 style="color: #5cb45c; text-align: center; margin-bottom: 1em;">Group 3</h3>
									<div id="group3Content"></div>
								</div>
							</div>
						</div>
					</div>
				</div>
			</div>
		</div>
	</div>
</section>
<script>
	const dataexamples = [
					{
					"group":1,
					"figures": 0,
					"prompt": "If you were to create a new dish inspired by this one, what ingredients would you use?",
					"response": "I understand that you are asking for suggestions to create a new dish inspired by the featured plate of mac and cheese. First, let's take into consideration the current ingredients on the plate, including macaroni and cheese, carrots, celery, and onions. For a new dish, I would suggest combining these elements while adding some new and creative twists.\n\nFor example, you could create a fusion of pasta dishes by incorporating other types of pasta, such as shells, penne, or rotini, along with a creamy sauce that combines elements of Alfredo, pesto, or cheese-based dishes like fettuccine alfredo. In addition, you could introduce a variety of vegetables like mushrooms, spinach, zucchini, or artichokes, as well as other protein sources such as chicken, ground beef, or even a bean-based option for vegetarians.\n\nFor a beverage, consider pairing the new dish with a crisp white wine, such as Pinot Grigio or Sauvignon Blanc, both of which can complement the flavors of pasta, cheese, and vegetables. To enhance the dining experience, you could also include a side salad, a lightly dressed green salad, or perhaps even a small serving of garlic bread.\n\nRemember to adjust the quantities of ingredients based on personal preferences and dietary requirements. Enjoy creating your own unique pasta dish inspired by the ingredients seen in the previous questions' images.",
					"imageR": "#",
					"imageP": ".\\assets\\diag_sample\\5.23plus\\image1.png",
					"color": "default"
				},
				{
					"group":1,
					"figures": 1,
					"prompt": "That sounds delicious! How would you recommend seasoning or spicing the new dish to enhance its flavors even further?",
					"response": "Sure, here's an image to help you visualize the new dish idea:\nImage, artist's concept of a fusion dish combining mac and cheese, seafood, and vegetables",
					"imageR": ".\\assets\\diag_sample\\5.23plus\\image2.jpeg",
					"imageP": "#",
					"color": "default"
				},
				{
					"group":1,
					"figures": 3,
					"prompt": "The AI provided an image of the fusion dish. I should probably ask a question about the visual representation to keep the conversation going. That image is quite helpful! Are there any specific elements in the image, like the seafood or vegetables used, that you think contribute most to the overall flavor profile of the dish?",
					"response": "To enhance the flavors of your new dish, consider using a combination of herbs and spices that complement the ingredients you've chosen. For instance, if you're using seafood, you might want to add some lemon zest, parsley, and dill to bring out the natural flavors of the fish. If you're using vegetables like mushrooms or spinach, try adding garlic, thyme, and a pinch of nutmeg to give it a rich, earthy taste.\n\nFor the pasta, you can season it with a bit of grated Parmesan cheese, black pepper, and a touch of salt to balance the creaminess of the sauce. If you're using a bean-based protein source, consider adding cumin, coriander, and a hint of chili powder to give it a warm, spicy kick.\n\nRemember to taste and adjust the seasoning as you go to ensure the flavors are balanced and harmonious. Enjoy experimenting with different combinations to find your perfect blend!",
					"imageR": "#",
					"imageP": "#",
					"color": "default"
				},
				{
					"group":1,
					"figures": 3,
					"prompt": "That's great advice. Considering the image you provided, and the suggested seasonings, what kind of sauce would best tie together the seafood, vegetables, and pasta in this fusion dish?",
					"response": "To tie together the seafood, vegetables, and pasta in this fusion dish, I recommend using a creamy garlic and herb sauce. This type of sauce will complement the flavors of the seafood and vegetables while also enhancing the pasta. You can make the sauce by saut√©ing garlic and herbs (such as parsley, thyme, and basil) in butter or olive oil, then adding heavy cream and letting it simmer until it thickens. Season with salt and pepper to taste, and you'll have a delicious sauce that brings all the components of your dish together.\n\nWould you like some suggestions on how to prepare the seafood and vegetables for this dish?",
					"imageR": "#",
					"imageP": "#",
					"color": "default"
				},
				{
					"group":1,
					"figures": 3,
					"prompt": "Yes, please! Specifically, could you suggest a cooking method for the seafood and vegetables that would best complement the creamy garlic and herb sauce you recommended?",
					"response": "To prepare the seafood and vegetables for this dish, you can follow these steps:\n\n1. Seafood: Choose your preferred type of seafood, such as shrimp, scallops, or fish fillets. Season with salt, pepper, and any desired herbs or spices. Saut√© in a pan with a bit of olive oil or butter until cooked through.\n\n2. Vegetables: Select a variety of vegetables that complement the seafood and pasta, such as bell peppers, zucchini, and cherry tomatoes. Slice or chop them into bite-sized pieces. Saut√© in a pan with a bit of olive oil or butter until tender but still slightly crisp.\n\n3. Pasta: Cook the pasta according to package instructions until al dente. Drain and set aside.\n\n4. Sauce: Prepare the creamy garlic and herb sauce as previously mentioned.\n\n5. Assembly: In a large pan, combine the cooked seafood, vegetables, and pasta. Pour the sauce over the mixture and toss gently to coat everything evenly.\n\nServe hot and enjoy your delicious fusion dish!\n\nWould you like some ideas for side dishes or beverages to pair with this meal?",
					"imageR": "#",
					"imageP": "#",
					"color": "default"
				},
				{
					"group":1,
					"figures": 1,
					"prompt": "Yes, please! Specifically, could you suggest a cooking method for the seafood and vegetables that would best complement the creamy garlic and herb sauce you recommended?",
					"response": "Certainly! To complement the creamy garlic and herb sauce, I suggest the following cooking methods for your seafood and vegetables:\n\nSeafood:\n- Saut√©ing: Saut√©ing is a quick and easy method that works well with seafood like shrimp, scallops, or diced fish. Heat some olive oil or butter in a pan over medium-high heat, then add the seafood and cook until it's just cooked through. Be careful not to overcook it, as seafood can become rubbery.\n- Grilling: Grilling adds a smoky flavor to seafood. Brush the seafood with olive oil and season with salt, pepper, and any other desired spices. Grill over medium heat until cooked through, flipping once.\n- Poaching: Poaching is a gentle method that's great for delicate fish fillets. Submerge the fish in simmering liquid (such as vegetable broth or white wine) until cooked through.\n\nVegetables:\n- Roasting: Roasting brings out the natural sweetness of vegetables. Toss the vegetables with olive oil, salt, pepper, and herbs, then spread them out on a baking sheet and roast in a preheated oven until tender and slightly caramelized. Image, roasted vegetables on a baking sheet\n- Steaming: Steaming is a healthy way to cook vegetables, as it helps retain their nutrients. Place the vegetables in a steamer basket over boiling water and steam until tender-crisp.\n- Blanching: Blanching involves briefly cooking vegetables in boiling water, then immediately plunging them into ice water to stop the cooking process. This method helps preserve their color and texture.\n\nThese methods will help bring out the best flavors and textures in your seafood and vegetables, making them a perfect match for the creamy garlic and herb sauce. Would you like some specific recommendations for which seafood and vegetables pair best with this type of sauce?",
					"imageR": ".\\assets\\diag_sample\\5.23plus\\image3.jpeg",
					"imageP": "#",
					"color": "default"
				},
				{
					"group":1,
					"figures": 1,
					"prompt": "Yes, please! Specifically, could you suggest a cooking method for the seafood and vegetables that would best complement the creamy garlic and herb sauce you recommended?",
					"response": "To best complement the creamy garlic and herb sauce, a great cooking method for the seafood and vegetables would be to lightly saut√© them. Saut√©ing will retain their natural flavors while creating a pleasing texture that pairs well with the pasta. Start by heating some olive oil or butter in a pan over medium heat. Add the seafood, such as shrimp or scallops, and cook until they are just done and opaque. Remove them from the pan and set aside. Then, saut√© your choice of vegetables, like bell peppers, zucchini, or asparagus, until they're tender yet crisp. Once the vegetables are cooked, return the seafood to the pan, add the prepared creamy garlic and herb sauce, toss with cooked pasta, and serve.\n\nThis method ensures a harmonious blend of flavors and textures, enhancing the dish's overall experience.\nImage, illustrated steps for saut√©ing seafood and vegetables\n\nWould you like to see some plating ideas for serving this dish?",
					"imageR": ".\\assets\\diag_sample\\5.23plus\\image4.jpeg",
					"imageP": "#",
					"color": "default"
				},
				{
					"group":2,
					"figures": 0,
					"prompt": "If you were to plan a road trip using this map, which cities or landmarks would you include in your itinerary?",
					"response": "Dependent on the specific road trip, the cities and landmarks included in the itinerary might range from a few to multiple destinations. To help, I can provide recommendations based on popular road trip routes covering various cities and landmarks. For example, the Great American Road Trip or the Pacific Coast Highway routes include famous American cities such as San Francisco, Los Angeles, Santa Cruz, San Diego, and unique landmarks like the Golden Gate Bridge, Big Sur, and the Redwood Forest. If the trip is aimed at reaching more Canadian cities, options like the Trans-Canada Highway route might include cities like Vancouver, Victoria, Banff, and Calgary with natural wonders like the Rocky Mountains, Victoria Island, and picturesque lakes. To find more customized itineraries, please visit bestoftheroad.com and explore travel reviews, photos, and videos from around the United States and Canada.",
					"imageR": "#",
					"imageP": ".\\assets\\diag_sample\\5.23plus\\image5.png",
					"color": "default"
				},
				{
					"group":2,
					"figures": 3,
					"prompt": "That sounds exciting! Are there any specific activities or attractions you'd recommend experiencing along these routes?",
					"response": "Recommended road trips vary depending on the interests of the travelers, so BestOfTheRoad.com offers tailored itineraries to suit specific preferences. Furthermore, the site features diverse travel reviews, photos, and videos to provide in-depth experiences and guide trippers in their choices. Additionally, BestOfTheRoad.com allows users to input their preferred locations, landmarks, or cities alongside their travel preferences, making for a unique road trip experience. Explore BestOfTheRoad.com to plan your next road trip and discover the best routes to your desired destinations.",
					"imageR": "#",
					"imageP": "#",
					"color": "default"
				},
				{
					"group":2,
					"figures": 3,
					"prompt": "That's great that BestOfTheRoad.com offers so many options. Does it also provide information on the best times of year to visit certain locations, considering weather or seasonal events?",
					"response": "Yes, BestOfTheRoad.com does offer information on the best times to visit certain locations, taking into account weather conditions and seasonal events. They provide detailed travel reviews, photos, and videos that can help you plan your trip according to your preferences. Would you like some suggestions on the best time to visit New York State based on the map you provided?",
					"imageR": "#",
					"imageP": "#",
					"color": "default"
				},
				{
					"group":2,
					"figures": 1,
					"prompt": "Yes, please! I'm curious to know about the best time to visit New York State, especially considering the map I provided. What kind of seasonal events or weather conditions should I be aware of? Also, does BestOfTheRoad.com highlight any specific regions or attractions within New York State based on the time of year?",
					"response": "New York State has a diverse climate, with each season offering unique experiences. Spring (March to May) is ideal for hiking and exploring the state's parks, while summer (June to August) is perfect for beach visits and outdoor festivals. Fall (September to November) brings stunning foliage, and winter (December to February) offers skiing and snowboarding opportunities. Image, a collage of New York State in different seasons BestOfTheRoad.com highlights various regions and attractions throughout the year, such as the Finger Lakes wine region in spring, the Adirondack Mountains in summer, the Catskills in fall, and Lake Placid in winter. Would you like more information on specific events or attractions during your preferred season?",
					"imageR": ".\\assets\\diag_sample\\5.23plus\\image6.jpeg",
					"imageP": "#",
					"color": "default"
				},
				{
					"group":2,
					"figures": 1,
					"prompt": "That's a helpful overview of New York State's seasons! Given the image and your recommendations, could you elaborate on the Finger Lakes wine region in the spring? Specifically, what kind of events or activities are popular during that time, and what makes it a good time to visit that area?",
					"response": "In the spring, the Finger Lakes wine region becomes a popular destination for wine enthusiasts and nature lovers alike. The region hosts several events and activities during this time, including wine tastings, vineyard tours, and the annual Finger Lakes Wine Festival. The mild temperatures and blooming flowers create a picturesque setting for exploring the wineries and scenic landscapes. It's a great time to visit because the crowds are smaller compared to peak tourist seasons, allowing for a more intimate and relaxed experience. Image, a photo of the Finger Lakes wine region in spring\n\nWould you like to know more about specific wineries or events happening in the Finger Lakes wine region during the spring?",
					"imageR": ".\\assets\\diag_sample\\5.23plus\\image7.jpeg",
					"imageP": "#",
					"color": "default"
				},
				{
					"group":2,
					"figures": 1,
					"prompt": "That's a helpful overview of New York State's seasons! Given the image and your recommendations, could you elaborate on the Finger Lakes wine region in the spring? Specifically, what kind of events or activities are popular during that time, and what makes it a good time to visit that area?",
					"response": "Spring is a fantastic time to visit the Finger Lakes wine region! The weather is starting to warm up, the vineyards are coming back to life, and there are fewer crowds compared to the summer and fall.\n\nPopular events and activities include:\n- Wine tasting: Many wineries begin to release their new vintages in the spring.\n- Spring festivals: Some towns host festivals celebrating the season with food, crafts, and live music.\n- Outdoor activities: Hiking, biking, and kayaking are all great ways to enjoy the natural beauty of the region as the weather improves.\n\nWhat makes it a good time to visit:\n- Beautiful scenery: The vineyards are lush and green, and the lakes are sparkling in the sunshine.\n- Pleasant weather: The temperatures are mild, making it comfortable to explore the region.\n- Fewer crowds: You'll have more space to relax and enjoy the wineries and attractions.\n\nImage, A photograph showcasing vineyards in the Finger Lakes region during spring, with budding vines and clear blue skies.\n\nWould you like some specific winery or event recommendations for a spring visit to the Finger Lakes?",
					"imageR": ".\\assets\\diag_sample\\5.23plus\\image8.jpeg",
					"imageP": "#",
					"color": "default"
				},
				{
					"group":2,
					"figures": 1,
					"prompt": "That's a helpful overview of New York State's seasons! Given the image and your recommendations, could you elaborate on the Finger Lakes wine region in the spring? Specifically, what kind of events or activities are popular during that time, and what makes it a good time to visit that area?",
					"response": "The Finger Lakes wine region is an excellent destination in the spring for several reasons. As the snow melts and the temperatures rise, the vineyards begin to bud, offering a picturesque landscape and promising a new season of wine production. Spring is a perfect time for wine enthusiasts to visit, as many wineries host special events such as wine tastings and vineyard tours that highlight their new releases.\n\nPopular events during this time include the Wine & Herb Festival, where visitors can pair wines with herbs and explore the culinary side of the Finger Lakes. Additionally, many wineries and local businesses participate in Earth Day events focused on sustainability and the environment, creating a unique opportunity to engage with local culture and initiatives.\n\nSpring also brings more moderate weather, making it ideal for outdoor activities like hiking and exploring the region's natural beauty, including the lakes themselves. For those interested in cultural experiences, spring often features local festivals and craft fairs, providing a vibrant mix of art, food, and entertainment.\n\nImage, scenic view of blooming vineyards in the Finger Lakes wine region during spring\n\nWould you like to know about specific wineries to visit or additional activities that complement your trip in the Finger Lakes?",
					"imageR": ".\\assets\\diag_sample\\5.23plus\\image9.jpeg",
					"imageP": "#",
					"color": "default"
				},
				{
					"group":3,
					"figures": 0,
					"prompt": "How would you enhance the functionality and aesthetic appeal of this bathroom using a minimal budget? Please provide specific suggestions for changes or additions.",
					"response": "Based on the image, here are some budget-friendly suggestions to enhance the functionality and aesthetic appeal of the bathroom:\n\n- New Shower Curtain: Replace the existing shower curtain with a more modern and visually appealing one. Consider a bold pattern or color to add a focal point.\n- Updated Hardware: Swapping out the existing faucet, shower head, and cabinet hardware (if applicable) with more stylish and modern options can make a big difference. Look for affordable options at hardware stores or online.\n- Mirror Upgrade: If possible, replace the existing mirror with a larger, more decorative one. A framed mirror can add a touch of elegance. If replacing isn't feasible, consider adding a frame to the existing mirror.\n- Declutter and Organize: Remove any unnecessary items from the countertops and shelves. Use attractive storage containers to keep toiletries and other items organized and out of sight.\n- Fresh Paint: If the walls are looking tired, consider a fresh coat of paint. Choose a light and airy color to brighten up the space.\n- New Accessories: Add a few new accessories such as a stylish soap dispenser, toothbrush holder, and a small plant to add some personality to the space.\n- Improved Lighting: Assess the existing lighting and consider adding additional light sources, such as a vanity light or a small lamp, to brighten up the space and create a more inviting atmosphere.\n\nHere is an image of a possible upgrade.\nImage, showing a bathroom with stylish shower curtain, updated hardware, framed mirror, and decluttered countertop\n\nWould you like some specific product recommendations for any of these suggestions?",
					"imageR": ".\\assets\\diag_sample\\5.23plus\\image11.jpeg",
					"imageP": ".\\assets\\diag_sample\\5.23plus\\image10.png",
					"color": "default"
				},
				{
					"group":3,
					"figures": 1,
					"prompt": "These are great suggestions! What are some affordable ways to improve the lighting in a bathroom?",
					"response": "To improve lighting in the bathroom on a budget, consider adding a stylish vanity light above the sink. There are many affordable options available, such as this one:\nImage, showing a bathroom with a stylish vanity light positioned above the sink\n\nWould you like more lighting ideas or other product recommendations?",
					"imageR": ".\\assets\\diag_sample\\5.23plus\\image12.jpeg",
					"imageP": "#",
					"color": "default"
				},
				{
					"group":3,
					"figures": 1,
					"prompt": "That's a great start. Besides the vanity light, are there any other affordable lighting options that could significantly improve the overall brightness and ambiance of the bathroom, perhaps focusing on ambient or accent lighting?",
					"response": "For affordable lighting options, you could consider adding LED strip lights under the cabinets or around the mirror for ambient lighting. Another option is to install a dimmer switch to control the brightness of the existing light fixture. Additionally, placing a small table lamp on a shelf or countertop can provide accent lighting and create a cozy atmosphere. Here's an example of how LED strip lights can be used in a bathroom:\nImage, showing a bathroom with LED strip lights installed under the cabinets and around the mirror\n\nWould you like more information on installing LED strip lights or other lighting solutions?",
					"imageR": ".\\assets\\diag_sample\\5.23plus\\image13.jpeg",
					"imageP": "#",
					"color": "default"
				},
				{
					"group":3,
					"figures": 1,
					"prompt": "That's helpful! What are some things to consider when selecting the color temperature for LED strip lights in a bathroom to achieve a specific ambiance?",
					"response": "When selecting the color temperature for LED strip lights in a bathroom, consider the following factors to achieve a specific ambiance:\n\n1. Purpose: Determine the primary function of the bathroom - whether it's for relaxation, grooming, or both. This will help you decide if you need warm, cool, or neutral white lighting.\n\n2. Mood: Warm white (2700K-3000K) creates a cozy and relaxing atmosphere, ideal for a spa-like experience. Cool white (4000K-5000K) provides a brighter, more energizing environment suitable for grooming tasks. Neutral white (3500K-4000K) offers a balanced look that works well for general illumination.\n\n3. Existing Decor: Match the color temperature of the LED strip lights to the existing decor and fixtures in the bathroom. For example, if you have warm-toned materials like wood or beige tiles, warm white lighting will complement them better.\n\n4. Energy Efficiency: LED strip lights are energy-efficient, but choosing the right color temperature can further reduce energy consumption. Warmer tones tend to consume less energy compared to cooler tones.\n\n5. Dimmability: Opt for dimmable LED strip lights to adjust the brightness and color temperature according to your needs and preferences throughout the day.\n\nHere's an example of how different color temperatures can affect the ambiance of a bathroom:\nImage, showing a bathroom with warm white, neutral white, and cool white LED strip lights\n\nWould you like more information on how to install LED strip lights in your bathroom or any other lighting-related questions?",
					"imageR": ".\\assets\\diag_sample\\5.23plus\\image14.jpeg",
					"imageP": "#",
					"color": "default"
				},
				{
					"group":3,
					"figures": 0,
					"prompt": "That's a very comprehensive guide to choosing the right color temperature! Considering the points you've made, especially regarding the existing decor, what are some potential pitfalls or common mistakes people make when selecting LED strip light color temperatures for their bathrooms, and how can they be avoided?",
					"response": "Potential pitfalls or common mistakes people make when selecting LED strip light color temperatures for their bathrooms include:\n\n1. Ignoring the existing decor: Choosing a color temperature that doesn't match the existing decor can result in an unharmonious and unappealing appearance. It's essential to consider the colors and materials present in the bathroom when selecting LED strip lights.\n\n2. Overlooking the purpose of the bathroom: Failing to consider the primary function of the bathroom can lead to inappropriate lighting choices. For instance, using cool white lighting in a bathroom meant for relaxation might not create the desired ambiance.\n\n3. Neglecting energy efficiency: Some people might overlook the energy-saving benefits of certain color temperatures. Warmer tones generally consume less energy than cooler tones, so choosing the right color temperature can contribute to energy savings.\n\n4. Not considering dimmability: Dimmable LED strip lights allow users to adjust the brightness and color temperature according to their needs and preferences. Ignoring this feature can limit the versatility and functionality of the lighting setup.\n\nTo avoid these pitfalls, carefully assess the existing decor, the purpose of the bathroom, and your personal preferences before selecting LED strip light color temperatures. Additionally, consider the energy efficiency and dimmability of the lights to ensure a well-rounded and functional lighting solution.\n\nWould you like more information on how to choose the right LED strip lights for your bathroom or any other lighting-related questions?",
					"imageR": "#",
					"imageP": "#",
					"color": "default"
				},
				{
					"group":3,
					"figures": 1,
					"prompt": "That's a very comprehensive guide to choosing the right color temperature! Considering the points you've made, especially regarding the existing decor, what are some potential pitfalls or common mistakes people make when selecting LED strip light color temperatures for their bathrooms, and how can they be avoided?",
					"response": "Here are some common pitfalls and how to avoid them when selecting LED strip light color temperatures for bathrooms:\n\n- Mismatched Tones: A common mistake is choosing a color temperature that clashes with the existing bathroom finishes. For example, pairing cool white lights with warm-toned beige tiles (as seen in the original image) can make the space feel unbalanced and sterile. Solution: Carefully consider the existing color palette and choose a color temperature that complements it. In this case, warm white or neutral white might be better choices.\n\n- Overlooking Functionality: Selecting a color temperature solely based on aesthetics can compromise functionality. For instance, using overly warm lighting can make it difficult to see clearly when applying makeup or shaving. Solution: Consider the tasks performed in the bathroom and choose a color temperature that provides adequate illumination for those tasks. A combination of lighting types (e.g., neutral white for the vanity area and warm white for ambient lighting) can be an effective compromise.\n\n- Ignoring Room Size: The size of the bathroom also influences the ideal color temperature. In small bathrooms, overly cool lighting can feel harsh and clinical, while overly warm lighting can make the space feel cramped. Solution: Adjust the color temperature and brightness of the LED strip lights to suit the size of the bathroom. Dimmable LED strip lights can be particularly useful in smaller spaces, allowing you to adjust the lighting to your liking.\n\n- Uneven Lighting Distribution: Installing LED strip lights without considering the overall lighting scheme can result in uneven light distribution and shadows. Solution: Plan the placement of the LED strip lights carefully to ensure that they provide adequate and even illumination throughout the bathroom. Consider using multiple strips of lights to create a more balanced effect.\n\n- Neglecting Light Reflectance: The color of the walls and other surfaces in the bathroom can affect how the light is reflected and perceived. Darker colors absorb light, while lighter colors reflect it. Solution: Choose a color temperature that takes into account the light reflectance of the bathroom surfaces. If the bathroom has dark walls or fixtures, you may need to use brighter or cooler lighting to compensate.\n\nImage, showing a bathroom with a combination of warm and cool LED strip lights, highlighting different areas and functions\n\nWould you like to explore specific placement ideas for LED strip lights in the bathroom, considering these pitfalls?",
					"imageR": ".\\assets\\diag_sample\\5.23plus\\image15.jpeg",
					"imageP": "#",
					"color": "default"
				},
				{
					"group":3,
					"figures": 1,
					"prompt": "That's a very comprehensive guide to choosing the right color temperature! Considering the points you've made, especially regarding the existing decor, what are some potential pitfalls or common mistakes people make when selecting LED strip light color temperatures for their bathrooms, and how can they be avoided?",
					"response": "When selecting LED strip light color temperatures for bathrooms, there are some common pitfalls and mistakes that you might want to avoid:\n\n1. Ignoring the Purpose: One major mistake is not considering the bathroom's primary use. If the bathroom is used mainly for grooming, a cooler color temperature (between 4000K-5000K) will provide brighter, clearer light suited for tasks. For relaxation, warmer temperatures (2700K-3000K) create a cozy ambiance.\n\n2. Clashing with Decor: Selecting a color temperature that doesn't complement existing decor can result in an unpleasant mismatch. For instance, cool whites can clash with warm-toned decors such as warm wood and beige tiles, as seen in your bathroom image.\n\n3. Not Testing Light in the Space: Failing to test the chosen LED light in the actual bathroom space can lead to poor results. Lighting can look different in various environments, so a test can ensure the temperature suits the decor and functional needs.\n\n4. Ignoring Dimmability: Choosing non-dimmable LED strips can limit flexibility. Dimmable options allow you to adjust the brightness to create the desired ambiance at different times.\n\n5. Overlooking CRI (Color Rendering Index): Low CRI LEDs might not accurately render the colors in your bathroom, affecting makeup application and decor appearance. Ensure the LED strips have a CRI of at least 80 for accurate color depiction.\n\nBy being mindful of these aspects, you can avoid these pitfalls and select the perfect LED lighting for your bathroom.\n\nImage, showing how different LED color temperatures can affect the bathroom's ambiance\n\nWould you like advice on specific brands or types of LED strip lights that would suit your bathroom's decor and function?",
					"imageR": ".\\assets\\diag_sample\\5.23plus\\image16.jpeg",
					"imageP": "#",
					"color": "default"
				},
				{
					"group":3,
					"figures": 0,
					"prompt": "What can you infer about the time of day based on the lighting and the activities in the image?",
					"response": "From the scene, I can infer that it is a casual time of day, most likely during daytime, as there is lighting coming through the windows, allowing for indoor activities such as reading a newspaper or spending leisure time sharing a meal. Additionally, the man and woman are comfortably seated at a dining table, which suggests a relaxed and informal setting.",
					"imageR": "#",
					"imageP": ".\\assets\\diag_sample\\5.23plus\\image17.png",
					"color": "default"
				},
				{
					"group":3,
					"figures": 3,
					"prompt": "Is there anything specific about the attire or objects in the image that can give further clues about the time of day or the occasion?",
					"response": "The attire, specifically the bathrobes, suggests a relaxed morning setting, possibly breakfast or brunch. The presence of a newspaper also supports the idea of a leisurely morning. The food on the table indicates a casual meal rather than a formal occasion.\n\nWould you like to explore what kind of food is on the table?",
					"imageR": "#",
					"imageP": "#",
					"color": "default"
				},
				{
					"group":3,
					"figures": 3,
					"prompt": "Yes, please tell me more about the food on the table. What kind of meal does it appear to be, and does that provide any further clues about the time of day or occasion?",
					"response": "The meal appears to be a light breakfast or brunch, featuring items such as toast, fruit, and juice. The casual setting and attire suggest it might be a weekend or a day off. The presence of a newspaper and the relaxed posture of the individuals imply a leisurely morning. Would you like to know more about the types of food typically served during brunch?",
					"imageR": "#",
					"imageP": "#",
					"color": "default"
				},
				{
					"group":3,
					"figures": 3,
					"prompt": "You mentioned the food and attire suggest it might be a weekend or a day off. Are there any other visual cues in the image, such as the presence or absence of work-related items, that further support this inference?",
					"response": "The meal appears to be a light breakfast or brunch, featuring items such as toast, fruit, and juice. The casual setting and attire suggest it might be a weekend or a day off. The presence of a newspaper and the relaxed posture of the individuals imply a leisurely morning. Would you like to know more about the types of food typically served during brunch?",
					"imageR": "#",
					"imageP": "#",
					"color": "default"
				},
				{
					"group":3,
					"figures": 3,
					"prompt": "Before we delve into brunch foods, could you analyze the surroundings for any visual cues related to work? For instance, are there any briefcases, laptops, or other office-related items that might suggest otherwise?",
					"response": "The image shows a couple enjoying a meal together in a bright, sunlit room. There are no visible briefcases, laptops, or other office-related items in the scene. The setting appears to be a relaxed and informal environment, suggesting that they are not engaged in work-related activities at the moment. Would you like to discuss the types of food typically served during brunch?",
					"imageR": "#",
					"imageP": "#",
					"color": "default"
				},
				{
					"group":3,
					"figures": 3,
					"prompt": "Before we delve into brunch foods, could you analyze the surroundings for any visual cues related to work? For instance, are there any briefcases, laptops, or other office-related items that might suggest otherwise?",
					"response": "Based on the image, there are no visible briefcases, laptops, or other office-related items. This absence reinforces the idea of a relaxed setting, consistent with a weekend or day off, as previously inferred from the attire and the meal. Would you like to explore some brunch food options?",
					"imageR": ".\\assets\\diag_sample\\5.23plus\\image18.jpeg",
					"imageP": "#",
					"color": "default"
				},
				{
					"group":3,
					"figures": 1,
					"prompt": "Before we delve into brunch foods, could you analyze the surroundings for any visual cues related to work? For instance, are there any briefcases, laptops, or other office-related items that might suggest otherwise?",
					"response": "Analyzing the surroundings for work-related visual cues, there are no apparent office-related items such as briefcases, laptops, or any other work-related tools visible in the image. The setting is very much conducive to relaxation, with elements such as a leisurely breakfast table set with fruit, juice, and toast, suggesting a casual morning or brunch atmosphere. The attire‚Äîbathrobes‚Äîand the presence of a newspaper further reinforce the idea of leisure time, perhaps during a weekend or a day off. Would you like to explore more about typical brunch settings or dishes?",
					"imageR": ".\\assets\\diag_sample\\5.23plus\\image19.jpeg",
					"imageP": "#",
					"color": "default"
				},
				];
	
	
	// ÂàõÂª∫‰∏â‰∏™ÁªÑÁöÑÂÆπÂô®
	const group1Content = document.getElementById('group1Content');
	const group2Content = document.getElementById('group2Content');
	const group3Content = document.getElementById('group3Content');
	
	// ÂáΩÊï∞ÔºöÂ§ÑÁêÜÂçï‰∏™ÂØπËØùÂπ∂ËøîÂõû HTML ÂÖÉÁ¥†
	function createDialogueElement(item, roundNumber) {
	    const roundDiv = document.createElement('div');
	    roundDiv.style.marginBottom = '1.5em';
	    roundDiv.style.padding = '0.5em';
	    roundDiv.style.borderBottom = '1px solid #eee';
	
	    // Prompt Ê†áÈ¢ò
	    const promptTitle = document.createElement('h4');
	    promptTitle.textContent = `Round ${roundNumber} Prompt`;
	    promptTitle.style.color = '#666';
	    promptTitle.style.marginBottom = '0.3em';
	    promptTitle.style.fontSize = '0.95em';
	
	    // Prompt ÊñáÊú¨
	    const promptText = document.createElement('p');
	    promptText.textContent = item.prompt;
	    promptText.style.fontWeight = 'bold';
	    promptText.style.marginBottom = '0.5em';
	    promptText.style.fontSize = '0.95em';
	
	    // Response Ê†áÈ¢ò
	    const responseTitle = document.createElement('h4');
	    responseTitle.textContent = `Round ${roundNumber} Response`;
	    responseTitle.style.color = '#666';
	    responseTitle.style.marginBottom = '0.3em';
	    responseTitle.style.fontSize = '0.95em';
	
	    // Response ÊñáÊú¨
	    const responseText = document.createElement('p');
	    responseText.textContent = item.response;
	    responseText.style.marginBottom = '0.5em';
	    responseText.style.fontSize = '0.95em';
	
	    // ÁªÑË£Ö Prompt Âå∫Âüü
	    roundDiv.appendChild(promptTitle);
	    roundDiv.appendChild(promptText);
	
	    // Â¶ÇÊûú Prompt ÊúâÂõæÁâáÔºåÊòæÁ§∫ÂõæÁâáÔºàÂõ∫ÂÆöÂ§ßÂ∞èÔºâ
	    if (item.imageP && item.imageP !== '#') {
	        const promptImg = document.createElement('img');
	        promptImg.src = item.imageP.replace(/\\/g, '/');
	        promptImg.alt = `Round ${roundNumber} Prompt Image`;
	        promptImg.style.width = '100%';
	        promptImg.style.height = '150px';
	        promptImg.style.objectFit = 'cover';
	        promptImg.style.marginBottom = '0.5em';
	        promptImg.style.borderRadius = '4px';
	        roundDiv.appendChild(promptImg);
	    }
	
	    // ÁªÑË£Ö Response Âå∫Âüü
	    roundDiv.appendChild(responseTitle);
	    roundDiv.appendChild(responseText);
	
	    // Â¶ÇÊûú Response ÊúâÂõæÁâáÔºåÊòæÁ§∫ÂõæÁâáÔºàÂõ∫ÂÆöÂ§ßÂ∞èÔºâ
	    if (item.imageR && item.imageR !== '#') {
	        const responseImg = document.createElement('img');
	        responseImg.src = item.imageR.replace(/\\/g, '/');
	        responseImg.alt = `Round ${roundNumber} Response Image`;
	        responseImg.style.width = '100%';
	        responseImg.style.height = '150px';
	        responseImg.style.objectFit = 'cover';
	        responseImg.style.marginTop = '0.5em';
	        responseImg.style.borderRadius = '4px';
	        roundDiv.appendChild(responseImg);
	    }
	
	    return roundDiv;
	}
	
	// ÊåâÁªÑÂ§ÑÁêÜÊï∞ÊçÆÂπ∂Ê∑ªÂä†Âà∞ÂØπÂ∫îÁöÑÂÆπÂô®
	let group1Round = 1;
	let group2Round = 1;
	let group3Round = 1;
	
	dataexamples.forEach(item => {
	    if (item.group === 1) {
	        const element = createDialogueElement(item, group1Round);
	        group1Content.appendChild(element);
	        group1Round++;
	    } else if (item.group === 2) {
	        const element = createDialogueElement(item, group2Round);
	        group2Content.appendChild(element);
	        group2Round++;
	    } else if (item.group === 3) {
	        const element = createDialogueElement(item, group3Round);
	        group3Content.appendChild(element);
	        group3Round++;
	    }
	});
</script>
<section>
	<div class="container" style="margin-bottom: 2vh;">
		<div class="columns is-centered has-text-centered">
			<div class="column is-four-fifths">
				<h2 class="title is-3" id="Evaluation" style="text-align: center;"> Case Study </h2>
				<p style="text-align: justify;">This case study revealed that the two models fall short in image-text consistency, instruction following, multi-turn interaction, and image quality. By contrast, our dataset enables <strong>better adherence to user intent through high-quality
						multimodal understanding and generation.</strong></p>
				<div id="dialog-sample-container"></div>
				<svg style="display:none;">
					<symbol id="icon-gemini" viewBox="0 0 24 24">
						<?xml version="1.0" ?><svg id="Apple" viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg">
							<defs>
								<style>
									.cls-1{fill:#f0432c;}.cls-2{fill:#ffbf00;}.cls-3{fill:#24aa49;}.cls-4{fill:#3384fc;}
								</style>
							</defs>
							<title />
							<path class="cls-1" d="M90,93.25l65.06,65.06a140.31,140.31,0,0,1,201-1v0l65-65.09a232.33,232.33,0,0,0-331,1Z" />
							<path class="cls-2" d="M90.85,419.76l65-65a140.34,140.34,0,0,1,1-198.71L91.81,91a232.35,232.35,0,0,0-1,328.8Z" />
							<polygon class="cls-3" points="301.98 301.84 301.98 302.8 302.94 302.8 301.98 301.84" />
							<path class="cls-3" d="M255.5,488.21a231.65,231.65,0,0,0,164.55-68.3L355,354.84a140.3,140.3,0,0,1-199,0l-65,65A231.65,231.65,0,0,0,255.5,488.21Z" />
							<path class="cls-4" d="M484.36,210.33l-181.42.25V302.8h85.84a140.7,140.7,0,0,1-33.69,52.91l65.05,65.06a232.65,232.65,0,0,0,64.22-210.44Z" />
						</svg>
					</symbol>
					<symbol id="icon-janus" viewBox="0 0 24 24">
						<svg xmlns="http://www.w3.org/2000/svg" shape-rendering="geometricPrecision" text-rendering="geometricPrecision" image-rendering="optimizeQuality" fill-rule="evenodd" clip-rule="evenodd" viewBox="0 0 512 509.64">
							<path fill="#fff" d="M115.612 0h280.775C459.974 0 512 52.026 512 115.612v278.415c0 63.587-52.026 115.613-115.613 115.613H115.612C52.026 509.64 0 457.614 0 394.027V115.612C0 52.026 52.026 0 115.612 0z" />
							<path fill="#4D6BFE" fill-rule="nonzero"
								d="M440.898 139.167c-4.001-1.961-5.723 1.776-8.062 3.673-.801.612-1.479 1.407-2.154 2.141-5.848 6.246-12.681 10.349-21.607 9.859-13.048-.734-24.192 3.368-34.04 13.348-2.093-12.307-9.048-19.658-19.635-24.37-5.54-2.449-11.141-4.9-15.02-10.227-2.708-3.795-3.447-8.021-4.801-12.185-.861-2.509-1.725-5.082-4.618-5.512-3.139-.49-4.372 2.142-5.601 4.349-4.925 9.002-6.833 18.921-6.647 28.962.432 22.597 9.972 40.597 28.932 53.397 2.154 1.47 2.707 2.939 2.032 5.082-1.293 4.41-2.832 8.695-4.186 13.105-.862 2.817-2.157 3.429-5.172 2.205-10.402-4.346-19.391-10.778-27.332-18.553-13.481-13.044-25.668-27.434-40.873-38.702a177.614 177.614 0 00-10.834-7.409c-15.512-15.063 2.032-27.434 6.094-28.902 4.247-1.532 1.478-6.797-12.251-6.736-13.727.061-26.285 4.653-42.288 10.777-2.34.92-4.801 1.593-7.326 2.142-14.527-2.756-29.608-3.368-45.367-1.593-29.671 3.305-53.368 17.329-70.788 41.272-20.928 28.785-25.854 61.482-19.821 95.59 6.34 35.943 24.683 65.704 52.876 88.974 29.239 24.123 62.911 35.943 101.32 33.677 23.329-1.346 49.307-4.468 78.607-29.27 7.387 3.673 15.142 5.144 28.008 6.246 9.911.92 19.452-.49 26.839-2.019 11.573-2.449 10.773-13.166 6.586-15.124-33.915-15.797-26.47-9.368-33.24-14.573 17.235-20.39 43.213-41.577 53.369-110.222.8-5.448.121-8.877 0-13.287-.061-2.692.553-3.734 3.632-4.041 8.494-.981 16.742-3.305 24.314-7.471 21.975-12.002 30.84-31.719 32.933-55.355.307-3.612-.061-7.348-3.879-9.245v-.003zM249.4 351.89c-32.872-25.838-48.814-34.352-55.4-33.984-6.155.368-5.048 7.41-3.694 12.002 1.415 4.532 3.264 7.654 5.848 11.634 1.785 2.634 3.017 6.551-1.784 9.493-10.587 6.55-28.993-2.205-29.856-2.635-21.421-12.614-39.334-29.269-51.954-52.047-12.187-21.924-19.267-45.435-20.435-70.542-.308-6.061 1.478-8.207 7.509-9.307 7.94-1.471 16.127-1.778 24.068-.615 33.547 4.9 62.108 19.902 86.054 43.66 13.666 13.531 24.007 29.699 34.658 45.496 11.326 16.778 23.514 32.761 39.026 45.865 5.479 4.592 9.848 8.083 14.035 10.656-12.62 1.407-33.673 1.714-48.075-9.676zm15.899-102.519c.521-2.111 2.421-3.658 4.722-3.658a4.74 4.74 0 011.661.305c.678.246 1.293.614 1.786 1.163.861.859 1.354 2.083 1.354 3.368 0 2.695-2.154 4.837-4.862 4.837a4.748 4.748 0 01-4.738-4.034 5.01 5.01 0 01.077-1.981zm47.208 26.915c-2.606.996-5.2 1.778-7.707 1.88-4.679.244-9.787-1.654-12.556-3.981-4.308-3.612-7.386-5.631-8.679-11.941-.554-2.695-.247-6.858.246-9.246 1.108-5.144-.124-8.451-3.754-11.451-2.954-2.449-6.711-3.122-10.834-3.122-1.539 0-2.954-.673-4.001-1.224-1.724-.856-3.139-3-1.785-5.634.432-.856 2.525-2.939 3.018-3.305 5.6-3.185 12.065-2.144 18.034.244 5.54 2.266 9.727 6.429 15.759 12.307 6.155 7.102 7.263 9.063 10.773 14.39 2.771 4.163 5.294 8.451 7.018 13.348.877 2.561.071 4.74-2.341 6.277-.981.625-2.109 1.044-3.191 1.458z" />
						</svg>
					</symbol>
					<symbol id="icon-dataset" viewBox="0 0 24 24" fill="currentColor">
						<svg id="Layer_1" data-name="Layer 1" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 122.88 116.41">
							<defs>
								<style>
									.cls-1{fill-rule:evenodd;}
								</style>
							</defs>
							<title>database-table</title>
							<path class="cls-1"
								d="M16.49,24.88C24.05,27.41,34.57,29,46.26,29S68.48,27.41,76,24.88c6.63-2.22,10.73-4.9,10.73-7.52S82.67,12.06,76,9.84C68.48,7.33,58,5.75,46.27,5.75S24.06,7.33,16.49,9.84c-14.06,4.7-14.46,10.21,0,15ZM64.91,55.34h48.73a9.27,9.27,0,0,1,9.24,9.24v42.58a9.27,9.27,0,0,1-9.24,9.25H64.91a9.27,9.27,0,0,1-9.24-9.25V64.58a9.27,9.27,0,0,1,9.24-9.24ZM91.09,99.18H118v12H91.09v-12Zm-30.89,0H87.13v12H60.2v-12Zm0-31.89H87.13v12H60.2v-12Zm0,15.94H87.13v12H60.2v-12ZM91.09,67.29H118v12H91.09v-12Zm0,15.94H118v12H91.09v-12ZM5.82,45.77c.52,2.45,4.5,4.91,10.68,7,7.22,2.42,17.16,3.95,28.24,4.08v5.77c-11.67-.13-22.25-1.78-30.05-4.39A35.86,35.86,0,0,1,5.84,54V71.27c.52,2.45,4.5,4.91,10.68,7,7.22,2.4,17.15,3.94,28.22,4.07v5.75c-11.67-.14-22.25-1.78-30.05-4.4A36.08,36.08,0,0,1,5.83,79.5V96.75c.52,2.45,4.51,4.91,10.68,7,7.22,2.41,17.16,4,28.23,4.08v5.75c-11.67-.13-22.24-1.78-30-4.4C10.4,107.72,0,103,0,97.38V95.55C0,69.86,0,43.06,0,17.41c0-5.43,5.61-10,14.66-13C22.82,1.68,34,0,46.27,0S69.7,1.68,77.87,4.41s13.64,6.78,14.55,11.53a3,3,0,0,1,.16,1v28.6H86.8V26.09a36.69,36.69,0,0,1-8.93,4.22c-8.15,2.75-19.31,4.41-31.58,4.41S22.83,33,14.66,30.31A36.26,36.26,0,0,1,5.8,26.14V45.77Z" />
						</svg>
					</symbol>
				</svg>
				<div class="diag-dropdown-container">
					<label for="exampleSelect">Select Cases: </label>
					<select id="exampleSelect"></select>
				</div>
				<div class="diag-container">
					<div id="Gemini" class="diag-section diag-default">
						<div class="diag-section-header diag-default">
							<svg>
								<use xlink:href="#icon-gemini"></use>
							</svg>
							<span class="highlight">Gemini ‚ñº</span>
						</div>
						<div class="diag-content"></div>
					</div>
					<div id="Janus" class="diag-section diag-default">
						<div class="diag-section-header diag-default">
							<svg>
								<use xlink:href="#icon-janus"></use>
							</svg>
							<span class="highlight">Janus ‚ñº</span>
						</div>
						<div class="diag-content"></div>
					</div>
					<div id="Ours" class="diag-section diag-orange">
						<div class="diag-section-header diag-orange">
							<svg>
								<use xlink:href="#icon-dataset"></use>
							</svg>
							<span class="highlight">Ours ‚ñº</span>
						</div>
						<div class="diag-content"></div>
					</div>
				</div>
				<script>
					const data = [
								{
									"model": "Janus",
									"title": "Case 1",
									"figures": 1,
									"prompt": "What immediate actions would you recommend for the local authorities to take in response to the situation depicted in the image, and why?",
									"response": "The image depicts a flooded area, with water covering the street and reflecting surrounding trees and buildings. The floodwater appears to be at a significant depth, potentially affecting traffic and access to homes. Immediate actions for local authorities to take in response to this situation would include: 1. *Evacuation and Safety Measures*: Ensure the safety of residents by providing alternative shelters and evacuation routes. This includes setting up emergency shelters and coordinating with local emergency services. 2. *Flood Control and Drainage*: Activate flood control measures such as opening storm drains and releasing water from nearby reservoirs or flood management systems to reduce water accumulation. 3. *Communication and Coordination*: Establish communication channels with affected residents to provide updates on the flood situation, safety instructions, and resources available. 4. *Damage Assessment and Repair*: Send teams to assess the extent of the damage and initiate repair efforts to restore infrastructure, such as roads, bridges, and utilities. 5. *Environmental Monitoring*: Monitor water quality and potential contamination risks to ensure public health and safety.",
									"imageR": ".\\assets\\diag_sample\\image1.jpg",
									"imageP": "#",
									"color": "default"
								},
								{
									"model": "Janus",
									"title": "Case 1",
									"figures": 1,
									"prompt": "Please tell me how to build temporary roads or bridges, and generate one in the diagram.",
									"response": "To build temporary roads or bridges, follow these steps: 1. *Assessment and Planning*: Evaluate the flood-affected area to determine the most suitable locations for temporary roads or bridges. Consider factors such as water depth, flow velocity, and structural stability. 2. *Materials and Equipment*: Gather materials like steel beams, concrete blocks, and construction tools. Ensure that the equipment is suitable for the water conditions, such as using waterproof materials and equipment. 3. *Foundation Preparation*: Prepare the ground by clearing debris and leveling the surface. Ensure that the foundation is stable and can support the weight of the temporary structure. 4. *Construction*: Construct the temporary road or bridge using the prepared materials. Use strong materials to ensure durability and safety. 5. *Testing and Maintenance*: Test the temporary structure for stability and safety. Regularly maintain it to ensure it remains functional and safe for use.",
									"imageR": ".\\assets\\diag_sample\\image2.jpg",
									"imageP": "#",
									"color": "default"
								},
								{
									"model": "Janus",
									"title": "Case 1",
									"figures": 3,
									"prompt": "How long does it take to build such a bridge?",
									"response": "Building a temporary bridge or road in flood-affected areas requires careful planning and execution. The time taken depends on several factors, including the complexity of the structure, the depth of the water, and the availability of materials and equipment. 1. *Planning and Assessment*: This phase typically takes a few days to a week. It involves assessing the flood conditions, determining the best location for the bridge or road, and planning the construction process. 2. *Material Procurement*: Gathering materials such as steel beams, concrete blocks, and construction tools can take a few days. 3. *Construction*: Building the bridge or road can take several weeks to a few months, depending on the complexity and the number of workers involved. 4. *Testing and Maintenance*: Once completed, the bridge or road needs to be tested for safety and durability. Regular maintenance is required to ensure its longevity. Overall, the construction of a temporary bridge or road in flood-affected areas can take anywhere from a few weeks to several months.",
									"imageR": "#",
									"imageP": "#",
									"color": "default"
								},
								{
									"model": "Gemini",
									"title": "Case 1",
									"figures": 1,
									"prompt": "What immediate actions would you recommend for the local authorities to take in response to the situation depicted in the image, and why?",
									"response": "The image depicts a street scene with significant floodwater inundating buildings and vehicles. Some vehicles appear partially submerged. *Immediate Actions for Local Authorities:* 1. *Issue Emergency Alerts and Warnings:* Utilize all available channels (sirens, SMS, social media, radio, television) to inform residents about the dangerous flooding and advise them to seek higher ground immediately. Emphasize the risks of driving or walking through floodwaters. 2. *Activate Emergency Response Teams:* Deploy police, fire department, and emergency medical services to the affected areas. Their immediate priorities should be: * *Life Safety:* Conducting search and rescue operations for individuals trapped in homes or vehicles. * *Medical Assistance:* Providing aid to anyone injured. * *Security:* Securing the flooded areas to prevent looting and ensure public safety. 3. *Establish Shelters:* Open designated emergency shelters (schools, community centers, etc.) for displaced residents, providing them with temporary housing, food, water, and essential supplies. 4. *Assess the Situation and Infrastructure:* Begin assessing the extent of the damage to roads, bridges, power lines, and other critical infrastructure. This will help in understanding the scope of the disaster and planning the subsequent response. 5. *Block Off Affected Roads:* Close roads leading into and within the flooded areas to prevent further traffic and potential accidents in the dangerous conditions. *Why These Actions Are Crucial:* These immediate actions prioritize the safety and well-being of the affected population. Swift warnings and the deployment of emergency services are essential to minimize casualties and injuries. Providing shelter addresses the immediate needs of those displaced from their homes. Assessing the damage and securing the area lays the groundwork for a coordinated and effective longer-term recovery effort. Preventing access to flooded areas reduces the risk of further harm and allows emergency responders to work more efficiently.",
									"imageR": ".\\assets\\diag_sample\\image3.png",
									"imageP": "#",
									"color": "default"
								},
								{
									"model": "Gemini",
									"title": "Case 1",
									"figures": 1,
									"prompt": "In addition to these immediate actions, what long-term strategies could the local authorities implement to prevent or mitigate future flooding incidents in this area?",
									"response": "I will generate an image depicting a flooded street scene to help illustrate potential immediate actions for local authorities.",
									"imageR": ".\\assets\\diag_sample\\image4.png",
									"imageP": "#",
									"color": "default"
								},
								{
									"model": "Gemini",
									"title": "Case 1",
									"figures": 1,
									"prompt": "It seems that your image does not show any long-term strategies, please supplement with textual description.",
									"response": "Immediate Actions:\n* Emergency Response Activation: Deploy emergency services, including police, fire department, and ambulance, to the affected areas.\n* Evacuation and Shelter: Initiate evacuation procedures for residents in high-risk zones, providing safe shelter and essential supplies.\n* Road Closure and Traffic Management: Close flooded roads and manage traffic flow to ensure safety and allow access for emergency vehicles.\n* Damage Assessment: Begin preliminary assessments of the extent of the flooding and damage to infrastructure and property.\n* Communication and Information: Keep the public informed about the situation, safety measures, and available assistance through various channels.\n* Resource Mobilization: Activate and coordinate resources, including personnel, equipment, and supplies, for rescue and relief efforts.",
									"imageR": ".\\assets\\diag_sample\\image5.png",
									"imageP": "#",
									"color": "default"
								},
								{
									"model": "Ours",
									"title": "Case 1",
									"figures": 3,
									"prompt": "What immediate actions would you recommend for the local authorities to take in response to the situation depicted in the image, and why?",
									"response": "The image shows a flooded street with a traffic light and power lines submerged in water. Based on the current state, I would recommend that the local authorities take immediate actions to ensure the safety of residents and maintain traffic flow.\n\nFirst and foremost, they should deploy emergency personnel to the affected areas to rescue people that might be in danger and ensure their safety. Given the possible risk of electrocution from the submerged power lines, it is vital to shut off and secure the power supply to the area.\n\nNext, the local authorities should assess the traffic light's functionality and ensure it remains operational to manage the flow of vehicles and pedestrians. If necessary, they may need to replace or repair the damaged traffic light and lines to restore proper functionality.\n\nFurthermore, they should work on clearing and draining the water accumulated in the streets to minimize the risk of infections and to restore normality to daily life. They might consider using specialized machinery and personnel for this task, if the situation is severe or widespread.\n\nLastly, the local authorities should communicate their actions and plans effectively to the public via social media, emergency alerts, and notifications to keep residents informed about the situation and any assistance or resources available.\n\nConclusion: Look for opportunities to improve the traffic flow, safety, and well-being of the community during the recovery process.",
									"imageR": "#",
									"imageP": "#",
									"color": "yellow"
								},
								{
									"model": "Ours",
									"title": "Case 1",
									"figures": 3,
									"prompt": "In addition to these immediate actions, what long-term strategies could the local authorities implement to prevent or mitigate future flooding incidents in this area?",
									"response": "Based on the previous response, I would recommend that the local authorities in the area take immediate actions to ensure the safety of residents and maintain traffic flow in the flooded street. To achieve this, they should deploy emergency personnel to rescue people that may be in danger and ensure their safety. Given the possible risk of electrocution from the submerged power lines, it is crucial to shut off and secure the power supply to the area. \n\nIn addition to these immediate actions, long-term strategies could include implementing flood-prevention measures, such as improving the city's drainage system, raising roads and infrastructure to minimize the impact of flooding, creating emergency response plans, and conducting regular maintenance checks on traffic lights and power lines to reduce the risk of damage during extreme weather events.\n\nThe implementation of these measures can help prevent or mitigate future flooding incidents in the area, ensuring the safety and well-being of residents and minimizing the disruption to daily life.",
									"imageR": "#",
									"imageP": "#",
									"color": "yellow"
								},
								{
									"model": "Ours",
									"title": "Case 1",
									"figures": 1,
									"prompt": "That's a great overview. Could you elaborate on how the local authorities might go about 'improving the city's drainage system'? What specific improvements or technologies might be considered?",
									"response": "To improve the city's drainage system and mitigate future flooding effectively, local authorities can consider several specific improvements and technologies:\n\n1. *Upgrading and Expanding Drainage Infrastructure*: By increasing the capacity of existing drainage systems and constructing additional channels, the city can better accommodate heavy rainfall and reduce flood risk. This might involve widening culverts, enlarging stormwater pipes, and constructing detention basins to manage water flow more effectively.\n\n2. *Implementing Green Infrastructure*: Integrating green technology such as rain gardens, permeable pavements, and green roofs can help absorb and divert rainwater naturally. These solutions not only reduce immediate drainage loads but also improve urban aesthetics and biodiversity.\n\n3. *Advanced Hydrological Modeling*: Utilizing sophisticated software to simulate rainfall scenarios can help identify potential weaknesses in drainage and plan infrastructure improvements more strategically. This modeling aids in efficient planning and resource allocation.\n\n4. *Sensor-Based Monitoring Systems*: Implementing sensor technology in critical drainage locations can provide real-time data on water levels and system performance. This allows for proactive management and quick responses during heavy rain events.\n\n5. *Public Awareness and Engagement*: Educating citizens about flood risks and encouraging practices that reduce blockage in drainage systems, like proper disposal of waste, can significantly improve overall efficiency.\n\n6. *Regular Maintenance and Cleaning*: Conducting frequent checks and cleaning of drainage systems ensures they remain clear of debris, reducing the likelihood of blockages during storms.\n\n7. *Infrastructure Integration*: Consideration of multi-use designs that integrate drainage systems with urban features can both save space and enhance functionality, such as park spaces that double as flood basins.\n\nThese strategies, when combined, can create a robust drainage system tailored to the city's unique needs, reducing the impact of future flooding events. \n\nWould you like more information on the benefits of green infrastructure?",
									"imageR": ".\\assets\\diag_sample\\image6.jpg",
									"imageP": "#",
									"color": "yellow"
								},
								{
									"model": "Gemini",
									"title": "Case 2",
									"figures": 2,
									"prompt": "Considering the cleanliness and arrangement, what can you infer about the maintenance of this bathroom?",
									"response": "The bathroom appears to be meticulously maintained. The surfaces are clear of clutter and look clean, suggesting regular tidying. The arrangement of items seems deliberate and organized, further indicating a commitment to upkeep.",
									"imageR": ".\\assets\\diag_sample\\image8.png",
									"imageP": ".\\assets\\diag_sample\\image7.png",
									"color": "default"
								},
								{
									"model": "Gemini",
									"title": "Case 2",
									"figures": 0,
									"prompt": "The bathroom image you generated looks a bit cluttered. What kind of updates or additions could enhance the bathroom's functionality or aesthetics?",
									"response": "The current image shows a bathroom with some items on the counter and a few things in the shower area. Updates to enhance functionality and aesthetics could include adding closed storage like drawers or cabinets to reduce visible clutter, incorporating a consistent color scheme for towels and accessories, and perhaps a larger mirror to visually expand the space.",
									"imageR": "#",
									"imageP": ".\\assets\\diag_sample\\image9.png",
									"color": "default"
								},
								{
									"model": "Gemini",
									"title": "Case 2",
									"figures": 1,
									"prompt": "That's helpful! Considering the potential for adding storage or decorative elements, how would you prioritize these additions based on cost-effectiveness and impact on the bathroom's overall appeal?",
									"response": "The image displays a clean and relatively well-maintained bathroom. The sink area is tidy, and the surfaces appear to be clear of excessive clutter. This suggests a regular cleaning routine. The presence of what looks like a toothbrush holder and a neatly placed hand towel indicates attention to organization.",
									"imageR": ".\\assets\\diag_sample\\image10.png",
									"imageP": "#",
									"color": "default"
								},
								{
									"model": "Janus",
									"title": "Case 2",
									"figures": 2,
									"prompt": "Considering the cleanliness and arrangement, what can you infer about the maintenance of this bathroom?",
									"response": "The bathroom appears to be well-maintained, as evidenced by the cleanliness of the tiles, the organized placement of items, and the absence of visible dirt or clutter. The mirror is clean, and the sink and surrounding area are tidy, suggesting regular upkeep. The presence of a trash bin and a toilet paper holder further indicates attention to cleanliness and organization.",
									"imageR": ".\\assets\\diag_sample\\image12.png",
									"imageP": ".\\assets\\diag_sample\\image11.png",
									"color": "default"
								},
								{
									"model": "Janus",
									"title": "Case 2",
									"figures": 1,
									"prompt": "The toilet picture you generated looks dark and plain. What kind of updates or additions could enhance the bathroom's functionality or aesthetics?",
									"response": "To enhance the functionality and aesthetics of the bathroom, several updates or additions could be considered:\n\n1. Lighting: Adding a brighter or more modern light fixture above the mirror could improve visibility and create a more inviting atmosphere.\n2. Storage Solutions: Installing additional shelves or cabinets could provide more storage space for toiletries and other bathroom essentials.\n3. Decorative Elements: Introducing decorative elements such as a small plant, artwork, or a stylish soap dispenser could add personality and warmth to the space.\n4. Color Scheme: Updating the tiles or adding colorful accents could refresh the overall look and feel of the bathroom.\n5. Mirror Frame: A decorative frame for the mirror could add a touch of elegance and make the space feel more cohesive.",
									"imageR": ".\\assets\\diag_sample\\image13.png",
									"imageP": "#",
									"color": "default"
								},
								{
									"model": "Janus",
									"title": "Case 2",
									"figures": 3,
									"prompt": "Please present the complete scene of this toilet to me, not just the sink.",
									"response": "The complete scene of this toilet includes a white ceramic toilet with a standard flush mechanism, positioned against tiled walls. Above the toilet, there is a rectangular mirror with a simple frame, reflecting part of the room. To the right of the toilet, there is a wall-mounted metal rack with a wire basket, likely intended for holding toilet paper rolls. The floor is tiled in a light color, matching the walls, and there is a small trash bin with a lid located near the toilet. The overall design is functional and minimalistic, with a focus on cleanliness and practicality.",
									"imageR": "#",
									"imageP": "#",
									"color": "default"
								},
								{
									"model": "Ours",
									"title": "Case 2",
									"figures": 1,
									"prompt": "Considering the cleanliness and arrangement, what can you infer about the maintenance of this bathroom?",
									"response": "Based on the cleanliness of the sink area, bright white room, and mirror, it can be inferred that the bathroom is well-maintained. The white sink, white tiles, and white curtains contribute to a clean and organized appearance, which suggests that the space is cared for. Another notable aspect of the bathroom is the empty wall space, also shown in the image, which could indicate that the bathroom might need to be updated or finished. Overall, the cleanliness and arrangement of the bathroom seem to show good maintenance practices.",
									"imageR": "#",
									"imageP": ".\\assets\\diag_sample\\image14.png",
									"color": "yellow"
								},
								{
									"model": "Ours",
									"title": "Case 2",
									"figures": 1,
									"prompt": "You mentioned that the bathroom might need to be updated or finished due to the empty wall space. What kind of updates or additions could enhance the bathroom's functionality or aesthetics?",
									"response": "Updating or finishing the empty wall space in the bathroom could enhance its functionality and aesthetics. Some possible additions or updates could include installing additional storage, such as shelves or cabinets, to organize toiletries and accessories; installing a mirror above the sink to create the illusion of a larger space; or even incorporating decorative elements like artwork or plants to add visual interest and make the bathroom feel more inviting.",
									"imageR": ".\\assets\\diag_sample\\image15.jpg",
									"imageP": "#",
									"color": "yellow"
								},
								{
									"model": "Ours",
									"title": "Case 2",
									"figures": 3,
									"prompt": "That's helpful! Considering the potential for adding storage or decorative elements, how would you prioritize these additions based on cost-effectiveness and impact on the bathroom's overall appeal?",
									"response": "To prioritize additions based on cost-effectiveness and impact on the bathroom's overall appeal, I recommend starting with functional improvements that also enhance aesthetics. Here are some suggestions:\n\n1. Additional Storage: Installing floating shelves or a vanity with built-in cabinets can provide much-needed storage while maintaining a sleek look. This is both practical and visually appealing.\n\n2. Mirror Above Sink: Adding a mirror above the sink not only serves a practical purpose but also makes the space feel larger and brighter.\n\n3. Decorative Elements: Incorporating small, affordable items like a plant or a piece of artwork can add personality and warmth to the bathroom without breaking the bank.\n\n4. Lighting: Upgrading the lighting fixture can significantly improve the ambiance of the bathroom. Opt for energy-efficient LED lights that offer both cost savings and better illumination.\n\n5. Tile Accents: Adding a tile backsplash or border can give the bathroom a fresh look without requiring a full renovation.\n\nEach of these additions can be tailored to fit your budget and style preferences, ensuring a balanced approach to enhancing both functionality and aesthetics. Would you like to see some visual examples of these ideas?",
									"imageR": "#",
									"imageP": "#",
									"color": "yellow"
								}
								];
					
					const exampleSelect = document.getElementById('exampleSelect');
					const uniqueTitles = [...new Set(data.map(item => item.title))];
					uniqueTitles.forEach(title => {
					  const option = document.createElement('option');
					  option.value = title;
					  option.textContent = title;
					  exampleSelect.appendChild(option);
					});
					
					function parseBold(text) {
					  return text.replace(/\*\*(.*?)\*\*/g, '<strong>$1</strong>');
					}
					
					function parseOrderedList(text) {
					    const lines = text.split('\n');
					    let htmlOutput = '';
					    let currentListItemContent = [];
					    let inListScope = false;
					
					    function flushListItem() {
					        if (currentListItemContent.length > 0) {
					            htmlOutput += `<li>${parseBold(currentListItemContent.join('<br>'))}</li>`;
					            currentListItemContent = [];
					        }
					    }
					
					    function startList() {
					        if (!inListScope) {
					            htmlOutput += '<ol>';
					            inListScope = true;
					        }
					    }
					
					    function endList() {
					        if (inListScope) {
					            flushListItem();
					            htmlOutput += '</ol>';
					            inListScope = false;
					        }
					    }
					
					    for (const line of lines) {
					        if (/^\d+\.\s/.test(line)) {
					            startList();
					            flushListItem();
					            currentListItemContent.push(line.replace(/^\d+\.\s/, ''));
					        } else if (inListScope && line.trim() === '' && currentListItemContent.length > 0) {
					            currentListItemContent.push('');
					        } else if (inListScope && currentListItemContent.length > 0) {
					             currentListItemContent.push(line);
					        } else {
					            endList();
					            if (line.trim() !== '') {
					                htmlOutput += `<p>${parseBold(line)}</p>`;
					            }
					        }
					    }
					    endList();
					    if (htmlOutput.trim() === '' && text.trim() !== '') {
					        return '<p>' + parseBold(text.replace(/\n/g, '<br>')) + '</p>';
					    }
					    return htmlOutput;
					}
					
					function typeWriter(html, el, callback) {
					  el.innerHTML = '';
					  const tempDiv = document.createElement('div');
					  tempDiv.innerHTML = html;
					  let charDelay = 5;
					
					  let currentNode = tempDiv.firstChild;
					  function processNextRootNode() {
					    if (currentNode) {
					      const next = currentNode.nextSibling;
					      typeNodeRecursive(currentNode, el, () => {
					        currentNode = next;
					        processNextRootNode();
					      });
					    } else {
					      callback && callback();
					    }
					  }
					
					  function typeNodeRecursive(sourceNode, targetParent, doneCallback) {
					    if (sourceNode.nodeType === Node.TEXT_NODE) {
					      let text = sourceNode.textContent;
					      let i = 0;
					      function typeChar() {
					        if (i < text.length) {
					          targetParent.appendChild(document.createTextNode(text[i]));
					          i++;
					          setTimeout(typeChar, charDelay);
					        } else {
					          doneCallback();
					        }
					      }
					      typeChar();
					    } else if (sourceNode.nodeType === Node.ELEMENT_NODE) {
					      const newElement = document.createElement(sourceNode.nodeName);
					      for (let attr of sourceNode.attributes) {
					        newElement.setAttribute(attr.name, attr.value);
					      }
					      targetParent.appendChild(newElement);
					
					      let childNode = sourceNode.firstChild;
					      function processChildren() {
					        if (childNode) {
					          const nextChild = childNode.nextSibling;
					          typeNodeRecursive(childNode, newElement, () => {
					            childNode = nextChild;
					            processChildren();
					          });
					        } else {
					          doneCallback();
					        }
					      }
					      processChildren();
					    } else {
					      doneCallback();
					    }
					  }
					  processNextRootNode();
					}
					
					function createDialogueElement(item) {
					  const dialogueContainer = document.createElement('div');
					  dialogueContainer.className = 'diag-dialogue';
					
					  const promptDiv = document.createElement('div');
					  promptDiv.className = 'diag-prompt';
					  promptDiv.textContent = item.prompt;
					  dialogueContainer.appendChild(promptDiv);
					
					  if (item.imageP && item.imageP !== '#') {
					    const promptImg = document.createElement('img');
					    promptImg.src = item.imageP;
					    promptImg.alt = "Prompt image";
					    promptDiv.appendChild(promptImg);
					  }
					
					  const responseContainer = document.createElement('div');
					  responseContainer.className = 'diag-response';
					
					  const responseTextDiv = document.createElement('div');
					  responseTextDiv.className = 'diag-response-text-area diag-typing-caret';
					  
					  let responseImgElement = null;
					  if (item.imageR && item.imageR !== '#') {
					    responseImgElement = document.createElement('img');
					    responseImgElement.src = item.imageR;
					    responseImgElement.alt = "Response image";
					    responseImgElement.style.display = 'none';
					  }
					
					  responseContainer.appendChild(responseTextDiv);
					  if (responseImgElement) {
					      responseContainer.appendChild(responseImgElement);
					  }
					  
					  dialogueContainer.appendChild(responseContainer);
					
					  const processedResponseText = parseOrderedList(item.response);
					
					  typeWriter(processedResponseText, responseTextDiv, () => {
					    responseTextDiv.classList.remove('diag-typing-caret');
					    if (responseImgElement) {
					      responseImgElement.style.display = 'block';
					    }
					  });
					
					  return dialogueContainer;
					}
					
					function renderExample(title) {
					  const sections = {
					    Gemini: document.querySelector('#Gemini .diag-content'), // Prefixed
					    Janus: document.querySelector('#Janus .diag-content'),  // Prefixed
					    Ours: document.querySelector('#Ours .diag-content')     // Prefixed
					  };
					
					  document.getElementById('Gemini').className = 'diag-section diag-default'; // Prefixed
					  document.querySelector('#Gemini .diag-section-header').className = 'diag-section-header diag-default'; // Prefixed
					  document.querySelector('#Gemini .diag-section-header span').textContent = 'Gemini ‚ñº';
					
					  document.getElementById('Janus').className = 'diag-section diag-default'; // Prefixed
					  document.querySelector('#Janus .diag-section-header').className = 'diag-section-header diag-default'; // Prefixed
					  document.querySelector('#Janus .diag-section-header span').textContent = 'Janus ‚ñº';
					
					  document.getElementById('Ours').className = 'diag-section diag-orange'; // Prefixed
					  document.querySelector('#Ours .diag-section-header').className = 'diag-section-header diag-orange'; // Prefixed
					  document.querySelector('#Ours .diag-section-header span').textContent = 'Ours ‚ñº';
					
					  Object.values(sections).forEach(section => section.innerHTML = '');
					
					  const filtered = data.filter(d => d.title === title);
					
					  ['Gemini', 'Janus', 'Ours'].forEach(model => {
					    const items = filtered.filter(item => item.model === model);
					    const modelSectionContent = sections[model];
					    if (modelSectionContent) {
					        if (items.length === 0) {
					            modelSectionContent.innerHTML = '<p><i>No data for this example.</i></p>';
					        } else {
					            items.forEach(item => {
					                const el = createDialogueElement(item);
					                modelSectionContent.appendChild(el);
					            });
					        }
					    }
					  });
					}
					
					document.querySelectorAll('.diag-section-header').forEach(header => { // Prefixed
					  header.addEventListener('click', () => {
					    const content = header.nextElementSibling;
					    const currentSpan = header.querySelector('span');
					    if (content.style.display === 'none' || content.style.display === '') {
					      content.style.display = 'block';
					      if(currentSpan) currentSpan.textContent = currentSpan.textContent.replace('‚ñ∂', '‚ñº');
					    } else {
					      content.style.display = 'none';
					      if(currentSpan) currentSpan.textContent = currentSpan.textContent.replace('‚ñº', '‚ñ∂');
					    }
					  });
					});
					
					exampleSelect.addEventListener('change', e => {
					  renderExample(e.target.value);
					});
					
					if (uniqueTitles.length > 0) {
					    exampleSelect.value = uniqueTitles[0];
					    renderExample(uniqueTitles[0]);
					}
					
				</script>
			</div>
		</div>
	</div>
</section>
<!-- @PAN TODO: bibtex -->
<section class="section" id="BibTeX">
	<div class="container is-max-desktop content">
		<h2 class="title is-3 has-text-centered">BibTeX</h2>
		<pre><code style="color:#000000;">@article{chen2025intermt,
	title={InterMT: Multi-Turn Interleaved Preference Alignment with Human Feedback},
	author={Boyuan Chen and Donghai Hong and Jiaming Ji and Jiacheng Zheng and Bowen Dong and Jiayi Zhou and Kaile Wang and Josef Dai and Xuyao Wang and Wenqi Chen and Qirui Zheng and Wenxin Li and Sirui Han and Yike Guo and Yaodong Yang},
	year={2025},
	institution={Peking University and Hong Kong University of Science and Technology},
	url={https://pku-intermt.github.io},
	keywords={Multimodal Learning, Multi-Turn Interaction, Human Feedback, Preference Alignment}
	}</code></pre>
	</div>
</section>
<section>
	<div class="section" id="org-banners" style="display: flex; align-items: center;">
		<a href="https://www.pku.edu.cn/" target="_blank" rel="external" style="margin-right: 20px;">
			<img class="center-block org-banner" src="./assets/PKU_logo.png" style="width: 150px; height: 150px;">
		</a>
		<a href="https://hkust.edu.hk/" target="_blank" class="ext-link" style="margin-left: 20px;">
			<img class="center-block org-banner" src="./assets/HKUST-STD.svg" style="width: 400px; height: 200px;">
		</a>
	</div>
</section>
<footer class="footer">
	<!-- <div class="container"> -->
	<div class="content has-text-centered">
	</div>
	<div class="columns is-centered">
		<div class="column is-8">
			<div class="content">
				<p> This website is website adapted from <a href="https://nerfies.github.io/">Nerfies</a>, licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>. </p>
			</div>
		</div>
	</div>
	<!-- </div> -->
</footer>
</div>
</section>
</body>
</html>
